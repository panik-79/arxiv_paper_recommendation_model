{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/panik-79/arxiv_paper_recommendation_model/blob/main/minor_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "118cec97",
      "metadata": {
        "id": "118cec97"
      },
      "source": [
        "# This Notebook Perform two things...........\n",
        "\n",
        "# 1 Section:                                                                 \n",
        "Research Area Subject Area Prediction (Large Scale classification) using shallow Multi-Layer Perceptron (MLP) model\n",
        "\n",
        "# 2 Section:\n",
        "Research Paper Recommendation for reading: using sentence transformer model\n",
        "\n",
        "Research Papers dataset link::\n",
        "https://www.kaggle.com/datasets/spsayakpaul/arxiv-paper-abstracts/data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eda1c5f2-22d3-41c9-9690-a515719253ba",
      "metadata": {
        "id": "eda1c5f2-22d3-41c9-9690-a515719253ba"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "2b7cb120",
      "metadata": {
        "id": "2b7cb120"
      },
      "source": [
        "# 1 Section:                                                                 "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5adda69d",
      "metadata": {
        "id": "5adda69d"
      },
      "source": [
        "# Loading tools and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a0cb9160",
      "metadata": {
        "id": "a0cb9160"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from ast import literal_eval\n",
        "# is used for safely evaluating strings containing Python literals or container displays\n",
        "# (e.g., lists, dictionaries) to their corresponding Python objects.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "130dc39f",
      "metadata": {
        "id": "130dc39f"
      },
      "outputs": [],
      "source": [
        "arxiv_data = pd.read_csv(\"arxiv_data_210930-054931.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a912841",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "6a912841",
        "outputId": "ea3f0030-3e01-41ee-870c-8b27db5a4e99"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                           terms  \\\n",
              "0                      ['cs.LG']   \n",
              "1             ['cs.LG', 'cs.AI']   \n",
              "2  ['cs.LG', 'cs.CR', 'stat.ML']   \n",
              "3             ['cs.LG', 'cs.CR']   \n",
              "4                      ['cs.LG']   \n",
              "\n",
              "                                              titles  \\\n",
              "0  Multi-Level Attention Pooling for Graph Neural...   \n",
              "1  Decision Forests vs. Deep Networks: Conceptual...   \n",
              "2  Power up! Robust Graph Convolutional Network v...   \n",
              "3  Releasing Graph Neural Networks with Different...   \n",
              "4  Recurrence-Aware Long-Term Cognitive Network f...   \n",
              "\n",
              "                                           abstracts  \n",
              "0  Graph neural networks (GNNs) have been widely ...  \n",
              "1  Deep networks and decision forests (such as ra...  \n",
              "2  Graph convolutional networks (GCNs) are powerf...  \n",
              "3  With the increasing popularity of Graph Neural...  \n",
              "4  Machine learning solutions for pattern classif...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b9f8d855-c9c9-4072-a95b-4dc3b8650ebe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>terms</th>\n",
              "      <th>titles</th>\n",
              "      <th>abstracts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>['cs.LG']</td>\n",
              "      <td>Multi-Level Attention Pooling for Graph Neural...</td>\n",
              "      <td>Graph neural networks (GNNs) have been widely ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>['cs.LG', 'cs.AI']</td>\n",
              "      <td>Decision Forests vs. Deep Networks: Conceptual...</td>\n",
              "      <td>Deep networks and decision forests (such as ra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>['cs.LG', 'cs.CR', 'stat.ML']</td>\n",
              "      <td>Power up! Robust Graph Convolutional Network v...</td>\n",
              "      <td>Graph convolutional networks (GCNs) are powerf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>['cs.LG', 'cs.CR']</td>\n",
              "      <td>Releasing Graph Neural Networks with Different...</td>\n",
              "      <td>With the increasing popularity of Graph Neural...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>['cs.LG']</td>\n",
              "      <td>Recurrence-Aware Long-Term Cognitive Network f...</td>\n",
              "      <td>Machine learning solutions for pattern classif...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b9f8d855-c9c9-4072-a95b-4dc3b8650ebe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b9f8d855-c9c9-4072-a95b-4dc3b8650ebe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b9f8d855-c9c9-4072-a95b-4dc3b8650ebe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6b69f774-595a-4ee3-944d-1fff2edd4fd1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6b69f774-595a-4ee3-944d-1fff2edd4fd1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6b69f774-595a-4ee3-944d-1fff2edd4fd1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "arxiv_data",
              "summary": "{\n  \"name\": \"arxiv_data\",\n  \"rows\": 56181,\n  \"fields\": [\n    {\n      \"column\": \"terms\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3402,\n        \"samples\": [\n          \"['cs.LG', 'cs.CL', 'cs.CV', 'stat.ML']\",\n          \"['cs.LG', 'cs.AI', 'cs.CV', 'stat.ME', 'stat.ML']\",\n          \"['stat.ML', 'cs.LG', 'physics.chem-ph', 'physics.data-an']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"titles\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 41105,\n        \"samples\": [\n          \"Semi-supervised Federated Learning for Activity Recognition\",\n          \"SATR-DL: Improving Surgical Skill Assessment and Task Recognition in Robot-assisted Surgery with Deep Neural Networks\",\n          \"A Hybrid Stochastic Policy Gradient Algorithm for Reinforcement Learning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstracts\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 41115,\n        \"samples\": [\n          \"Counterfactual inference has become a ubiquitous tool in online\\nadvertisement, recommendation systems, medical diagnosis, and econometrics.\\nAccurate modeling of outcome distributions associated with different\\ninterventions -- known as counterfactual distributions -- is crucial for the\\nsuccess of these applications. In this work, we propose to model counterfactual\\ndistributions using a novel Hilbert space representation called counterfactual\\nmean embedding (CME). The CME embeds the associated counterfactual distribution\\ninto a reproducing kernel Hilbert space (RKHS) endowed with a positive definite\\nkernel, which allows us to perform causal inference over the entire landscape\\nof the counterfactual distribution. Based on this representation, we propose a\\ndistributional treatment effect (DTE) that can quantify the causal effect over\\nentire outcome distributions. Our approach is nonparametric as the CME can be\\nestimated under the unconfoundedness assumption from observational data without\\nrequiring any parametric assumption about the underlying distributions. We also\\nestablish a rate of convergence of the proposed estimator which depends on the\\nsmoothness of the conditional mean and the Radon-Nikodym derivative of the\\nunderlying marginal distributions. Furthermore, our framework allows for more\\ncomplex outcomes such as images, sequences, and graphs. Our experimental\\nresults on synthetic data and off-policy evaluation tasks demonstrate the\\nadvantages of the proposed estimator.\",\n          \"Current multi-person localisation and tracking systems have an over reliance\\non the use of appearance models for target re-identification and almost no\\napproaches employ a complete deep learning solution for both objectives. We\\npresent a novel, complete deep learning framework for multi-person localisation\\nand tracking. In this context we first introduce a light weight sequential\\nGenerative Adversarial Network architecture for person localisation, which\\novercomes issues related to occlusions and noisy detections, typically found in\\na multi person environment. In the proposed tracking framework we build upon\\nrecent advances in pedestrian trajectory prediction approaches and propose a\\nnovel data association scheme based on predicted trajectories. This removes the\\nneed for computationally expensive person re-identification systems based on\\nappearance features and generates human like trajectories with minimal\\nfragmentation. The proposed method is evaluated on multiple public benchmarks\\nincluding both static and dynamic cameras and is capable of generating\\noutstanding performance, especially among other recently proposed deep neural\\nnetwork based approaches.\",\n          \"Unifying text detection and text recognition in an end-to-end training\\nfashion has become a new trend for reading text in the wild, as these two tasks\\nare highly relevant and complementary. In this paper, we investigate the\\nproblem of scene text spotting, which aims at simultaneous text detection and\\nrecognition in natural images. An end-to-end trainable neural network named as\\nMask TextSpotter is presented. Different from the previous text spotters that\\nfollow the pipeline consisting of a proposal generation network and a\\nsequence-to-sequence recognition network, Mask TextSpotter enjoys a simple and\\nsmooth end-to-end learning procedure, in which both detection and recognition\\ncan be achieved directly from two-dimensional space via semantic segmentation.\\nFurther, a spatial attention module is proposed to enhance the performance and\\nuniversality. Benefiting from the proposed two-dimensional representation on\\nboth detection and recognition, it easily handles text instances of irregular\\nshapes, for instance, curved text. We evaluate it on four English datasets and\\none multi-language dataset, achieving consistently superior performance over\\nstate-of-the-art methods in both detection and end-to-end text recognition\\ntasks. Moreover, we further investigate the recognition module of our method\\nseparately, which significantly outperforms state-of-the-art methods on both\\nregular and irregular text datasets for scene text recognition.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "arxiv_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "796e8c0b",
      "metadata": {
        "id": "796e8c0b"
      },
      "source": [
        "# Data Cleaning and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f6ce7b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f6ce7b0",
        "outputId": "1c419435-e316-4365-e553-ca6efe0acba9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(56181, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "arxiv_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dd86c28",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dd86c28",
        "outputId": "1945cd7d-d593-47d5-a701-da751674e1b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "terms        0\n",
              "titles       0\n",
              "abstracts    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "arxiv_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efcdf227",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efcdf227",
        "outputId": "fc89dbf5-8a95-48ac-9459-b90464e23c6a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15054"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "arxiv_data.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arxiv_data[\"terms\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_pz-XvGXzSr",
        "outputId": "ca6213d8-1fe4-4cd8-9dbe-9710b247d3b6"
      },
      "id": "C_pz-XvGXzSr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                          ['cs.LG']\n",
              "1                                 ['cs.LG', 'cs.AI']\n",
              "2                      ['cs.LG', 'cs.CR', 'stat.ML']\n",
              "3                                 ['cs.LG', 'cs.CR']\n",
              "4                                          ['cs.LG']\n",
              "                            ...                     \n",
              "56176                             ['cs.CV', 'cs.IR']\n",
              "56177    ['cs.LG', 'cs.AI', 'cs.CL', 'I.2.6; I.2.7']\n",
              "56178                                      ['cs.LG']\n",
              "56179                ['stat.ML', 'cs.LG', 'math.OC']\n",
              "56180                  ['cs.LG', 'cs.AI', 'stat.ML']\n",
              "Name: terms, Length: 56181, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c14beaba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c14beaba",
        "outputId": "da3e89e9-94c1-43c5-b980-6c349cf018a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels : ['cs.LG' 'cs.AI' 'cs.CR' ... 'D.1.3; G.4; I.2.8; I.2.11; I.5.3; J.3'\n",
            " '68T07, 68T45, 68T10, 68T50, 68U35' 'I.2.0; G.3']\n",
            "lenght : 1177\n"
          ]
        }
      ],
      "source": [
        "# getting unique labels\n",
        "labels_column = arxiv_data[\"terms\"].apply(literal_eval)\n",
        "labels = labels_column.explode().unique()\n",
        "print(\"labels :\",labels)\n",
        "print(\"lenght :\",len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d52c4165",
      "metadata": {
        "id": "d52c4165",
        "outputId": "7e6fd69a-1d13-4466-de36-90730fbb48f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 41105 rows in the deduplicated dataset.\n",
            "2503\n",
            "3401\n"
          ]
        }
      ],
      "source": [
        "# remove duplicate entries based on the \"titles\" (terms) column\n",
        "# This filters the DataFrame, keeping only the rows where the titles are not duplicated.\n",
        "arxiv_data = arxiv_data[~arxiv_data['titles'].duplicated()]\n",
        "print(f\"There are {len(arxiv_data)} rows in the deduplicated dataset.\")\n",
        "# There are some terms with occurrence as low as 1.\n",
        "print(sum(arxiv_data['terms'].value_counts()==1))\n",
        "# how many unique terms\n",
        "print(arxiv_data['terms'].nunique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5883bdf4",
      "metadata": {
        "id": "5883bdf4",
        "outputId": "25d9ed38-05c2-406a-d349-c89a6eeeb1f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(38602, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "# Filtering the rare terms. (it keeps only those rows where the \"terms\" value occurs more than once in the original DataFrame.)\n",
        "arxiv_data_filtered = arxiv_data.groupby('terms').filter(lambda x: len(x) > 1)\n",
        "arxiv_data_filtered.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8e35864",
      "metadata": {
        "id": "a8e35864",
        "outputId": "5c77abe2-cee5-48c9-a2ee-379af1ecfec0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list(['cs.LG']), list(['cs.LG', 'cs.AI']),\n",
              "       list(['cs.LG', 'cs.CR', 'stat.ML'])], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "# It evaluates the given string containing a Python literal or container display (e.g., a list or dictionary) and returns the corresponding Python object.\n",
        "arxiv_data_filtered['terms'] = arxiv_data_filtered['terms'].apply(lambda x: literal_eval(x))\n",
        "arxiv_data_filtered['terms'].values[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22ba3861",
      "metadata": {
        "id": "22ba3861"
      },
      "source": [
        "# train and test split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd52b83c",
      "metadata": {
        "id": "fd52b83c",
        "outputId": "3535ed69-db2c-4389-ab2b-fec032811927",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in training set: 34741\n",
            "Number of rows in validation set: 1930\n",
            "Number of rows in test set: 1931\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3843            [cs.LG, stat.ML]\n",
              "49212           [cs.LG, stat.ML]\n",
              "23801                    [cs.CV]\n",
              "5627                     [cs.CV]\n",
              "19457                    [cs.CV]\n",
              "                  ...           \n",
              "41969                    [cs.CV]\n",
              "40731                    [cs.CV]\n",
              "16190    [cs.LG, cs.AI, stat.ML]\n",
              "48609           [stat.ML, cs.LG]\n",
              "7676       [cs.LG, cs.AI, cs.MA]\n",
              "Name: terms, Length: 34741, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "test_split = 0.1\n",
        "\n",
        "# Initial train and test split.\n",
        "# The stratify parameter ensures that the splitting is done in a way that preserves the same distribution of labels (terms) in both the training and test sets.\n",
        "train_df, test_df = train_test_split(arxiv_data_filtered,test_size=test_split,stratify=arxiv_data_filtered[\"terms\"].values,)\n",
        "\n",
        "# Splitting the test set further into validation\n",
        "# and new test sets.\n",
        "val_df = test_df.sample(frac=0.5)\n",
        "test_df.drop(val_df.index, inplace=True)\n",
        "\n",
        "print(f\"Number of rows in training set: {len(train_df)}\")\n",
        "print(f\"Number of rows in validation set: {len(val_df)}\")\n",
        "print(f\"Number of rows in test set: {len(test_df)}\")\n",
        "\n",
        "train_df[\"terms\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee848e0d",
      "metadata": {
        "id": "ee848e0d",
        "outputId": "4f7ff5d7-4dfb-40b3-f446-a146beb7aee8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:\n",
            "\n",
            "['0' '1' '10' '14f05' '14j26' '14j60' '2' '3' '4' '5' '6' '60l10' '60l20'\n",
            " '62h30' '62h35' '62h99' '65d19' '68' '68q32' '68t01' '68t05' '68t07'\n",
            " '68t10' '68t30' '68t45' '68t99' '68txx' '68u01' '68u10' '7' '8' '9' 'ai'\n",
            " 'an' 'ao' 'ap' 'app' 'ar' 'as' 'astro' 'at' 'bio' 'bm' 'cc' 'cd' 'ce'\n",
            " 'cg' 'chem' 'cl' 'class' 'co' 'comp' 'cond' 'cp' 'cr' 'cs' 'cv' 'cy'\n",
            " 'data' 'db' 'dc' 'dis' 'dl' 'dm' 'ds' 'dyn' 'e' 'ec' 'econ' 'eess' 'em'\n",
            " 'et' 'ex' 'f' 'fa' 'fin' 'fl' 'flu' 'g' 'geo' 'gn' 'gr' 'gt' 'h' 'hc'\n",
            " 'hep' 'i' 'im' 'ir' 'it' 'iv' 'j' 'k' 'lg' 'lo' 'ma' 'mat' 'math' 'me'\n",
            " 'mech' 'med' 'ml' 'mm' 'mn' 'mp' 'ms' 'mtrl' 'na' 'nc' 'ne' 'ni' 'nlin'\n",
            " 'nn' 'oc' 'optics' 'ot' 'pe' 'pf' 'ph' 'physics' 'pl' 'plasm' 'pm' 'pr'\n",
            " 'primary' 'q' 'qm' 'quant' 'rm' 'ro' 'sc' 'sci' 'sd' 'se' 'secondary'\n",
            " 'si' 'soc' 'soft' 'sp' 'st' 'stat' 'sy' 'th' 'to' 'tr']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Preprocess train_df[\"terms\"] by joining the individual strings in each list\n",
        "terms_strings = train_df[\"terms\"].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Create a CountVectorizer object\n",
        "vectorizer = CountVectorizer(binary=True, token_pattern=r'\\b\\w+\\b')\n",
        "\n",
        "# Fit the vectorizer to the preprocessed terms and transform them into a binary bag-of-words representation\n",
        "X_train = vectorizer.fit_transform(terms_strings)\n",
        "\n",
        "# Retrieve vocabulary\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(\"Vocabulary:\\n\")\n",
        "print(vocab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd32a8c9",
      "metadata": {
        "id": "bd32a8c9",
        "outputId": "e7b755f1-5dfc-44ef-d82e-e3274ac15ebb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Categories:\n",
            "[' ' '(' ')' ',' '-' '.' '0' '1' '2' '3' '4' '5' '6' '7' '8' '9' ';' 'A'\n",
            " 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S'\n",
            " 'T' 'U' 'V' 'Y' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'l' 'm' 'n' 'o' 'p'\n",
            " 'q' 'r' 's' 't' 'u' 'x' 'y']\n",
            "Label-binarized representation:\n",
            "[[1 0 0 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " ...\n",
            " [1 0 0 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('c',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " 's',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " '.',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " 'L',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " 'G',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " 's',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " 't',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " 'a',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " 't',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " '.',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " 'M',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " 'L')"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ],
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Convert list of labels to list of tuples for MultiLabelBinarizer\n",
        "label_tuples = [tuple(label) for label in train_df[\"terms\"]]\n",
        "\n",
        "# Create a MultiLabelBinarizer object\n",
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "# Fit the MultiLabelBinarizer to all labels and transform them into a binary representation\n",
        "labels_binarized = mlb.fit_transform(label_tuples)\n",
        "\n",
        "# Get the unique categories from MultiLabelBinarizer\n",
        "unique_categories = mlb.classes_\n",
        "\n",
        "# Print unique categories\n",
        "print(\"Unique Categories:\")\n",
        "print(unique_categories)\n",
        "\n",
        "# Print binarized representation\n",
        "print(\"Label-binarized representation:\")\n",
        "print(labels_binarized)\n",
        "\n",
        "label_tuples[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "120e17de",
      "metadata": {
        "id": "120e17de"
      },
      "outputs": [],
      "source": [
        "# Define parameters\n",
        "max_seqlen = 150\n",
        "batch_size = 128\n",
        "padding_token = \"<pad>\"\n",
        "\n",
        "def make_dataset(dataframe, is_train=True):\n",
        "    # Convert labels to a list of lists\n",
        "    labels = dataframe[\"terms\"].values.tolist()\n",
        "\n",
        "    # Convert labels to binarized representation\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    label_binarized = mlb.fit_transform(labels)\n",
        "\n",
        "    # Create dataset of abstracts and binarized label sequences\n",
        "    dataset = list(zip(dataframe[\"abstracts\"].values, label_binarized))\n",
        "\n",
        "    # Shuffle data for training dataset\n",
        "    if is_train:\n",
        "        np.random.shuffle(dataset)\n",
        "\n",
        "    # Batch the dataset\n",
        "    num_batches = len(dataset) // batch_size\n",
        "    if len(dataset) % batch_size != 0:\n",
        "        num_batches += 1\n",
        "    batches = [dataset[i * batch_size:(i + 1) * batch_size] for i in range(num_batches)]\n",
        "\n",
        "    return batches\n",
        "\n",
        "# Example usage:\n",
        "train_batches = make_dataset(train_df, is_train=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c36ee52",
      "metadata": {
        "id": "6c36ee52"
      },
      "outputs": [],
      "source": [
        "train_dataset = make_dataset(train_df, is_train=True)\n",
        "validation_dataset = make_dataset(val_df, is_train=False)\n",
        "test_dataset = make_dataset(test_df, is_train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d409c7bf",
      "metadata": {
        "id": "d409c7bf",
        "outputId": "37310fc8-ae47-4196-8471-fbba2fc9ace8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Meta-Sim2 proceeds by learning to sequentially sample rule expansions from a\n",
            "given probabilistic scene grammar. Due to the discrete nature of the problem,\n",
            "we use Reinforcement Learning to train our model, and design a feature space\n",
            "divergence between our synthesized and target images that is key to successful\n",
            "training. Experiments on a real driving dataset show that, without any\n",
            "supervision, we can successfully learn to generate data that captures discrete\n",
            "structural statistics of objects, such as their frequency, in real images. We\n",
            "also show that this leads to downstream improvement in the performance of an\n",
            "object detector trained on our generated dataset as opposed to other baseline\n",
            "simulation methods. Project page:\n",
            "https://nv-tlabs.github.io/meta-sim-structure/.\n",
            "Label(s): [' ', '.', 'C', 'G', 'I', 'L', 'R', 'V', 'c', 'e', 's']\n",
            " \n",
            "Abstract: Security, privacy, and fairness have become critical in the era of data\n",
            "science and machine learning. More and more we see that achieving universally\n",
            "secure, private, and fair systems is practically impossible. We have seen for\n",
            "example how generative adversarial networks can be used to learn about the\n",
            "expected private training data; how the exploitation of additional data can\n",
            "reveal private information in the original one; and how what looks like\n",
            "unrelated features can teach us about each other. Confronted with this\n",
            "challenge, in this paper we open a new line of research, where the security,\n",
            "privacy, and fairness is learned and used in a closed environment. The goal is\n",
            "to ensure that a given entity (e.g., the company or the government), trusted to\n",
            "infer certain information with our data, is blocked from inferring protected\n",
            "information from it. For example, a hospital might be allowed to produce\n",
            "diagnosis on the patient (the positive task), without being able to infer the\n",
            "gender of the subject (negative task). Similarly, a company can guarantee that\n",
            "internally it is not using the provided data for any undesired task, an\n",
            "important goal that is not contradicting the virtually impossible challenge of\n",
            "blocking everybody from the undesired task. We design a system that learns to\n",
            "succeed on the positive task while simultaneously fail at the negative one, and\n",
            "illustrate this with challenging cases where the positive task is actually\n",
            "harder than the negative one being blocked. Fairness, to the information in the\n",
            "negative task, is often automatically obtained as a result of this proposed\n",
            "approach. The particular framework and examples open the door to security,\n",
            "privacy, and fairness in very important closed scenarios, ranging from private\n",
            "data accumulation companies like social networks to law-enforcement and\n",
            "hospitals.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: We introduce PyTorch Geometric, a library for deep learning on irregularly\n",
            "structured input data such as graphs, point clouds and manifolds, built upon\n",
            "PyTorch. In addition to general graph data structures and processing methods,\n",
            "it contains a variety of recently published methods from the domains of\n",
            "relational learning and 3D data processing. PyTorch Geometric achieves high\n",
            "data throughput by leveraging sparse GPU acceleration, by providing dedicated\n",
            "CUDA kernels and by introducing efficient mini-batch handling for input\n",
            "examples of different size. In this work, we present the library in detail and\n",
            "perform a comprehensive comparative study of the implemented methods in\n",
            "homogeneous evaluation scenarios.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Registration is an important task in automated medical image analysis.\n",
            "Although deep learning (DL) based image registration methods out perform time\n",
            "consuming conventional approaches, they are heavily dependent on training data\n",
            "and do not generalize well for new images types. We present a DL based approach\n",
            "that can register an image pair which is different from the training images.\n",
            "This is achieved by training generative adversarial networks (GANs) in\n",
            "combination with segmentation information and transfer learning. Experiments on\n",
            "chest Xray and brain MR images show that our method gives better registration\n",
            "performance over conventional methods.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: The author of this work proposes an overview of the recent semi-supervised\n",
            "learning approaches and related works. Despite the remarkable success of neural\n",
            "networks in various applications, there exist few formidable constraints\n",
            "including the need for a large amount of labeled data. Therefore,\n",
            "semi-supervised learning, which is a learning scheme in which the scarce labels\n",
            "and a larger amount of unlabeled data are utilized to train models (e.g., deep\n",
            "neural networks) is getting more important. Based on the key assumptions of\n",
            "semi-supervised learning, which are the manifold assumption, cluster\n",
            "assumption, and continuity assumption, the work reviews the recent\n",
            "semi-supervised learning approaches. In particular, the methods in regard to\n",
            "using deep neural networks in a semi-supervised learning setting are primarily\n",
            "discussed. In addition, the existing works are first classified based on the\n",
            "underlying idea and explained, and then the holistic approaches that unify the\n",
            "aforementioned ideas are detailed.\n",
            "Label(s): [' ', '.', 'A', 'C', 'G', 'I', 'L', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Fine-grained location prediction on smart phones can be used to improve\n",
            "app/system performance. Application scenarios include video quality adaptation\n",
            "as a function of the 5G network quality at predicted user locations, and\n",
            "augmented reality apps that speed up content rendering based on predicted user\n",
            "locations. Such use cases require prediction error in the same range as the GPS\n",
            "error, and no existing works on location prediction can achieve this level of\n",
            "accuracy. We present a system for fine-grained location prediction (FGLP) of\n",
            "mobile users, based on GPS traces collected on the phones. FGLP has two\n",
            "components: a federated learning framework and a prediction model. The\n",
            "framework runs on the phones of the users and also on a server that coordinates\n",
            "learning from all users in the system. FGLP represents the user location data\n",
            "as relative points in an abstract 2D space, which enables learning across\n",
            "different physical spaces. The model merges Bidirectional Long Short-Term\n",
            "Memory (BiLSTM) and Convolutional Neural Networks (CNN), where BiLSTM learns\n",
            "the speed and direction of the mobile users, and CNN learns information such as\n",
            "user movement preferences. FGLP uses federated learning to protect user privacy\n",
            "and reduce bandwidth consumption. Our experimental results, using a dataset\n",
            "with over 600,000 users, demonstrate that FGLP outperforms baseline models in\n",
            "terms of prediction accuracy. We also demonstrate that FGLP works well in\n",
            "conjunction with transfer learning, which enables model reusability. Finally,\n",
            "benchmark results on several types of Android phones demonstrate FGLP's\n",
            "feasibility in real life.\n",
            "Label(s): [' ', '.', 'G', 'L', 'S', 'Y', 'c', 'e', 's']\n",
            " \n",
            "Abstract: Multivariate time series forecasting is an important machine learning problem\n",
            "across many domains, including predictions of solar plant energy output,\n",
            "electricity consumption, and traffic jam situation. Temporal data arise in\n",
            "these real-world applications often involves a mixture of long-term and\n",
            "short-term patterns, for which traditional approaches such as Autoregressive\n",
            "models and Gaussian Process may fail. In this paper, we proposed a novel deep\n",
            "learning framework, namely Long- and Short-term Time-series network (LSTNet),\n",
            "to address this open challenge. LSTNet uses the Convolution Neural Network\n",
            "(CNN) and the Recurrent Neural Network (RNN) to extract short-term local\n",
            "dependency patterns among variables and to discover long-term patterns for time\n",
            "series trends. Furthermore, we leverage traditional autoregressive model to\n",
            "tackle the scale insensitive problem of the neural network model. In our\n",
            "evaluation on real-world data with complex mixtures of repetitive patterns,\n",
            "LSTNet achieved significant performance improvements over that of several\n",
            "state-of-the-art baseline methods. All the data and experiment codes are\n",
            "available online.\n",
            "Label(s): [' ', '.', 'G', 'L', 'c', 's']\n",
            " \n",
            "Abstract: Digitally retouching images has become a popular trend, with people posting\n",
            "altered images on social media and even magazines posting flawless facial\n",
            "images of celebrities. Further, with advancements in Generative Adversarial\n",
            "Networks (GANs), now changing attributes and retouching have become very easy.\n",
            "Such synthetic alterations have adverse effect on face recognition algorithms.\n",
            "While researchers have proposed to detect image tampering, detecting GANs\n",
            "generated images has still not been explored. This paper proposes a supervised\n",
            "deep learning algorithm using Convolutional Neural Networks (CNNs) to detect\n",
            "synthetically altered images. The algorithm yields an accuracy of 99.65% on\n",
            "detecting retouching on the ND-IIITD dataset. It outperforms the previous state\n",
            "of the art which reported an accuracy of 87% on the database. For\n",
            "distinguishing between real images and images generated using GANs, the\n",
            "proposed algorithm yields an accuracy of 99.83%.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Basing on the analysis by revealing the equivalence of modern networks, we\n",
            "find that both ResNet and DenseNet are essentially derived from the same \"dense\n",
            "topology\", yet they only differ in the form of connection -- addition (dubbed\n",
            "\"inner link\") vs. concatenation (dubbed \"outer link\"). However, both two forms\n",
            "of connections have the superiority and insufficiency. To combine their\n",
            "advantages and avoid certain limitations on representation learning, we present\n",
            "a highly efficient and modularized Mixed Link Network (MixNet) which is\n",
            "equipped with flexible inner link and outer link modules. Consequently, ResNet,\n",
            "DenseNet and Dual Path Network (DPN) can be regarded as a special case of\n",
            "MixNet, respectively. Furthermore, we demonstrate that MixNets can achieve\n",
            "superior efficiency in parameter over the state-of-the-art architectures on\n",
            "many competitive datasets like CIFAR-10/100, SVHN and ImageNet.\n",
            "Label(s): [' ', '.', 'G', 'L', 'c', 's']\n",
            " \n",
            "Abstract: 3D multi-object tracking (MOT) and trajectory forecasting are two critical\n",
            "components in modern 3D perception systems. We hypothesize that it is\n",
            "beneficial to unify both tasks under one framework to learn a shared feature\n",
            "representation of agent interaction. To evaluate this hypothesis, we propose a\n",
            "unified solution for 3D MOT and trajectory forecasting which also incorporates\n",
            "two additional novel computational units. First, we employ a feature\n",
            "interaction technique by introducing Graph Neural Networks (GNNs) to capture\n",
            "the way in which multiple agents interact with one another. The GNN is able to\n",
            "model complex hierarchical interactions, improve the discriminative feature\n",
            "learning for MOT association, and provide socially-aware context for trajectory\n",
            "forecasting. Second, we use a diversity sampling function to improve the\n",
            "quality and diversity of our forecasted trajectories. The learned sampling\n",
            "function is trained to efficiently extract a variety of outcomes from a\n",
            "generative trajectory distribution and helps avoid the problem of generating\n",
            "many duplicate trajectory samples. We show that our method achieves\n",
            "state-of-the-art performance on the KITTI dataset. Our project website is at\n",
            "http://www.xinshuoweng.com/projects/GNNTrkForecast.\n",
            "Label(s): [' ', '.', 'A', 'C', 'G', 'L', 'M', 'O', 'R', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Recent approaches for instance-aware semantic labeling have augmented\n",
            "convolutional neural networks (CNNs) with complex multi-task architectures or\n",
            "computationally expensive graphical models. We present a method that leverages\n",
            "a fully convolutional network (FCN) to predict semantic labels, depth and an\n",
            "instance-based encoding using each pixel's direction towards its corresponding\n",
            "instance center. Subsequently, we apply low-level computer vision techniques to\n",
            "generate state-of-the-art instance segmentation on the street scene datasets\n",
            "KITTI and Cityscapes. Our approach outperforms existing works by a large margin\n",
            "and can additionally predict absolute distances of individual instances from a\n",
            "monocular image as well as a pixel-level semantic labeling.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Computational food analysis (CFA), a broad set of methods that attempt to\n",
            "automate food understanding, naturally requires analysis of multi-modal\n",
            "evidence of a particular food or dish, e.g. images, recipe text, preparation\n",
            "video, nutrition labels, etc. A key to making CFA possible is multi-modal\n",
            "shared subspace learning, which in turn can be used for cross-modal retrieval\n",
            "and/or synthesis, particularly, between food images and their corresponding\n",
            "textual recipes. In this work we propose a simple yet novel architecture for\n",
            "shared subspace learning, which is used to tackle the food image-to-recipe\n",
            "retrieval problem. Our proposed method employs an effective transformer based\n",
            "multilingual recipe encoder coupled with a traditional image embedding\n",
            "architecture. Experimental analysis on the public Recipe1M dataset shows that\n",
            "the subspace learned via the proposed method outperforms the current\n",
            "state-of-the-arts (SoTA) in food retrieval by a large margin, obtaining\n",
            "recall@1 of 0.64. Furthermore, in order to demonstrate the representational\n",
            "power of the learned subspace, we propose a generative food image synthesis\n",
            "model conditioned on the embeddings of recipes. Synthesized images can\n",
            "effectively reproduce the visual appearance of paired samples, achieving R@1 of\n",
            "0.68 in the image-to-recipe retrieval experiment, thus effectively capturing\n",
            "the semantics of the textual recipe.\n",
            "Label(s): [' ', '.', 'C', 'G', 'I', 'L', 'R', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Adversarial examples are firstly investigated in the area of computer vision:\n",
            "by adding some carefully designed ''noise'' to the original input image, the\n",
            "perturbed image that cannot be distinguished from the original one by human,\n",
            "can fool a well-trained classifier easily. In recent years, researchers also\n",
            "demonstrated that adversarial examples can mislead deep reinforcement learning\n",
            "(DRL) agents on playing video games using image inputs with similar methods.\n",
            "However, although DRL has been more and more popular in the area of intelligent\n",
            "transportation systems, there is little research investigating the impacts of\n",
            "adversarial attacks on them, especially for algorithms that do not take images\n",
            "as inputs. In this work, we investigated several fast methods to generate\n",
            "adversarial examples to significantly degrade the performance of a well-trained\n",
            "DRL- based energy management system of an extended range electric delivery\n",
            "vehicle. The perturbed inputs are low-dimensional state representations and\n",
            "close to the original inputs quantified by different kinds of norms. Our work\n",
            "shows that, to apply DRL agents on real-world transportation systems,\n",
            "adversarial examples in the form of cyber-attack should be considered\n",
            "carefully, especially for applications that may lead to serious safety issues.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: This paper addresses the task of estimating the 6 degrees of freedom pose of\n",
            "a known 3D object from depth information represented by a point cloud. Deep\n",
            "features learned by convolutional neural networks from color information have\n",
            "been the dominant features to be used for inferring object poses, while depth\n",
            "information receives much less attention. However, depth information contains\n",
            "rich geometric information of the object shape, which is important for\n",
            "inferring the object pose. We use depth information represented by point clouds\n",
            "as the input to both deep networks and geometry-based pose refinement and use\n",
            "separate networks for rotation and translation regression. We argue that the\n",
            "axis-angle representation is a suitable rotation representation for deep\n",
            "learning, and use a geodesic loss function for rotation regression. Ablation\n",
            "studies show that these design choices outperform alternatives such as the\n",
            "quaternion representation and L2 loss, or regressing translation and rotation\n",
            "with the same network. Our simple yet effective approach clearly outperforms\n",
            "state-of-the-art methods on the YCB-video dataset. The implementation and\n",
            "trained model are avaliable at: https://github.com/GeeeG/CloudPose.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Reinforcement learning is showing great potentials in robotics applications,\n",
            "including autonomous driving, robot manipulation and locomotion. However, with\n",
            "complex uncertainties in the real-world environment, it is difficult to\n",
            "guarantee the successful generalization and sim-to-real transfer of learned\n",
            "policies theoretically. In this paper, we introduce and extend the idea of\n",
            "robust stability and $H_\\infty$ control to design policies with both stability\n",
            "and robustness guarantee. Specifically, a sample-based approach for analyzing\n",
            "the Lyapunov stability and performance robustness of a learning-based control\n",
            "system is proposed. Based on the theoretical results, a maximum entropy\n",
            "algorithm is developed for searching Lyapunov function and designing a policy\n",
            "with provable robust stability guarantee. Without any specific domain\n",
            "knowledge, our method can find a policy that is robust to various uncertainties\n",
            "and generalizes well to different test environments. In our experiments, we\n",
            "show that our method achieves better robustness to both large impulsive\n",
            "disturbances and parametric variations in the environment than the state-of-art\n",
            "results in both robust and generic RL, as well as classic control. Anonymous\n",
            "code is available to reproduce the experimental results at\n",
            "https://github.com/RobustStabilityGuaranteeRL/RobustStabilityGuaranteeRL.\n",
            "Label(s): [' ', '.', 'G', 'L', 'O', 'R', 'S', 'Y', 'c', 'e', 's']\n",
            " \n",
            "Abstract: Data-driven design is making headway into a number of application areas,\n",
            "including protein, small-molecule, and materials engineering. The design goal\n",
            "is to construct an object with desired properties, such as a protein that binds\n",
            "to a therapeutic target, or a superconducting material with a higher critical\n",
            "temperature than previously observed. To that end, costly experimental\n",
            "measurements are being replaced with calls to high-capacity regression models\n",
            "trained on labeled data, which can be leveraged in an in silico search for\n",
            "design candidates. However, the design goal necessitates moving into regions of\n",
            "the design space beyond where such models were trained. Therefore, one can ask:\n",
            "should the regression model be altered as the design algorithm explores the\n",
            "design space, in the absence of new data? Herein, we answer this question in\n",
            "the affirmative. In particular, we (i) formalize the data-driven design problem\n",
            "as a non-zero-sum game, (ii) develop a principled strategy for retraining the\n",
            "regression model as the design algorithm proceeds---what we refer to as\n",
            "autofocusing, and (iii) demonstrate the promise of autofocusing empirically.\n",
            "Label(s): [' ', '-', '.', 'G', 'L', 'M', 'Q', 'a', 'b', 'c', 'i', 'o', 'q', 's', 't']\n",
            " \n",
            "Abstract: Shadow detection and shadow removal are fundamental and challenging tasks,\n",
            "requiring an understanding of the global image semantics. This paper presents a\n",
            "novel deep neural network design for shadow detection and removal by analyzing\n",
            "the spatial image context in a direction-aware manner. To achieve this, we\n",
            "first formulate the direction-aware attention mechanism in a spatial recurrent\n",
            "neural network (RNN) by introducing attention weights when aggregating spatial\n",
            "context features in the RNN. By learning these weights through training, we can\n",
            "recover direction-aware spatial context (DSC) for detecting and removing\n",
            "shadows. This design is developed into the DSC module and embedded in a\n",
            "convolutional neural network (CNN) to learn the DSC features at different\n",
            "levels. Moreover, we design a weighted cross entropy loss to make effective the\n",
            "training for shadow detection and further adopt the network for shadow removal\n",
            "by using a Euclidean loss function and formulating a color transfer function to\n",
            "address the color and luminosity inconsistencies in the training pairs. We\n",
            "employed two shadow detection benchmark datasets and two shadow removal\n",
            "benchmark datasets, and performed various experiments to evaluate our method.\n",
            "Experimental results show that our method performs favorably against the\n",
            "state-of-the-art methods for both shadow detection and shadow removal.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: We introduce the vine copula autoencoder (VCAE), a flexible generative model\n",
            "for high-dimensional distributions built in a straightforward three-step\n",
            "procedure.\n",
            "  First, an autoencoder (AE) compresses the data into a lower dimensional\n",
            "representation. Second, the multivariate distribution of the encoded data is\n",
            "estimated with vine copulas. Third, a generative model is obtained by combining\n",
            "the estimated distribution with the decoder part of the AE. As such, the\n",
            "proposed approach can transform any already trained AE into a flexible\n",
            "generative model at a low computational cost. This is an advantage over\n",
            "existing generative models such as adversarial networks and variational AEs\n",
            "which can be difficult to train and can impose strong assumptions on the latent\n",
            "space. Experiments on MNIST, Street View House Numbers and Large-Scale\n",
            "CelebFaces Attributes datasets show that VCAEs can achieve competitive results\n",
            "to standard baselines.\n",
            "Label(s): [' ', '.', 'C', 'G', 'L', 'M', 'V', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Many solutions to cost-sensitive classification (and regression) rely on some\n",
            "or all of the following assumptions: we have complete knowledge about the cost\n",
            "context at training time, we can easily re-train whenever the cost context\n",
            "changes, and we have technique-specific methods (such as cost-sensitive\n",
            "decision trees) that can take advantage of that information. In this paper we\n",
            "address the problem of selecting models and minimising joint cost (integrating\n",
            "both misclassification cost and test costs) without any of the above\n",
            "assumptions. We introduce methods and plots (such as the so-called JROC plots)\n",
            "that can work with any off-the-shelf predictive technique, including ensembles,\n",
            "such that we reframe the model to use the appropriate subset of attributes (the\n",
            "feature configuration) during deployment time. In other words, models are\n",
            "trained with the available attributes (once and for all) and then deployed by\n",
            "setting missing values on the attributes that are deemed ineffective for\n",
            "reducing the joint cost. As the number of feature configuration combinations\n",
            "grows exponentially with the number of features we introduce quadratic methods\n",
            "that are able to approximate the optimal configuration and model choices, as\n",
            "shown by the experimental results.\n",
            "Label(s): [' ', '.', 'G', 'L', 'c', 's']\n",
            " \n",
            "Abstract: We investigate the problem of learning a probabilistic distribution over\n",
            "three-dimensional shapes given two-dimensional views of multiple objects taken\n",
            "from unknown viewpoints. Our approach called projective generative adversarial\n",
            "network (PrGAN) trains a deep generative model of 3D shapes whose projections\n",
            "(or renderings) match the distributions of the provided 2D distribution. The\n",
            "addition of a differentiable projection module allows us to infer the\n",
            "underlying 3D shape distribution without access to any explicit 3D or viewpoint\n",
            "annotation during the learning phase. We show that our approach produces 3D\n",
            "shapes of comparable quality to GANs trained directly on 3D data. %for a number\n",
            "of shape categoriesincluding chairs, airplanes, and cars. Experiments also show\n",
            "that the disentangled representation of 2D shapes into geometry and viewpoint\n",
            "leads to a good generative model of 2D shapes. The key advantage of our model\n",
            "is that it estimates 3D shape, viewpoint, and generates novel views from an\n",
            "input image in a completely unsupervised manner. We further investigate how the\n",
            "generative models can be improved if additional information such as depth,\n",
            "viewpoint or part segmentations is available at training time. To this end, we\n",
            "present new differentiable projection operators that can be used by PrGAN to\n",
            "learn better 3D generative models. Our experiments show that our method can\n",
            "successfully leverage extra visual cues to create more diverse and accurate\n",
            "shapes.\n",
            "Label(s): [' ', '.', 'C', 'G', 'L', 'R', 'V', 'c', 's']\n",
            " \n",
            "Abstract: We address the challenging problem of generating facial attributes using a\n",
            "single image in an unconstrained pose. In contrast to prior works that largely\n",
            "consider generation on 2D near-frontal images, we propose a GAN-based framework\n",
            "to generate attributes directly on a dense 3D representation given by UV\n",
            "texture and position maps, resulting in photorealistic,\n",
            "geometrically-consistent and identity-preserving outputs. Starting from a\n",
            "self-occluded UV texture map obtained by applying an off-the-shelf 3D\n",
            "reconstruction method, we propose two novel components. First, a texture\n",
            "completion generative adversarial network (TC-GAN) completes the partial UV\n",
            "texture map. Second, a 3D attribute generation GAN (3DA-GAN) synthesizes the\n",
            "target attribute while obtaining an appearance consistent with 3D face geometry\n",
            "and preserving identity. Extensive experiments on CelebA, LFW and IJB-A show\n",
            "that our method achieves consistently better attribute generation accuracy than\n",
            "prior methods, a higher degree of qualitative photorealism and preserves face\n",
            "identity information.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Video activity localisation has recently attained increasing attention due to\n",
            "its practical values in automatically localising the most salient visual\n",
            "segments corresponding to their language descriptions (sentences) from\n",
            "untrimmed and unstructured videos. For supervised model training, a temporal\n",
            "annotation of both the start and end time index of each video segment for a\n",
            "sentence (a video moment) must be given. This is not only very expensive but\n",
            "also sensitive to ambiguity and subjective annotation bias, a much harder task\n",
            "than image labelling. In this work, we develop a more accurate\n",
            "weakly-supervised solution by introducing Cross-Sentence Relations Mining (CRM)\n",
            "in video moment proposal generation and matching when only a paragraph\n",
            "description of activities without per-sentence temporal annotation is\n",
            "available. Specifically, we explore two cross-sentence relational constraints:\n",
            "(1) Temporal ordering and (2) semantic consistency among sentences in a\n",
            "paragraph description of video activities. Existing weakly-supervised\n",
            "techniques only consider within-sentence video segment correlations in training\n",
            "without considering cross-sentence paragraph context. This can mislead due to\n",
            "ambiguous expressions of individual sentences with visually indiscriminate\n",
            "video moment proposals in isolation. Experiments on two publicly available\n",
            "activity localisation datasets show the advantages of our approach over the\n",
            "state-of-the-art weakly supervised methods, especially so when the video\n",
            "activity descriptions become more complex.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: We introduce Pixel-aligned Implicit Function (PIFu), a highly effective\n",
            "implicit representation that locally aligns pixels of 2D images with the global\n",
            "context of their corresponding 3D object. Using PIFu, we propose an end-to-end\n",
            "deep learning method for digitizing highly detailed clothed humans that can\n",
            "infer both 3D surface and texture from a single image, and optionally, multiple\n",
            "input images. Highly intricate shapes, such as hairstyles, clothing, as well as\n",
            "their variations and deformations can be digitized in a unified way. Compared\n",
            "to existing representations used for 3D deep learning, PIFu can produce\n",
            "high-resolution surfaces including largely unseen regions such as the back of a\n",
            "person. In particular, it is memory efficient unlike the voxel representation,\n",
            "can handle arbitrary topology, and the resulting surface is spatially aligned\n",
            "with the input image. Furthermore, while previous techniques are designed to\n",
            "process either a single image or multiple views, PIFu extends naturally to\n",
            "arbitrary number of views. We demonstrate high-resolution and robust\n",
            "reconstructions on real world images from the DeepFashion dataset, which\n",
            "contains a variety of challenging clothing types. Our method achieves\n",
            "state-of-the-art performance on a public benchmark and outperforms the prior\n",
            "work for clothed human digitization from a single image.\n",
            "Label(s): [' ', '.', 'C', 'G', 'R', 'V', 'c', 's']\n",
            " \n",
            "Abstract: In the current context of Big Data, the nature of many forecasting problems\n",
            "has changed from predicting isolated time series to predicting many time series\n",
            "from similar sources. This has opened up the opportunity to develop competitive\n",
            "global forecasting models that simultaneously learn from many time series. But,\n",
            "it still remains unclear when global forecasting models can outperform the\n",
            "univariate benchmarks, especially along the dimensions of the\n",
            "homogeneity/heterogeneity of series, the complexity of patterns in the series,\n",
            "the complexity of forecasting models, and the lengths/number of series. Our\n",
            "study attempts to address this problem through investigating the effect from\n",
            "these factors, by simulating a number of datasets that have controllable time\n",
            "series characteristics. Specifically, we simulate time series from simple data\n",
            "generating processes (DGP), such as Auto Regressive (AR) and Seasonal AR, to\n",
            "complex DGPs, such as Chaotic Logistic Map, Self-Exciting Threshold\n",
            "Auto-Regressive, and Mackey-Glass Equations. The data heterogeneity is\n",
            "introduced by mixing time series generated from several DGPs into a single\n",
            "dataset. The lengths and the number of series in the dataset are varied in\n",
            "different scenarios. We perform experiments on these datasets using global\n",
            "forecasting models including Recurrent Neural Networks (RNN), Feed-Forward\n",
            "Neural Networks, Pooled Regression (PR) models and Light Gradient Boosting\n",
            "Models (LGBM), and compare their performance against standard statistical\n",
            "univariate forecasting techniques. Our experiments demonstrate that when\n",
            "trained as global forecasting models, techniques such as RNNs and LGBMs, which\n",
            "have complex non-linear modelling capabilities, are competitive methods in\n",
            "general under challenging forecasting scenarios such as series having short\n",
            "lengths, datasets with heterogeneous series and having minimal prior knowledge\n",
            "of the patterns of the series.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Paucity of large curated hand-labeled training data for every\n",
            "domain-of-interest forms a major bottleneck in the deployment of machine\n",
            "learning models in computer vision and other fields. Recent work (Data\n",
            "Programming) has shown how distant supervision signals in the form of labeling\n",
            "functions can be used to obtain labels for given data in near-constant time. In\n",
            "this work, we present Adversarial Data Programming (ADP), which presents an\n",
            "adversarial methodology to generate data as well as a curated aggregated label\n",
            "has given a set of weak labeling functions. We validated our method on the\n",
            "MNIST, Fashion MNIST, CIFAR 10 and SVHN datasets, and it outperformed many\n",
            "state-of-the-art models. We conducted extensive experiments to study its\n",
            "usefulness, as well as showed how the proposed ADP framework can be used for\n",
            "transfer learning as well as multi-task learning, where data from two domains\n",
            "are generated simultaneously using the framework along with the label\n",
            "information. Our future work will involve understanding the theoretical\n",
            "implications of this new framework from a game-theoretic perspective, as well\n",
            "as explore the performance of the method on more complex datasets.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: We tackle unsupervised anomaly detection (UAD), a problem of detecting data\n",
            "that significantly differ from normal data. UAD is typically solved by using\n",
            "density estimation. Recently, deep neural network (DNN)-based density\n",
            "estimators, such as Normalizing Flows, have been attracting attention. However,\n",
            "one of their drawbacks is the difficulty in adapting them to the change in the\n",
            "normal data's distribution. To address this difficulty, we propose AdaFlow, a\n",
            "new DNN-based density estimator that can be easily adapted to the change of the\n",
            "distribution. AdaFlow is a unified model of a Normalizing Flow and Adaptive\n",
            "Batch-Normalizations, a module that enables DNNs to adapt to new distributions.\n",
            "AdaFlow can be adapted to a new distribution by just conducting forward\n",
            "propagation once per sample; hence, it can be used on devices that have limited\n",
            "computational resources. We have confirmed the effectiveness of the proposed\n",
            "model through an anomaly detection in a sound task. We also propose a method of\n",
            "applying AdaFlow to the unpaired cross-domain translation problem, in which one\n",
            "has to train a cross-domain translation model with only unpaired samples. We\n",
            "have confirmed that our model can be used for the cross-domain translation\n",
            "problem through experiments on image datasets.\n",
            "Label(s): [' ', '.', 'A', 'D', 'G', 'L', 'M', 'S', 'a', 'c', 'e', 's', 't']\n",
            " \n",
            "Abstract: Salient object detection in complex scenes and environments is a challenging\n",
            "research topic. Most works focus on RGB-based salient object detection, which\n",
            "limits its performance of real-life applications when confronted with adverse\n",
            "conditions such as dark environments and complex backgrounds. Taking advantage\n",
            "of RGB and thermal infrared images becomes a new research direction for\n",
            "detecting salient object in complex scenes recently, as thermal infrared\n",
            "spectrum imaging provides the complementary information and has been applied to\n",
            "many computer vision tasks. However, current research for RGBT salient object\n",
            "detection is limited by the lack of a large-scale dataset and comprehensive\n",
            "benchmark. This work contributes such a RGBT image dataset named VT5000,\n",
            "including 5000 spatially aligned RGBT image pairs with ground truth\n",
            "annotations. VT5000 has 11 challenges collected in different scenes and\n",
            "environments for exploring the robustness of algorithms. With this dataset, we\n",
            "propose a powerful baseline approach, which extracts multi-level features\n",
            "within each modality and aggregates these features of all modalities with the\n",
            "attention mechanism, for accurate RGBT salient object detection. Extensive\n",
            "experiments show that the proposed baseline approach outperforms the\n",
            "state-of-the-art methods on VT5000 dataset and other two public datasets. In\n",
            "addition, we carry out a comprehensive analysis of different algorithms of RGBT\n",
            "salient object detection on VT5000 dataset, and then make several valuable\n",
            "conclusions and provide some potential research directions for RGBT salient\n",
            "object detection.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Event cameras, i.e., the Dynamic and Active-pixel Vision Sensor (DAVIS) ones,\n",
            "capture the intensity changes in the scene and generates a stream of events in\n",
            "an asynchronous fashion. The output rate of such cameras can reach up to 10\n",
            "million events per second in high dynamic environments. DAVIS cameras use novel\n",
            "vision sensors that mimic human eyes. Their attractive attributes, such as high\n",
            "output rate, High Dynamic Range (HDR), and high pixel bandwidth, make them an\n",
            "ideal solution for applications that require high-frequency tracking. Moreover,\n",
            "applications that operate in challenging lighting scenarios can exploit the\n",
            "high HDR of event cameras, i.e., 140 dB compared to 60 dB of traditional\n",
            "cameras. In this paper, a novel asynchronous corner tracking method is proposed\n",
            "that uses both events and intensity images captured by a DAVIS camera. The\n",
            "Harris algorithm is used to extract features, i.e., frame-corners from\n",
            "keyframes, i.e., intensity images. Afterward, a matching algorithm is used to\n",
            "extract event-corners from the stream of events. Events are solely used to\n",
            "perform asynchronous tracking until the next keyframe is captured. Neighboring\n",
            "events, within a window size of 5x5 pixels around the event-corner, are used to\n",
            "calculate the velocity and direction of extracted event-corners by fitting the\n",
            "2D planar using a randomized Hough transform algorithm. Experimental evaluation\n",
            "showed that our approach is able to update the location of the extracted\n",
            "corners up to 100 times during the blind time of traditional cameras, i.e.,\n",
            "between two consecutive intensity images.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: The remarkable performance of deep neural networks depends on the\n",
            "availability of massive labeled data. To alleviate the load of data annotation,\n",
            "active deep learning aims to select a minimal set of training points to be\n",
            "labelled which yields maximal model accuracy. Most existing approaches\n",
            "implement either an `exploration'-type selection criterion, which aims at\n",
            "exploring the joint distribution of data and labels, or a `refinement'-type\n",
            "criterion which aims at localizing the detected decision boundaries. We propose\n",
            "a versatile and efficient criterion that automatically switches from\n",
            "exploration to refinement when the distribution has been sufficiently mapped.\n",
            "Our criterion relies on a process of diffusing the existing label information\n",
            "over a graph constructed from the hidden representation of the data set as\n",
            "provided by the neural network. This graph representation captures the\n",
            "intrinsic geometry of the approximated labeling function. The diffusion-based\n",
            "criterion is shown to be advantageous as it outperforms existing criteria for\n",
            "deep active learning.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: The pressure of ever-increasing patient demand and budget restrictions make\n",
            "hospital bed management a daily challenge for clinical staff. Most critical is\n",
            "the efficient allocation of resource-heavy Intensive Care Unit (ICU) beds to\n",
            "the patients who need life support. Central to solving this problem is knowing\n",
            "for how long the current set of ICU patients are likely to stay in the unit. In\n",
            "this work, we propose a new deep learning model based on the combination of\n",
            "temporal convolution and pointwise (1x1) convolution, to solve the length of\n",
            "stay prediction task on the eICU critical care dataset. The model - which we\n",
            "refer to as Temporal Pointwise Convolution (TPC) - is specifically designed to\n",
            "mitigate for common challenges with Electronic Health Records, such as\n",
            "skewness, irregular sampling and missing data. In doing so, we have achieved\n",
            "significant performance benefits of 18-51% (metric dependent) over the commonly\n",
            "used Long-Short Term Memory (LSTM) network, and the multi-head self-attention\n",
            "network known as the Transformer.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Despite increasing efforts on universal representations for visual\n",
            "recognition, few have addressed object detection. In this paper, we develop an\n",
            "effective and efficient universal object detection system that is capable of\n",
            "working on various image domains, from human faces and traffic signs to medical\n",
            "CT images. Unlike multi-domain models, this universal model does not require\n",
            "prior knowledge of the domain of interest. This is achieved by the introduction\n",
            "of a new family of adaptation layers, based on the principles of squeeze and\n",
            "excitation, and a new domain-attention mechanism. In the proposed universal\n",
            "detector, all parameters and computations are shared across domains, and a\n",
            "single network processes all domains all the time. Experiments, on a newly\n",
            "established universal object detection benchmark of 11 diverse datasets, show\n",
            "that the proposed detector outperforms a bank of individual detectors, a\n",
            "multi-domain detector, and a baseline universal detector, with a 1.3x parameter\n",
            "increase over a single-domain baseline detector. The code and benchmark will be\n",
            "released at http://www.svcl.ucsd.edu/projects/universal-detection/.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: The shapes and morphology of the organs and tissues are important prior\n",
            "knowledge in medical imaging recognition and segmentation. The morphological\n",
            "operation is a well-known method for morphological feature extraction. As the\n",
            "morphological operation is performed well in hand-crafted image segmentation\n",
            "techniques, it is also promising to design an approach to approximate\n",
            "morphological operation in the convolutional networks. However, using the\n",
            "traditional convolutional neural network as a black-box is usually hard to\n",
            "specify the morphological operation action. Here, we introduced a 3D\n",
            "morphological operation residual block to extract morphological features in\n",
            "end-to-end deep learning models for semantic segmentation. This study proposed\n",
            "a novel network block architecture that embedded the morphological operation as\n",
            "an infinitely strong prior in the convolutional neural network. Several 3D deep\n",
            "learning models with the proposed morphological operation block were built and\n",
            "compared in different medical imaging segmentation tasks. Experimental results\n",
            "showed the proposed network achieved a relatively higher performance in the\n",
            "segmentation tasks comparing with the conventional approach. In conclusion, the\n",
            "novel network block could be easily embedded in traditional networks and\n",
            "efficiently reinforce the deep learning models for medical imaging\n",
            "segmentation.\n",
            "Label(s): [' ', '.', 'C', 'G', 'I', 'L', 'V', 'c', 'e', 's']\n",
            " \n",
            "Abstract: Recent years have seen the vast potential of Graph Neural Networks (GNN) in\n",
            "many fields where data is structured as graphs (e.g., chemistry, recommender\n",
            "systems). In particular, GNNs are becoming increasingly popular in the field of\n",
            "networking, as graphs are intrinsically present at many levels (e.g., topology,\n",
            "routing). The main novelty of GNNs is their ability to generalize to other\n",
            "networks unseen during training, which is an essential feature for developing\n",
            "practical Machine Learning (ML) solutions for networking. However, implementing\n",
            "a functional GNN prototype is currently a cumbersome task that requires strong\n",
            "skills in neural network programming. This poses an important barrier to\n",
            "network engineers that often do not have the necessary ML expertise. In this\n",
            "article, we present IGNNITION, a novel open-source framework that enables fast\n",
            "prototyping of GNNs for networking systems. IGNNITION is based on an intuitive\n",
            "high-level abstraction that hides the complexity behind GNNs, while still\n",
            "offering great flexibility to build custom GNN architectures. To showcase the\n",
            "versatility and performance of this framework, we implement two\n",
            "state-of-the-art GNN models applied to different networking use cases. Our\n",
            "results show that the GNN models produced by IGNNITION are equivalent in terms\n",
            "of accuracy and performance to their native implementations in TensorFlow.\n",
            "Label(s): [' ', '.', 'A', 'G', 'I', 'L', 'N', 'P', 'S', 'c', 'e', 's']\n",
            " \n",
            "Abstract: Microsoft's internal big data analytics platform is comprised of hundreds of\n",
            "thousands of machines, serving over half a million jobs daily, from thousands\n",
            "of users. The majority of these jobs are recurring and are crucial for the\n",
            "company's operation. Although administrators spend significant effort tuning\n",
            "system performance, some jobs inevitably experience slowdowns, i.e., their\n",
            "execution time degrades over previous runs. Currently, the investigation of\n",
            "such slowdowns is a labor-intensive and error-prone process, which costs\n",
            "Microsoft significant human and machine resources, and negatively impacts\n",
            "several lines of businesses. In this work, we present Griffin, a system we\n",
            "built and have deployed in production last year to automatically discover the\n",
            "root cause of job slowdowns. Existing solutions either rely on labeled data\n",
            "(i.e., resolved incidents with labeled reasons for job slowdowns), which is in\n",
            "most cases non-existent or non-trivial to acquire, or on time-series analysis\n",
            "of individual metrics that do not target specific jobs holistically. In\n",
            "contrast, in Griffin we cast the problem to a corresponding regression one that\n",
            "predicts the runtime of a job, and show how the relative contributions of the\n",
            "features used to train our interpretable model can be exploited to rank the\n",
            "potential causes of job slowdowns. Evaluated over historical incidents, we show\n",
            "that Griffin discovers slowdown causes that are consistent with the ones\n",
            "validated by domain-expert engineers, in a fraction of the time required by\n",
            "them.\n",
            "Label(s): [' ', '.', 'C', 'D', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Abnormal event detection (AED) in urban surveillance videos has multiple\n",
            "challenges. Unlike other computer vision problems, the AED is not solely\n",
            "dependent on the content of frames. It also depends on the appearance of the\n",
            "objects and their movements in the scene. Various methods have been proposed to\n",
            "address the AED problem. Among those, deep learning based methods show the best\n",
            "results. This paper is based on deep learning methods and provides an effective\n",
            "way to detect and locate abnormal events in videos by handling spatio temporal\n",
            "data. This paper uses generative adversarial networks (GANs) and performs\n",
            "transfer learning algorithms on pre trained convolutional neural network (CNN)\n",
            "which result in an accurate and efficient model. The efficiency of the model is\n",
            "further improved by processing the optical flow information of the video. This\n",
            "paper runs experiments on two benchmark datasets for AED problem (UCSD Peds1\n",
            "and UCSD Peds2) and compares the results with other previous methods. The\n",
            "comparisons are based on various criteria such as area under curve (AUC) and\n",
            "true positive rate (TPR). Experimental results show that the proposed method\n",
            "can effectively detect and locate abnormal events in crowd scenes.\n",
            "Label(s): [' ', '.', 'C', 'G', 'L', 'V', 'c', 's']\n",
            " \n",
            "Abstract: This paper presents an efficient annotation procedure and an application\n",
            "thereof to end-to-end, rich semantic segmentation of the sensed environment\n",
            "using FMCW scanning radar. We advocate radar over the traditional sensors used\n",
            "for this task as it operates at longer ranges and is substantially more robust\n",
            "to adverse weather and illumination conditions. We avoid laborious manual\n",
            "labelling by exploiting the largest radar-focused urban autonomy dataset\n",
            "collected to date, correlating radar scans with RGB cameras and LiDAR sensors,\n",
            "for which semantic segmentation is an already consolidated procedure. The\n",
            "training procedure leverages a state-of-the-art natural image segmentation\n",
            "system which is publicly available and as such, in contrast to previous\n",
            "approaches, allows for the production of copious labels for the radar stream by\n",
            "incorporating four camera and two LiDAR streams. Additionally, the losses are\n",
            "computed taking into account labels to the radar sensor horizon by accumulating\n",
            "LiDAR returns along a pose-chain ahead and behind of the current vehicle\n",
            "position. Finally, we present the network with multi-channel radar scan inputs\n",
            "in order to deal with ephemeral and dynamic scene objects.\n",
            "Label(s): [' ', '.', 'C', 'O', 'R', 'V', 'c', 's']\n",
            " \n",
            "Abstract: A good clustering algorithm can discover natural groupings in data. These\n",
            "groupings, if used wisely, provide a form of weak supervision for learning\n",
            "representations. In this work, we present Clustering-based Contrastive Learning\n",
            "(CCL), a new clustering-based representation learning approach that uses labels\n",
            "obtained from clustering along with video constraints to learn discriminative\n",
            "face features. We demonstrate our method on the challenging task of learning\n",
            "representations for video face clustering. Through several ablation studies, we\n",
            "analyze the impact of creating pair-wise positive and negative labels from\n",
            "different sources. Experiments on three challenging video face clustering\n",
            "datasets: BBT-0101, BF-0502, and ACCIO show that CCL achieves a new\n",
            "state-of-the-art on all datasets.\n",
            "Label(s): [' ', '.', 'C', 'G', 'L', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Unmanned Aerial Systems (UAS) are being increasingly deployed for commercial,\n",
            "civilian, and military applications. The current UAS state-of-the-art still\n",
            "depends on a remote human controller with robust wireless links to perform\n",
            "several of these applications. The lack of autonomy restricts the domains of\n",
            "application and tasks for which a UAS can be deployed. Enabling autonomy and\n",
            "intelligence to the UAS will help overcome this hurdle and expand its use\n",
            "improving safety and efficiency. The exponential increase in computing\n",
            "resources and the availability of large amount of data in this digital era has\n",
            "led to the resurgence of machine learning from its last winter. Therefore, in\n",
            "this chapter, we discuss how some of the advances in machine learning,\n",
            "specifically deep learning and reinforcement learning can be leveraged to\n",
            "develop next-generation autonomous UAS. We first begin motivating this chapter\n",
            "by discussing the application, challenges, and opportunities of the current UAS\n",
            "in the introductory section. We then provide an overview of some of the key\n",
            "deep learning and reinforcement learning techniques discussed throughout this\n",
            "chapter. A key area of focus that will be essential to enable autonomy to UAS\n",
            "is computer vision. Accordingly, we discuss how deep learning approaches have\n",
            "been used to accomplish some of the basic tasks that contribute to providing\n",
            "UAS autonomy. Then we discuss how reinforcement learning is explored for using\n",
            "this information to provide autonomous control and navigation for UAS. Next, we\n",
            "provide the reader with directions to choose appropriate simulation suites and\n",
            "hardware platforms that will help to rapidly prototype novel machine learning\n",
            "based solutions for UAS. We additionally discuss the open problems and\n",
            "challenges pertaining to each aspect of developing autonomous UAS solutions to\n",
            "shine light on potential research areas.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'S', 'Y', 'a', 'c', 'e', 's', 't']\n",
            " \n",
            "Abstract: Many complex multi-agent systems such as robot swarms control and autonomous\n",
            "vehicle coordination can be modeled as Multi-Agent Reinforcement Learning\n",
            "(MARL) tasks. QMIX, a popular MARL algorithm base on the monotonicity\n",
            "constraint, has been used as a baseline for the benchmark environments, e.g.,\n",
            "Starcraft Multi-Agent Challenge (SMAC), Predator-Prey (PP). Recent variants of\n",
            "QMIX target relaxing the monotonicity constraint of QMIX to improve the\n",
            "expressive power of QMIX, allowing for performance improvement in SMAC.\n",
            "However, we find that such performance improvements of the variants are\n",
            "significantly affected by various implementation tricks. In this paper, we\n",
            "revisit the monotonicity constraint of QMIX, (1) we design a novel model RMC to\n",
            "further investigate the monotonicity constraint; the results show that\n",
            "monotonicity constraint can improve sample efficiency in some purely\n",
            "cooperative tasks. (2) we then re-evaluate the performance of QMIX and these\n",
            "variants by a grid hyperparameter search for the tricks; the results show QMIX\n",
            "achieves the best performance among them; (3) we analyze the monotonic mixing\n",
            "network from a theoretical perspective and show that it can represent any tasks\n",
            "which can be interpreted as purely cooperative. These analyses demonstrate that\n",
            "relaxing the monotonicity constraint of the mixing network will not always\n",
            "improve the performance of QMIX, which breaks our previous impressions of the\n",
            "monotonicity constraints. We open-source the code at\n",
            "\\url{https://github.com/hijkzzz/pymarl2}.\n",
            "Label(s): [' ', '.', 'A', 'G', 'I', 'L', 'M', 'c', 's']\n",
            " \n",
            "Abstract: In this paper, we address the Online Unsupervised Domain Adaptation (OUDA)\n",
            "problem, where the target data are unlabelled and arriving sequentially. The\n",
            "traditional methods on the OUDA problem mainly focus on transforming each\n",
            "arriving target data to the source domain, and they do not sufficiently\n",
            "consider the temporal coherency and accumulative statistics among the arriving\n",
            "target data. We propose a multi-step framework for the OUDA problem, which\n",
            "institutes a novel method to compute the mean-target subspace inspired by the\n",
            "geometrical interpretation on the Euclidean space. This mean-target subspace\n",
            "contains accumulative temporal information among the arrived target data.\n",
            "Moreover, the transformation matrix computed from the mean-target subspace is\n",
            "applied to the next target data as a preprocessing step, aligning the target\n",
            "data closer to the source domain. Experiments on four datasets demonstrated the\n",
            "contribution of each step in our proposed multi-step OUDA framework and its\n",
            "performance over previous approaches.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Model-based reinforcement learning (MBRL) with model-predictive control or\n",
            "online planning has shown great potential for locomotion control tasks in terms\n",
            "of both sample efficiency and asymptotic performance. Despite their initial\n",
            "successes, the existing planning methods search from candidate sequences\n",
            "randomly generated in the action space, which is inefficient in complex\n",
            "high-dimensional environments. In this paper, we propose a novel MBRL\n",
            "algorithm, model-based policy planning (POPLIN), that combines policy networks\n",
            "with online planning. More specifically, we formulate action planning at each\n",
            "time-step as an optimization problem using neural networks. We experiment with\n",
            "both optimization w.r.t. the action sequences initialized from the policy\n",
            "network, and also online optimization directly w.r.t. the parameters of the\n",
            "policy network. We show that POPLIN obtains state-of-the-art performance in the\n",
            "MuJoCo benchmarking environments, being about 3x more sample efficient than the\n",
            "state-of-the-art algorithms, such as PETS, TD3 and SAC. To explain the\n",
            "effectiveness of our algorithm, we show that the optimization surface in\n",
            "parameter space is smoother than in action space. Further more, we found the\n",
            "distilled policy network can be effectively applied without the expansive model\n",
            "predictive control during test time for some environments such as Cheetah. Code\n",
            "is released in https://github.com/WilsonWangTHU/POPLIN.\n",
            "Label(s): [' ', '.', 'A', 'G', 'I', 'L', 'M', 'O', 'R', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: The vast majority of deep models use multiple gradient signals, typically\n",
            "corresponding to a sum of multiple loss terms, to update a shared set of\n",
            "trainable weights. However, these multiple updates can impede optimal training\n",
            "by pulling the model in conflicting directions. We present Gradient Sign\n",
            "Dropout (GradDrop), a probabilistic masking procedure which samples gradients\n",
            "at an activation layer based on their level of consistency. GradDrop is\n",
            "implemented as a simple deep layer that can be used in any deep net and\n",
            "synergizes with other gradient balancing approaches. We show that GradDrop\n",
            "outperforms the state-of-the-art multiloss methods within traditional multitask\n",
            "and transfer learning settings, and we discuss how GradDrop reveals links\n",
            "between optimal multiloss training and gradient stochasticity.\n",
            "Label(s): [' ', '.', 'C', 'G', 'L', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Data in real-world application often exhibit skewed class distribution which\n",
            "poses an intense challenge for machine learning. Conventional classification\n",
            "algorithms are not effective in the case of imbalanced data distribution, and\n",
            "may fail when the data distribution is highly imbalanced. To address this\n",
            "issue, we propose a general imbalanced classification model based on deep\n",
            "reinforcement learning. We formulate the classification problem as a sequential\n",
            "decision-making process and solve it by deep Q-learning network. The agent\n",
            "performs a classification action on one sample at each time step, and the\n",
            "environment evaluates the classification action and returns a reward to the\n",
            "agent. The reward from minority class sample is larger so the agent is more\n",
            "sensitive to the minority class. The agent finally finds an optimal\n",
            "classification policy in imbalanced data under the guidance of specific reward\n",
            "function and beneficial learning environment. Experiments show that our\n",
            "proposed model outperforms the other imbalanced classification algorithms, and\n",
            "it can identify more minority samples and has great classification performance.\n",
            "Label(s): [' ', '.', 'A', 'G', 'I', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: In AI research and industry, machine learning is the most widely used tool.\n",
            "One of the most important machine learning algorithms is Gradient Boosting\n",
            "Decision Tree, i.e. GBDT whose training process needs considerable\n",
            "computational resources and time. To shorten GBDT training time, many works\n",
            "tried to apply GBDT on Parameter Server. However, those GBDT algorithms are\n",
            "synchronous parallel algorithms which fail to make full use of Parameter\n",
            "Server. In this paper, we examine the possibility of using asynchronous\n",
            "parallel methods to train GBDT model and name this algorithm as asynch-SGBDT\n",
            "(asynchronous parallel stochastic gradient boosting decision tree). Our\n",
            "theoretical and experimental results indicate that the scalability of\n",
            "asynch-SGBDT is influenced by the sample diversity of datasets, sampling rate,\n",
            "step length and the setting of GBDT tree. Experimental results also show\n",
            "asynch-SGBDT training process reaches a linear speedup in asynchronous parallel\n",
            "manner when datasets and GBDT trees meet high scalability requirements.\n",
            "Label(s): [' ', '.', 'C', 'D', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Convex optimizers have known many applications as differentiable layers\n",
            "within deep neural architectures. One application of these convex layers is to\n",
            "project points into a convex set. However, both forward and backward passes of\n",
            "these convex layers are significantly more expensive to compute than those of a\n",
            "typical neural network. We investigate in this paper whether an inexact, but\n",
            "cheaper projection, can drive a descent algorithm to an optimum. Specifically,\n",
            "we propose an interpolation-based projection that is computationally cheap and\n",
            "easy to compute given a convex, domain defining, function. We then propose an\n",
            "optimization algorithm that follows the gradient of the composition of the\n",
            "objective and the projection and prove its convergence for linear objectives\n",
            "and arbitrary convex and Lipschitz domain defining inequality constraints. In\n",
            "addition to the theoretical contributions, we demonstrate empirically the\n",
            "practical interest of the interpolation projection when used in conjunction\n",
            "with neural networks in a reinforcement learning and a supervised learning\n",
            "setting.\n",
            "Label(s): [' ', '.', 'C', 'G', 'L', 'M', 'O', 'a', 'c', 'h', 'm', 's', 't']\n",
            " \n",
            "Abstract: We present a neural-symbolic framework for observing the environment and\n",
            "continuously learning visual semantics and intuitive physics to reproduce them\n",
            "in an interactive simulation. The framework consists of five parts, a\n",
            "neural-symbolic hybrid network based on capsules for inverse graphics, an\n",
            "episodic memory to store observations, an interaction network for intuitive\n",
            "physics, a meta-learning agent that continuously improves the framework and a\n",
            "querying language that acts as the framework's interface for simulation. By\n",
            "means of lifelong meta-learning, the capsule network is expanded and trained\n",
            "continuously, in order to better adapt to its environment with each iteration.\n",
            "This enables it to learn new semantics using a few-shot approach and with\n",
            "minimal input from an oracle over its lifetime. From what it learned through\n",
            "observation, the part for intuitive physics infers all the required physical\n",
            "properties of the objects in a scene, enabling predictions. Finally, a custom\n",
            "query language ties all parts together, which allows to perform various mental\n",
            "simulation tasks, such as navigation, sorting and simulation of a game\n",
            "environment, with which we illustrate the potential of our novel approach.\n",
            "Label(s): [' ', '.', 'A', 'C', 'I', 'V', 'c', 's']\n",
            " \n",
            "Abstract: This paper discusses the technical challenges in maritime image processing\n",
            "and machine vision problems for video streams generated by cameras. Even well\n",
            "documented problems of horizon detection and registration of frames in a video\n",
            "are very challenging in maritime scenarios. More advanced problems of\n",
            "background subtraction and object detection in video streams are very\n",
            "challenging. Challenges arising from the dynamic nature of the background,\n",
            "unavailability of static cues, presence of small objects at distant\n",
            "backgrounds, illumination effects, all contribute to the challenges as\n",
            "discussed here.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: In this work, we move beyond the traditional complex-valued representations,\n",
            "introducing more expressive hypercomplex representations to model entities and\n",
            "relations for knowledge graph embeddings. More specifically, quaternion\n",
            "embeddings, hypercomplex-valued embeddings with three imaginary components, are\n",
            "utilized to represent entities. Relations are modelled as rotations in the\n",
            "quaternion space. The advantages of the proposed approach are: (1) Latent\n",
            "inter-dependencies (between all components) are aptly captured with Hamilton\n",
            "product, encouraging a more compact interaction between entities and relations;\n",
            "(2) Quaternions enable expressive rotation in four-dimensional space and have\n",
            "more degree of freedom than rotation in complex plane; (3) The proposed\n",
            "framework is a generalization of ComplEx on hypercomplex space while offering\n",
            "better geometrical interpretations, concurrently satisfying the key desiderata\n",
            "of relational representation learning (i.e., modeling symmetry, anti-symmetry\n",
            "and inversion). Experimental results demonstrate that our method achieves\n",
            "state-of-the-art performance on four well-established knowledge graph\n",
            "completion benchmarks.\n",
            "Label(s): [' ', '.', 'C', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: In this paper, we present a new Mask R-CNN based text detection approach\n",
            "which can robustly detect multi-oriented and curved text from natural scene\n",
            "images in a unified manner. To enhance the feature representation ability of\n",
            "Mask R-CNN for text detection tasks, we propose to use the Pyramid Attention\n",
            "Network (PAN) as a new backbone network of Mask R-CNN. Experiments demonstrate\n",
            "that PAN can suppress false alarms caused by text-like backgrounds more\n",
            "effectively. Our proposed approach has achieved superior performance on both\n",
            "multi-oriented (ICDAR-2015, ICDAR-2017 MLT) and curved (SCUT-CTW1500) text\n",
            "detection benchmark tasks by only using single-scale and single-model testing.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: In machine learning, one must acquire labels to help supervise a model that\n",
            "will be able to generalize to unseen data. However, the labeling process can be\n",
            "tedious, long, costly, and error-prone. It is often the case that most of our\n",
            "data is unlabeled. Semi-supervised learning (SSL) alleviates that by making\n",
            "strong assumptions about the relation between the labels and the input data\n",
            "distribution. This paradigm has been successful in practice, but most SSL\n",
            "algorithms end up fully trusting the few available labels. In real life, both\n",
            "humans and automated systems are prone to mistakes; it is essential that our\n",
            "algorithms are able to work with labels that are both few and also unreliable.\n",
            "Our work aims to perform an extensive empirical evaluation of existing\n",
            "graph-based semi-supervised algorithms, like Gaussian Fields and Harmonic\n",
            "Functions, Local and Global Consistency, Laplacian Eigenmaps, Graph\n",
            "Transduction Through Alternating Minimization. To do that, we compare the\n",
            "accuracy of classifiers while varying the amount of labeled data and label\n",
            "noise for many different samples. Our results show that, if the dataset is\n",
            "consistent with SSL assumptions, we are able to detect the noisiest instances,\n",
            "although this gets harder when the number of available labels decreases. Also,\n",
            "the Laplacian Eigenmaps algorithm performed better than label propagation when\n",
            "the data came from high-dimensional clusters.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Efficient model inference is an important and practical issue in the\n",
            "deployment of deep neural network on resource constraint platforms. Network\n",
            "quantization addresses this problem effectively by leveraging low-bit\n",
            "representation and arithmetic that could be conducted on dedicated embedded\n",
            "systems. In the previous works, the parameter bitwidth is set homogeneously and\n",
            "there is a trade-off between superior performance and aggressive compression.\n",
            "Actually the stacked network layers, which are generally regarded as\n",
            "hierarchical feature extractors, contribute diversely to the overall\n",
            "performance. For a well-trained neural network, the feature distributions of\n",
            "different categories differentiate gradually as the network propagates forward.\n",
            "Hence the capability requirement on the subsequent feature extractors is\n",
            "reduced. It indicates that the neurons in posterior layers could be assigned\n",
            "with lower bitwidth for quantized neural networks. Based on this observation, a\n",
            "simple but effective mixed-precision quantized neural network with\n",
            "progressively ecreasing bitwidth is proposed to improve the trade-off between\n",
            "accuracy and compression. Extensive experiments on typical network\n",
            "architectures and benchmark datasets demonstrate that the proposed method could\n",
            "achieve better or comparable results while reducing the memory space for\n",
            "quantized parameters by more than 30\\% in comparison with the homogeneous\n",
            "counterparts. In addition, the results also demonstrate that the\n",
            "higher-precision bottom layers could boost the 1-bit network performance\n",
            "appreciably due to a better preservation of the original image information\n",
            "while the lower-precision posterior layers contribute to the regularization of\n",
            "$k-$bit networks.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Anomaly detection aims to detect abnormal events by a model of normality. It\n",
            "plays an important role in many domains such as network intrusion detection,\n",
            "criminal activity identity and so on. With the rapidly growing size of\n",
            "accessible training data and high computation capacities, deep learning based\n",
            "anomaly detection has become more and more popular. In this paper, a new\n",
            "domain-based anomaly detection method based on generative adversarial networks\n",
            "(GAN) is proposed. Minimum likelihood regularization is proposed to make the\n",
            "generator produce more anomalies and prevent it from converging to normal data\n",
            "distribution. Proper ensemble of anomaly scores is shown to improve the\n",
            "stability of discriminator effectively. The proposed method has achieved\n",
            "significant improvement than other anomaly detection methods on Cifar10 and UCI\n",
            "datasets.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Exploration is a fundamental aspect of Reinforcement Learning, typically\n",
            "implemented using stochastic action-selection. Exploration, however, can be\n",
            "more efficient if directed toward gaining new world knowledge. Visit-counters\n",
            "have been proven useful both in practice and in theory for directed\n",
            "exploration. However, a major limitation of counters is their locality. While\n",
            "there are a few model-based solutions to this shortcoming, a model-free\n",
            "approach is still missing. We propose $E$-values, a generalization of counters\n",
            "that can be used to evaluate the propagating exploratory value over\n",
            "state-action trajectories. We compare our approach to commonly used RL\n",
            "techniques, and show that using $E$-values improves learning and performance\n",
            "over traditional counters. We also show how our method can be implemented with\n",
            "function approximation to efficiently learn continuous MDPs. We demonstrate\n",
            "this by showing that our approach surpasses state of the art performance in the\n",
            "Freeway Atari 2600 game.\n",
            "Label(s): [' ', '.', 'A', 'G', 'I', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: In the resource management of wireless networks, Federated Learning has been\n",
            "used to predict handovers. However, non-independent and identically distributed\n",
            "data degrade the accuracy performance of such predictions. To overcome the\n",
            "problem, Federated Learning can leverage data clustering algorithms and build a\n",
            "machine learning model for each cluster. However, traditional data clustering\n",
            "algorithms, when applied to the handover prediction, exhibit three main\n",
            "limitations: the risk of data privacy breach, the fixed shape of clusters, and\n",
            "the non-adaptive number of clusters. To overcome these limitations, in this\n",
            "paper, we propose a three-phased data clustering algorithm, namely: generative\n",
            "adversarial network-based clustering, cluster calibration, and cluster\n",
            "division. We show that the generative adversarial network-based clustering\n",
            "preserves privacy. The cluster calibration deals with dynamic environments by\n",
            "modifying clusters. Moreover, the divisive clustering explores the different\n",
            "number of clusters by repeatedly selecting and dividing a cluster into multiple\n",
            "clusters. A baseline algorithm and our algorithm are tested on a time series\n",
            "forecasting task. We show that our algorithm improves the performance of\n",
            "forecasting models, including cellular network handover, by 43%.\n",
            "Label(s): [' ', '.', 'G', 'L', 'c', 's']\n",
            " \n",
            "Abstract: Dense 3D visual mapping estimates as many as possible pixel depths, for each\n",
            "image. This results in very dense point clouds that often contain redundant and\n",
            "noisy information, especially for surfaces that are roughly planar, for\n",
            "instance, the ground or the walls in the scene. In this paper we leverage on\n",
            "semantic image segmentation to discriminate which regions of the scene require\n",
            "simplification and which should be kept at high level of details. We propose\n",
            "four different point cloud simplification methods which decimate the perceived\n",
            "point cloud by relying on class-specific local and global statistics still\n",
            "maintaining more points in the proximity of class boundaries to preserve the\n",
            "infra-class edges and discontinuities. 3D dense model is obtained by fusing the\n",
            "point clouds in a 3D Delaunay Triangulation to deal with variable point cloud\n",
            "density. In the experimental evaluation we have shown that, by leveraging on\n",
            "semantics, it is possible to simplify the model and diminish the noise\n",
            "affecting the point clouds.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: The incorporation of region regularization into max-flow segmentation has\n",
            "traditionally focused on ordering and part-whole relationships. A side effect\n",
            "of the development of such models is that it constrained regularization only to\n",
            "those cases, rather than allowing for arbitrary region regularization. Directed\n",
            "Acyclic Graphical Max-Flow (DAGMF) segmentation overcomes these limitations by\n",
            "allowing for the algorithm designer to specify an arbitrary directed acyclic\n",
            "graph to structure a max-flow segmentation. This allows for individual 'parts'\n",
            "to be a member of multiple distinct 'wholes.'\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Autonomous vehicles must balance a complex set of objectives. There is no\n",
            "consensus on how they should do so, nor on a model for specifying a desired\n",
            "driving behavior. We created a dataset to help address some of these questions\n",
            "in a limited operating domain. The data consists of 92 traffic scenarios, with\n",
            "multiple ways of traversing each scenario. Multiple annotators expressed their\n",
            "preference between pairs of scenario traversals. We used the data to compare an\n",
            "instance of a rulebook, carefully hand-crafted independently of the dataset,\n",
            "with several interpretable machine learning models such as Bayesian networks,\n",
            "decision trees, and logistic regression trained on the dataset. To compare\n",
            "driving behavior, these models use scores indicating by how much different\n",
            "scenario traversals violate each of 14 driving rules. The rules are\n",
            "interpretable and designed by subject-matter experts. First, we found that\n",
            "these rules were enough for these models to achieve a high classification\n",
            "accuracy on the dataset. Second, we found that the rulebook provides high\n",
            "interpretability without excessively sacrificing performance. Third, the data\n",
            "pointed to possible improvements in the rulebook and the rules, and to\n",
            "potential new rules. Fourth, we explored the interpretability vs performance\n",
            "trade-off by also training non-interpretable models such as a random forest.\n",
            "Finally, we make the dataset publicly available to encourage a discussion from\n",
            "the wider community on behavior specification for AVs. Please find it at\n",
            "github.com/bassam-motional/Reasonable-Crowd.\n",
            "Label(s): [' ', '.', 'G', 'L', 'O', 'R', 'c', 's']\n",
            " \n",
            "Abstract: To improve the sample efficiency of policy-gradient based reinforcement\n",
            "learning algorithms, we propose implicit distributional actor-critic (IDAC)\n",
            "that consists of a distributional critic, built on two deep generator networks\n",
            "(DGNs), and a semi-implicit actor (SIA), powered by a flexible policy\n",
            "distribution. We adopt a distributional perspective on the discounted\n",
            "cumulative return and model it with a state-action-dependent implicit\n",
            "distribution, which is approximated by the DGNs that take state-action pairs\n",
            "and random noises as their input. Moreover, we use the SIA to provide a\n",
            "semi-implicit policy distribution, which mixes the policy parameters with a\n",
            "reparameterizable distribution that is not constrained by an analytic density\n",
            "function. In this way, the policy's marginal distribution is implicit,\n",
            "providing the potential to model complex properties such as covariance\n",
            "structure and skewness, but its parameter and entropy can still be estimated.\n",
            "We incorporate these features with an off-policy algorithm framework to solve\n",
            "problems with continuous action space and compare IDAC with state-of-the-art\n",
            "algorithms on representative OpenAI Gym environments. We observe that IDAC\n",
            "outperforms these baselines in most tasks. Python code is provided.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: State-of-the-art image segmentation algorithms generally consist of at least\n",
            "two successive and distinct computations: a boundary detection process that\n",
            "uses local image information to classify image locations as boundaries between\n",
            "objects, followed by a pixel grouping step such as watershed or connected\n",
            "components that clusters pixels into segments. Prior work has varied the\n",
            "complexity and approach employed in these two steps, including the\n",
            "incorporation of multi-layer neural networks to perform boundary prediction,\n",
            "and the use of global optimizations during pixel clustering. We propose a\n",
            "unified and end-to-end trainable machine learning approach, flood-filling\n",
            "networks, in which a recurrent 3d convolutional network directly produces\n",
            "individual segments from a raw image. The proposed approach robustly segments\n",
            "images with an unknown and variable number of objects as well as highly\n",
            "variable object sizes. We demonstrate the approach on a challenging 3d image\n",
            "segmentation task, connectomic reconstruction from volume electron microscopy\n",
            "data, on which flood-filling neural networks substantially improve accuracy\n",
            "over other state-of-the-art methods. The proposed approach can replace complex\n",
            "multi-step segmentation pipelines with a single neural network that is learned\n",
            "end-to-end.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: We propose the Encoder-Recurrent-Decoder (ERD) model for recognition and\n",
            "prediction of human body pose in videos and motion capture. The ERD model is a\n",
            "recurrent neural network that incorporates nonlinear encoder and decoder\n",
            "networks before and after recurrent layers. We test instantiations of ERD\n",
            "architectures in the tasks of motion capture (mocap) generation, body pose\n",
            "labeling and body pose forecasting in videos. Our model handles mocap training\n",
            "data across multiple subjects and activity domains, and synthesizes novel\n",
            "motions while avoid drifting for long periods of time. For human pose labeling,\n",
            "ERD outperforms a per frame body part detector by resolving left-right body\n",
            "part confusions. For video pose forecasting, ERD predicts body joint\n",
            "displacements across a temporal horizon of 400ms and outperforms a first order\n",
            "motion model based on optical flow. ERDs extend previous Long Short Term Memory\n",
            "(LSTM) models in the literature to jointly learn representations and their\n",
            "dynamics. Our experiments show such representation learning is crucial for both\n",
            "labeling and prediction in space-time. We find this is a distinguishing feature\n",
            "between the spatio-temporal visual domain in comparison to 1D text, speech or\n",
            "handwriting, where straightforward hard coded representations have shown\n",
            "excellent results when directly combined with recurrent units.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Recent studies show that deep neural networks are vulnerable to adversarial\n",
            "examples which can be generated via certain types of transformations. Being\n",
            "robust to a desired family of adversarial attacks is then equivalent to being\n",
            "invariant to a family of transformations. Learning invariant representations\n",
            "then naturally emerges as an important goal to achieve which we explore in this\n",
            "paper within specific application contexts. Specifically, we propose a\n",
            "cyclically-trained adversarial network to learn a mapping from image space to\n",
            "latent representation space and back such that the latent representation is\n",
            "invariant to a specified factor of variation (e.g., identity). The learned\n",
            "mapping assures that the synthesized image is not only realistic, but has the\n",
            "same values for unspecified factors (e.g., pose and illumination) as the\n",
            "original image and a desired value of the specified factor. Unlike disentangled\n",
            "representation learning, which requires two latent spaces, one for specified\n",
            "and another for unspecified factors, invariant representation learning needs\n",
            "only one such space. We encourage invariance to a specified factor by applying\n",
            "adversarial training using a variational autoencoder in the image space as\n",
            "opposed to the latent space. We strengthen this invariance by introducing a\n",
            "cyclic training process (forward and backward cycle). We also propose a new\n",
            "method to evaluate conditional generative networks. It compares how well\n",
            "different factors of variation can be predicted from the synthesized, as\n",
            "opposed to real, images. In quantitative terms, our approach attains\n",
            "state-of-the-art performance in experiments spanning three datasets with\n",
            "factors such as identity, pose, illumination or style. Our method produces\n",
            "sharp, high-quality synthetic images with little visible artefacts compared to\n",
            "previous approaches.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Generative adversarial networks are a class of generative algorithms that\n",
            "have been widely used to produce state-of-the-art samples. In this paper, we\n",
            "investigate GAN to perform anomaly detection on time series dataset. In order\n",
            "to achieve this goal, a bibliography is made focusing on theoretical properties\n",
            "of GAN and GAN used for anomaly detection. A Wasserstein GAN has been chosen to\n",
            "learn the representation of normal data distribution and a stacked encoder with\n",
            "the generator performs the anomaly detection. W-GAN with encoder seems to\n",
            "produce state of the art anomaly detection scores on MNIST dataset and we\n",
            "investigate its usage on multi-variate time series.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: We present a novel approach for unsupervised learning of depth and ego-motion\n",
            "from monocular video. Unsupervised learning removes the need for separate\n",
            "supervisory signals (depth or ego-motion ground truth, or multi-view video).\n",
            "Prior work in unsupervised depth learning uses pixel-wise or gradient-based\n",
            "losses, which only consider pixels in small local neighborhoods. Our main\n",
            "contribution is to explicitly consider the inferred 3D geometry of the scene,\n",
            "enforcing consistency of the estimated 3D point clouds and ego-motion across\n",
            "consecutive frames. This is a challenging task and is solved by a novel\n",
            "(approximate) backpropagation algorithm for aligning 3D structures.\n",
            "  We combine this novel 3D-based loss with 2D losses based on photometric\n",
            "quality of frame reconstructions using estimated depth and ego-motion from\n",
            "adjacent frames. We also incorporate validity masks to avoid penalizing areas\n",
            "in which no useful information exists.\n",
            "  We test our algorithm on the KITTI dataset and on a video dataset captured on\n",
            "an uncalibrated mobile phone camera. Our proposed approach consistently\n",
            "improves depth estimates on both datasets, and outperforms the state-of-the-art\n",
            "for both depth and ego-motion. Because we only require a simple video, learning\n",
            "depth and ego-motion on large and varied datasets becomes possible. We\n",
            "demonstrate this by training on the low quality uncalibrated video dataset and\n",
            "evaluating on KITTI, ranking among top performing prior methods which are\n",
            "trained on KITTI itself.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: To assist researchers to identify Environmental Microorganisms (EMs)\n",
            "effectively, a Multiscale CNN-CRF (MSCC) framework for the EM image\n",
            "segmentation is proposed in this paper. There are two parts in this framework:\n",
            "The first is a novel pixel-level segmentation approach, using a newly\n",
            "introduced Convolutional Neural Network (CNN), namely, \"mU-Net-B3\", with a\n",
            "dense Conditional Random Field (CRF) postprocessing. The second is a VGG-16\n",
            "based patch-level segmentation method with a novel \"buffer\" strategy, which\n",
            "further improves the segmentation quality of the details of the EMs. In the\n",
            "experiment, compared with the state-of-the-art methods on 420 EM images, the\n",
            "proposed MSCC method reduces the memory requirement from 355 MB to 103 MB,\n",
            "improves the overall evaluation indexes (Dice, Jaccard, Recall, Accuracy) from\n",
            "85.24%, 77.42%, 82.27%, and 96.76% to 87.13%, 79.74%, 87.12%, and 96.91%,\n",
            "respectively, and reduces the volume overlap error from 22.58% to 20.26%.\n",
            "Therefore, the MSCC method shows great potential in the EM segmentation field.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Detecting facial forgery images and videos is an increasingly important topic\n",
            "in multimedia forensics. As forgery images and videos are usually compressed\n",
            "into different formats such as JPEG and H264 when circulating on the Internet,\n",
            "existing forgery-detection methods trained on uncompressed data often suffer\n",
            "from significant performance degradation in identifying them. To solve this\n",
            "problem, we propose a novel anti-compression facial forgery detection\n",
            "framework, which learns a compression-insensitive embedding feature space\n",
            "utilizing both original and compressed forgeries. Specifically, our approach\n",
            "consists of three ideas: (i) extracting compression-insensitive features from\n",
            "both uncompressed and compressed forgeries using an adversarial learning\n",
            "strategy; (ii) learning a robust partition by constructing a metric loss that\n",
            "can reduce the distance of the paired original and compressed images in the\n",
            "embedding space; (iii) improving the accuracy of tampered localization with an\n",
            "attention-transfer module. Experimental results demonstrate that, the proposed\n",
            "method is highly effective in handling both compressed and uncompressed facial\n",
            "forgery images.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: In many applications, multivariate samples may harbor previously unrecognized\n",
            "heterogeneity at the level of conditional independence or network structure.\n",
            "For example, in cancer biology, disease subtypes may differ with respect to\n",
            "subtype-specific interplay between molecular components. Then, both subtype\n",
            "discovery and estimation of subtype-specific networks present important and\n",
            "related challenges. To enable such analyses, we put forward a mixture model\n",
            "whose components are sparse Gaussian graphical models. This brings together\n",
            "model-based clustering and graphical modeling to permit simultaneous estimation\n",
            "of cluster assignments and cluster-specific networks. We carry out estimation\n",
            "within an L1-penalized framework, and investigate several specific penalization\n",
            "regimes. We present empirical results on simulated data and provide general\n",
            "recommendations for the formulation and use of mixtures of L1-penalized\n",
            "Gaussian graphical models.\n",
            "Label(s): [' ', '.', 'E', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Temporal action segmentation is a topic of increasing interest, however,\n",
            "annotating each frame in a video is cumbersome and costly. Weakly supervised\n",
            "approaches therefore aim at learning temporal action segmentation from videos\n",
            "that are only weakly labeled. In this work, we assume that for each training\n",
            "video only the list of actions is given that occur in the video, but not when,\n",
            "how often, and in which order they occur. In order to address this task, we\n",
            "propose an approach that can be trained end-to-end on such data. The approach\n",
            "divides the video into smaller temporal regions and predicts for each region\n",
            "the action label and its length. In addition, the network estimates the action\n",
            "labels for each frame. By measuring how consistent the frame-wise predictions\n",
            "are with respect to the temporal regions and the annotated action labels, the\n",
            "network learns to divide a video into class-consistent regions. We evaluate our\n",
            "approach on three datasets where the approach achieves state-of-the-art\n",
            "results.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Recent deep monocular depth estimation approaches based on supervised\n",
            "regression have achieved remarkable performance. However, they require costly\n",
            "ground truth annotations during training. To cope with this issue, in this\n",
            "paper we present a novel unsupervised deep learning approach for predicting\n",
            "depth maps. We introduce a new network architecture, named Progressive Fusion\n",
            "Network (PFN), that is specifically designed for binocular stereo depth\n",
            "estimation. This network is based on a multi-scale refinement strategy that\n",
            "combines the information provided by both stereo views. In addition, we propose\n",
            "to stack twice this network in order to form a cycle. This cycle approach can\n",
            "be interpreted as a form of data-augmentation since, at training time, the\n",
            "network learns both from the training set images (in the forward half-cycle)\n",
            "but also from the synthesized images (in the backward half-cycle). The\n",
            "architecture is jointly trained with adversarial learning. Extensive\n",
            "experiments on the publicly available datasets KITTI, Cityscapes and\n",
            "ApolloScape demonstrate the effectiveness of the proposed model which is\n",
            "competitive with other unsupervised deep learning methods for depth prediction.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Purpose: To validate the performance of a commercially-available,\n",
            "CE-certified deep learning (DL) system, RetCAD v.1.3.0 (Thirona, Nijmegen, The\n",
            "Netherlands), for the joint automatic detection of diabetic retinopathy (DR)\n",
            "and age-related macular degeneration (AMD) in color fundus (CF) images on a\n",
            "dataset with mixed presence of eye diseases.\n",
            "  Methods: Evaluation of joint detection of referable DR and AMD was performed\n",
            "on a DR-AMD dataset with 600 images acquired during routine clinical practice,\n",
            "containing referable and non-referable cases of both diseases. Each image was\n",
            "graded for DR and AMD by an experienced ophthalmologist to establish the\n",
            "reference standard (RS), and by four independent observers for comparison with\n",
            "human performance. Validation was furtherly assessed on Messidor (1200 images)\n",
            "for individual identification of referable DR, and the Age-Related Eye Disease\n",
            "Study (AREDS) dataset (133821 images) for referable AMD, against the\n",
            "corresponding RS.\n",
            "  Results: Regarding joint validation on the DR-AMD dataset, the system\n",
            "achieved an area under the ROC curve (AUC) of 95.1% for detection of referable\n",
            "DR (SE=90.1%, SP=90.6%). For referable AMD, the AUC was 94.9% (SE=91.8%,\n",
            "SP=87.5%). Average human performance for DR was SE=61.5% and SP=97.8%; for AMD,\n",
            "SE=76.5% and SP=96.1%. Regarding detection of referable DR in Messidor, AUC was\n",
            "97.5% (SE=92.0%, SP=92.1%); for referable AMD in AREDS, AUC was 92.7%\n",
            "(SE=85.8%, SP=86.0%).\n",
            "  Conclusions: The validated system performs comparably to human experts at\n",
            "simultaneous detection of DR and AMD. This shows that DL systems can facilitate\n",
            "access to joint screening of eye diseases and become a quick and reliable\n",
            "support for ophthalmological experts.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: We introduce a novel network, called CO-attention Siamese Network (COSNet),\n",
            "to address the unsupervised video object segmentation task from a holistic\n",
            "view. We emphasize the importance of inherent correlation among video frames\n",
            "and incorporate a global co-attention mechanism to improve further the\n",
            "state-of-the-art deep learning based solutions that primarily focus on learning\n",
            "discriminative foreground representations over appearance and motion in\n",
            "short-term temporal segments. The co-attention layers in our network provide\n",
            "efficient and competent stages for capturing global correlations and scene\n",
            "context by jointly computing and appending co-attention responses into a joint\n",
            "feature space. We train COSNet with pairs of video frames, which naturally\n",
            "augments training data and allows increased learning capacity. During the\n",
            "segmentation stage, the co-attention model encodes useful information by\n",
            "processing multiple reference frames together, which is leveraged to infer the\n",
            "frequently reappearing and salient foreground objects better. We propose a\n",
            "unified and end-to-end trainable framework where different co-attention\n",
            "variants can be derived for mining the rich context within videos. Our\n",
            "extensive experiments over three large benchmarks manifest that COSNet\n",
            "outperforms the current alternatives by a large margin.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Graph Neural Networks (GNNs) are a powerful tool for machine learning on\n",
            "graphs.GNNs combine node feature information with the graph structure by\n",
            "recursively passing neural messages along edges of the input graph. However,\n",
            "incorporating both graph structure and feature information leads to complex\n",
            "models, and explaining predictions made by GNNs remains unsolved. Here we\n",
            "propose GNNExplainer, the first general, model-agnostic approach for providing\n",
            "interpretable explanations for predictions of any GNN-based model on any\n",
            "graph-based machine learning task. Given an instance, GNNExplainer identifies a\n",
            "compact subgraph structure and a small subset of node features that have a\n",
            "crucial role in GNN's prediction. Further, GNNExplainer can generate consistent\n",
            "and concise explanations for an entire class of instances. We formulate\n",
            "GNNExplainer as an optimization task that maximizes the mutual information\n",
            "between a GNN's prediction and distribution of possible subgraph structures.\n",
            "Experiments on synthetic and real-world graphs show that our approach can\n",
            "identify important graph structures as well as node features, and outperforms\n",
            "baselines by 17.1% on average. GNNExplainer provides a variety of benefits,\n",
            "from the ability to visualize semantically relevant structures to\n",
            "interpretability, to giving insights into errors of faulty GNNs.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: In this paper, we introduce proximal gradient temporal difference learning,\n",
            "which provides a principled way of designing and analyzing true stochastic\n",
            "gradient temporal difference learning algorithms. We show how gradient TD (GTD)\n",
            "reinforcement learning methods can be formally derived, not by starting from\n",
            "their original objective functions, as previously attempted, but rather from a\n",
            "primal-dual saddle-point objective function. We also conduct a saddle-point\n",
            "error analysis to obtain finite-sample bounds on their performance. Previous\n",
            "analyses of this class of algorithms use stochastic approximation techniques to\n",
            "prove asymptotic convergence, and do not provide any finite-sample analysis. We\n",
            "also propose an accelerated algorithm, called GTD2-MP, that uses proximal\n",
            "``mirror maps'' to yield an improved convergence rate. The results of our\n",
            "theoretical analysis imply that the GTD family of algorithms are comparable and\n",
            "may indeed be preferred over existing least squares TD methods for off-policy\n",
            "learning, due to their linear complexity. We provide experimental results\n",
            "showing the improved performance of our accelerated gradient TD methods.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: This paper aims to explain adversarial attacks in terms of how adversarial\n",
            "perturbations contribute to the attacking task. We estimate attributions of\n",
            "different image regions to the decrease of the attacking cost based on the\n",
            "Shapley value. We define and quantify interactions among adversarial\n",
            "perturbation pixels, and decompose the entire perturbation map into relatively\n",
            "independent perturbation components. The decomposition of the perturbation map\n",
            "shows that adversarially-trained DNNs have more perturbation components in the\n",
            "foreground than normally-trained DNNs. Moreover, compared to the\n",
            "normally-trained DNN, the adversarially-trained DNN have more components which\n",
            "mainly decrease the score of the true category. Above analyses provide new\n",
            "insights into the understanding of adversarial attacks.\n",
            "Label(s): [' ', '.', 'A', 'C', 'G', 'I', 'L', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Solving the challenging problem of 3D object reconstruction from a single\n",
            "image appropriately gives existing technologies the ability to perform with a\n",
            "single monocular camera rather than requiring depth sensors. In recent years,\n",
            "thanks to the development of deep learning, 3D reconstruction of a single image\n",
            "has demonstrated impressive progress. Existing researches use Chamfer distance\n",
            "as a loss function to guide the training of the neural network. However, the\n",
            "Chamfer loss will give equal weights to all points inside the 3D point clouds.\n",
            "It tends to sacrifice fine-grained and thin structures to avoid incurring a\n",
            "high loss, which will lead to visually unsatisfactory results. This paper\n",
            "proposes a framework that can recover a detailed three-dimensional point cloud\n",
            "from a single image by focusing more on boundaries (edge and corner points).\n",
            "Experimental results demonstrate that the proposed method outperforms existing\n",
            "techniques significantly, both qualitatively and quantitatively, and has fewer\n",
            "training parameters.\n",
            "Label(s): [' ', '.', 'A', 'C', 'I', 'V', 'c', 's']\n",
            " \n",
            "Abstract: There are two main issues in RGB-D salient object detection: (1) how to\n",
            "effectively integrate the complementarity from the cross-modal RGB-D data; (2)\n",
            "how to prevent the contamination effect from the unreliable depth map. In fact,\n",
            "these two problems are linked and intertwined, but the previous methods tend to\n",
            "focus only on the first problem and ignore the consideration of depth map\n",
            "quality, which may yield the model fall into the sub-optimal state. In this\n",
            "paper, we address these two issues in a holistic model synergistically, and\n",
            "propose a novel network named DPANet to explicitly model the potentiality of\n",
            "the depth map and effectively integrate the cross-modal complementarity. By\n",
            "introducing the depth potentiality perception, the network can perceive the\n",
            "potentiality of depth information in a learning-based manner, and guide the\n",
            "fusion process of two modal data to prevent the contamination occurred. The\n",
            "gated multi-modality attention module in the fusion process exploits the\n",
            "attention mechanism with a gate controller to capture long-range dependencies\n",
            "from a cross-modal perspective. Experimental results compared with 15\n",
            "state-of-the-art methods on 8 datasets demonstrate the validity of the proposed\n",
            "approach both quantitatively and qualitatively.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: In most convolution neural networks (CNNs), downsampling hidden layers is\n",
            "adopted for increasing computation efficiency and the receptive field size.\n",
            "Such operation is commonly so-called pooling. Maximation and averaging over\n",
            "sliding windows (max/average pooling), and plain downsampling in the form of\n",
            "strided convolution are popular pooling methods. Since the pooling is a lossy\n",
            "procedure, a motivation of our work is to design a new pooling approach for\n",
            "less lossy in the dimensionality reduction. Inspired by the Fourier spectral\n",
            "pooling(FSP) proposed by Rippel et. al. [1], we present the Hartley transform\n",
            "based spectral pooling method in CNNs. Compared with FSP, the proposed spectral\n",
            "pooling avoids the use of complex arithmetic for frequency representation and\n",
            "reduces the computation. Spectral pooling preserves more structure features for\n",
            "network's discriminability than max and average pooling. We empirically show\n",
            "that Hartley spectral pooling gives rise to the convergence of training CNNs on\n",
            "MNIST and CIFAR-10 datasets.\n",
            "Label(s): [' ', '.', 'C', 'G', 'L', 'M', 'P', 'S', 'V', 'a', 'c', 'e', 's', 't']\n",
            " \n",
            "Abstract: Few-shot learning aims to correctly recognize query samples from unseen\n",
            "classes given a limited number of support samples, often by relying on global\n",
            "embeddings of images. In this paper, we propose to equip the backbone network\n",
            "with an attention agent, which is trained by reinforcement learning. The policy\n",
            "gradient algorithm is employed to train the agent towards adaptively localizing\n",
            "the representative regions on feature maps over time. We further design a\n",
            "reward function based on the prediction of the held-out data, thus helping the\n",
            "attention mechanism to generalize better across the unseen classes. The\n",
            "extensive experiments show, with the help of the reinforced attention, that our\n",
            "embedding network has the capability to progressively generate a more\n",
            "discriminative representation in few-shot learning. Moreover, experiments on\n",
            "the task of image classification also show the effectiveness of the proposed\n",
            "design.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Tissue texture is known to exhibit a heterogeneous or non-stationary nature,\n",
            "therefore using a single resolution approach for optimum classification might\n",
            "not suffice. A clinical decision support system that exploits the subband\n",
            "textural fractal characteristics for best bases selection of meningioma brain\n",
            "histopathological image classification is proposed. Each subband is analysed\n",
            "using its fractal dimension instead of energy, which has the advantage of being\n",
            "less sensitive to image intensity and abrupt changes in tissue texture. The\n",
            "most significant subband that best identifies texture discontinuities will be\n",
            "chosen for further decomposition, and its fractal characteristics would\n",
            "represent the optimal feature vector for classification. The performance was\n",
            "tested using the support vector machine (SVM), Bayesian and k-nearest neighbour\n",
            "(kNN) classifiers and a leave-one-patient-out method was employed for\n",
            "validation. Our method outperformed the classical energy based selection\n",
            "approaches, achieving for SVM, Bayesian and kNN classifiers an overall\n",
            "classification accuracy of 94.12%, 92.50% and 79.70%, as compared to 86.31%,\n",
            "83.19% and 51.63% for the co-occurrence matrix, and 76.01%, 73.50% and 50.69%\n",
            "for the energy texture signatures, respectively. These results indicate the\n",
            "potential usefulness as a decision support system that could complement\n",
            "radiologists diagnostic capability to discriminate higher order statistical\n",
            "textural information, for which it would be otherwise difficult via ordinary\n",
            "human vision.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Graph neural networks (GNNs) have achieved great success in many graph-based\n",
            "tasks. Much work is dedicated to empowering GNNs with the adaptive locality\n",
            "ability, which enables measuring the importance of neighboring nodes to the\n",
            "target node by a node-specific mechanism. However, the current node-specific\n",
            "mechanisms are deficient in distinguishing the importance of nodes in the\n",
            "topology structure. We believe that the structural importance of neighboring\n",
            "nodes is closely related to their importance in aggregation. In this paper, we\n",
            "introduce discrete graph curvature (the Ricci curvature) to quantify the\n",
            "strength of structural connection of pairwise nodes. And we propose Curvature\n",
            "Graph Neural Network (CGNN), which effectively improves the adaptive locality\n",
            "ability of GNNs by leveraging the structural property of graph curvature. To\n",
            "improve the adaptability of curvature to various datasets, we explicitly\n",
            "transform curvature into the weights of neighboring nodes by the necessary\n",
            "Negative Curvature Processing Module and Curvature Normalization Module. Then,\n",
            "we conduct numerous experiments on various synthetic datasets and real-world\n",
            "datasets. The experimental results on synthetic datasets show that CGNN\n",
            "effectively exploits the topology structure information, and the performance is\n",
            "improved significantly. CGNN outperforms the baselines on 5 dense node\n",
            "classification benchmark datasets. This study deepens the understanding of how\n",
            "to utilize advanced topology information and assign the importance of\n",
            "neighboring nodes from the perspective of graph curvature and encourages us to\n",
            "bridge the gap between graph theory and neural networks.\n",
            "Label(s): [' ', '.', 'A', 'G', 'I', 'L', 'c', 's']\n",
            " \n",
            "Abstract: Deep reinforcement learning has shown promising results in learning control\n",
            "policies for complex sequential decision-making tasks. However, these neural\n",
            "network-based policies are known to be vulnerable to adversarial examples. This\n",
            "vulnerability poses a potentially serious threat to safety-critical systems\n",
            "such as autonomous vehicles. In this paper, we propose a defense mechanism to\n",
            "defend reinforcement learning agents from adversarial attacks by leveraging an\n",
            "action-conditioned frame prediction module. Our core idea is that the\n",
            "adversarial examples targeting at a neural network-based policy are not\n",
            "effective for the frame prediction model. By comparing the action distribution\n",
            "produced by a policy from processing the current observed frame to the action\n",
            "distribution produced by the same policy from processing the predicted frame\n",
            "from the action-conditioned frame prediction module, we can detect the presence\n",
            "of adversarial examples. Beyond detecting the presence of adversarial examples,\n",
            "our method allows the agent to continue performing the task using the predicted\n",
            "frame when the agent is under attack. We evaluate the performance of our\n",
            "algorithm using five games in Atari 2600. Our results demonstrate that the\n",
            "proposed defense mechanism achieves favorable performance against baseline\n",
            "algorithms in detecting adversarial examples and in earning rewards when the\n",
            "agents are under attack.\n",
            "Label(s): [' ', '.', 'C', 'G', 'L', 'R', 'V', 'c', 's']\n",
            " \n",
            "Abstract: This work presents a new fine-grained transparent object segmentation\n",
            "dataset, termed Trans10K-v2, extending Trans10K-v1, the first large-scale\n",
            "transparent object segmentation dataset. Unlike Trans10K-v1 that only has two\n",
            "limited categories, our new dataset has several appealing benefits. (1) It has\n",
            "11 fine-grained categories of transparent objects, commonly occurring in the\n",
            "human domestic environment, making it more practical for real-world\n",
            "application. (2) Trans10K-v2 brings more challenges for the current advanced\n",
            "segmentation methods than its former version. Furthermore, a novel\n",
            "transformer-based segmentation pipeline termed Trans2Seg is proposed. Firstly,\n",
            "the transformer encoder of Trans2Seg provides the global receptive field in\n",
            "contrast to CNN's local receptive field, which shows excellent advantages over\n",
            "pure CNN architectures. Secondly, by formulating semantic segmentation as a\n",
            "problem of dictionary look-up, we design a set of learnable prototypes as the\n",
            "query of Trans2Seg's transformer decoder, where each prototype learns the\n",
            "statistics of one category in the whole dataset. We benchmark more than 20\n",
            "recent semantic segmentation methods, demonstrating that Trans2Seg\n",
            "significantly outperforms all the CNN-based methods, showing the proposed\n",
            "algorithm's potential ability to solve transparent object segmentation.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Object detection in wide area motion imagery (WAMI) has drawn the attention\n",
            "of the computer vision research community for a number of years. WAMI proposes\n",
            "a number of unique challenges including extremely small object sizes, both\n",
            "sparse and densely-packed objects, and extremely large search spaces (large\n",
            "video frames). Nearly all state-of-the-art methods in WAMI object detection\n",
            "report that appearance-based classifiers fail in this challenging data and\n",
            "instead rely almost entirely on motion information in the form of background\n",
            "subtraction or frame-differencing. In this work, we experimentally verify the\n",
            "failure of appearance-based classifiers in WAMI, such as Faster R-CNN and a\n",
            "heatmap-based fully convolutional neural network (CNN), and propose a novel\n",
            "two-stage spatio-temporal CNN which effectively and efficiently combines both\n",
            "appearance and motion information to significantly surpass the state-of-the-art\n",
            "in WAMI object detection. To reduce the large search space, the first stage\n",
            "(ClusterNet) takes in a set of extremely large video frames, combines the\n",
            "motion and appearance information within the convolutional architecture, and\n",
            "proposes regions of objects of interest (ROOBI). These ROOBI can contain from\n",
            "one to clusters of several hundred objects due to the large video frame size\n",
            "and varying object density in WAMI. The second stage (FoveaNet) then estimates\n",
            "the centroid location of all objects in that given ROOBI simultaneously via\n",
            "heatmap estimation. The proposed method exceeds state-of-the-art results on the\n",
            "WPAFB 2009 dataset by 5-16% for moving objects and nearly 50% for stopped\n",
            "objects, as well as being the first proposed method in wide area motion imagery\n",
            "to detect completely stationary objects.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: With various face presentation attacks arising under unseen scenarios, face\n",
            "anti-spoofing (FAS) based on domain generalization (DG) has drawn growing\n",
            "attention due to its robustness. Most existing methods utilize DG frameworks to\n",
            "align the features to seek a compact and generalized feature space. However,\n",
            "little attention has been paid to the feature extraction process for the FAS\n",
            "task, especially the influence of normalization, which also has a great impact\n",
            "on the generalization of the learned representation. To address this issue, we\n",
            "propose a novel perspective of face anti-spoofing that focuses on the\n",
            "normalization selection in the feature extraction process. Concretely, an\n",
            "Adaptive Normalized Representation Learning (ANRL) framework is devised, which\n",
            "adaptively selects feature normalization methods according to the inputs,\n",
            "aiming to learn domain-agnostic and discriminative representation. Moreover, to\n",
            "facilitate the representation learning, Dual Calibration Constraints are\n",
            "designed, including Inter-Domain Compatible loss and Inter-Class Separable\n",
            "loss, which provide a better optimization direction for generalizable\n",
            "representation. Extensive experiments and visualizations are presented to\n",
            "demonstrate the effectiveness of our method against the SOTA competitors.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Let $\\mathbf{X} = (X_i)_{1\\leq i \\leq n}$ be an i.i.d. sample of\n",
            "square-integrable variables in $\\mathbb{R}^d$, with common expectation $\\mu$\n",
            "and covariance matrix $\\Sigma$, both unknown. We consider the problem of\n",
            "testing if $\\mu$ is $\\eta$-close to zero, i.e. $\\|\\mu\\| \\leq \\eta $ against\n",
            "$\\|\\mu\\| \\geq (\\eta + \\delta)$; we also tackle the more general two-sample mean\n",
            "closeness testing problem. The aim of this paper is to obtain nonasymptotic\n",
            "upper and lower bounds on the minimal separation distance $\\delta$ such that we\n",
            "can control both the Type I and Type II errors at a given level. The main\n",
            "technical tools are concentration inequalities, first for a suitable estimator\n",
            "of $\\|\\mu\\|^2$ used a test statistic, and secondly for estimating the operator\n",
            "and Frobenius norms of $\\Sigma$ coming into the quantiles of said test\n",
            "statistic. These properties are obtained for Gaussian and bounded\n",
            "distributions. A particular attention is given to the dependence in the\n",
            "pseudo-dimension $d_*$ of the distribution, defined as $d_* :=\n",
            "\\|\\Sigma\\|_2^2/\\|\\Sigma\\|_\\infty^2$. In particular, for $\\eta=0$, the minimum\n",
            "separation distance is ${\\Theta}(d_*^{\\frac{1}{4}}\\sqrt{\\|\\Sigma\\|_\\infty/n})$,\n",
            "in contrast with the minimax estimation distance for $\\mu$, which is\n",
            "${\\Theta}(d_e^{\\frac{1}{2}}\\sqrt{\\|\\Sigma\\|_\\infty/n})$ (where\n",
            "$d_e:=\\|\\Sigma\\|_1/\\|\\Sigma\\|_\\infty$). This generalizes a phenomenon spelled\n",
            "out in particular by Baraud (2002).\n",
            "Label(s): [' ', '.', 'A', 'G', 'H', 'I', 'L', 'M', 'S', 'T', 'a', 'c', 'h', 'm', 's', 't']\n",
            " \n",
            "Abstract: Decomposition methods have been proposed to approximate solutions to large\n",
            "sequential decision making problems. In contexts where an agent interacts with\n",
            "multiple entities, utility decomposition can be used to separate the global\n",
            "objective into local tasks considering each individual entity independently. An\n",
            "arbitrator is then responsible for combining the individual utilities and\n",
            "selecting an action in real time to solve the global problem. Although these\n",
            "techniques can perform well empirically, they rely on strong assumptions of\n",
            "independence between the local tasks and sacrifice the optimality of the global\n",
            "solution. This paper proposes an approach that improves upon such approximate\n",
            "solutions by learning a correction term represented by a neural network. We\n",
            "demonstrate this approach on a fisheries management problem where multiple\n",
            "boats must coordinate to maximize their catch over time as well as on a\n",
            "pedestrian avoidance problem for autonomous driving. In each problem,\n",
            "decomposition methods can scale to multiple boats or pedestrians by using\n",
            "strategies involving one entity. We verify empirically that the proposed\n",
            "correction method significantly improves the decomposition method and\n",
            "outperforms a policy trained on the full scale problem without utility\n",
            "decomposition.\n",
            "Label(s): [' ', '.', 'A', 'G', 'I', 'L', 'c', 's']\n",
            " \n",
            "Abstract: This paper proposes to use Fast Fourier Transformation-based U-Net (a refined\n",
            "fully convolutional networks) and perform image convolution in neural networks.\n",
            "Leveraging the Fast Fourier Transformation, it reduces the image convolution\n",
            "costs involved in the Convolutional Neural Networks (CNNs) and thus reduces the\n",
            "overall computational costs. The proposed model identifies the object\n",
            "information from the images. We apply the Fast Fourier transform algorithm on\n",
            "an image data set to obtain more accessible information about the image data,\n",
            "before segmenting them through the U-Net architecture. More specifically, we\n",
            "implement the FFT-based convolutional neural network to improve the training\n",
            "time of the network. The proposed approach was applied to publicly available\n",
            "Broad Bioimage Benchmark Collection (BBBC) dataset. Our model demonstrated\n",
            "improvement in training time during convolution from $600-700$ ms/step to\n",
            "$400-500$ ms/step. We evaluated the accuracy of our model using Intersection\n",
            "over Union (IoU) metric showing significant improvements.\n",
            "Label(s): [' ', '.', 'C', 'G', 'I', 'L', 'V', 'c', 'e', 's']\n",
            " \n",
            "Abstract: Remarkable achievements have been attained by deep neural networks in various\n",
            "applications. However, the increasing depth and width of such models also lead\n",
            "to explosive growth in both storage and computation, which has restricted the\n",
            "deployment of deep neural networks on resource-limited edge devices. To address\n",
            "this problem, we propose the so-called SCAN framework for networks training and\n",
            "inference, which is orthogonal and complementary to existing acceleration and\n",
            "compression methods. The proposed SCAN firstly divides neural networks into\n",
            "multiple sections according to their depth and constructs shallow classifiers\n",
            "upon the intermediate features of different sections. Moreover, attention\n",
            "modules and knowledge distillation are utilized to enhance the accuracy of\n",
            "shallow classifiers. Based on this architecture, we further propose a threshold\n",
            "controlled scalable inference mechanism to approach human-like sample-specific\n",
            "inference. Experimental results show that SCAN can be easily equipped on\n",
            "various neural networks without any adjustment on hyper-parameters or neural\n",
            "networks architectures, yielding significant performance gain on CIFAR100 and\n",
            "ImageNet. Codes will be released on github soon.\n",
            "Label(s): [' ', '.', 'C', 'G', 'L', 'M', 'V', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: We propose to extend the concept of private information retrieval by allowing\n",
            "for distortion in the retrieval process and relaxing the perfect privacy\n",
            "requirement at the same time. In particular, we study the tradeoff between\n",
            "download rate, distortion, and user privacy leakage, and show that in the limit\n",
            "of large file sizes this trade-off can be captured via a novel\n",
            "information-theoretical formulation for datasets with a known distribution.\n",
            "Moreover, for scenarios where the statistics of the dataset is unknown, we\n",
            "propose a new deep learning framework by leveraging a generative adversarial\n",
            "network approach, which allows the user to learn efficient schemes from the\n",
            "data itself, minimizing the download cost. We evaluate the performance of the\n",
            "scheme on a synthetic Gaussian dataset as well as on both the MNIST and\n",
            "CIFAR-10 datasets. For the MNIST dataset, the data-driven approach\n",
            "significantly outperforms a non-learning based scheme which combines source\n",
            "coding with multiple file download, while the CIFAR-10 performance is notably\n",
            "better.\n",
            "Label(s): [' ', '.', 'G', 'I', 'L', 'T', 'a', 'c', 'h', 'm', 's', 't']\n",
            " \n",
            "Abstract: Convolutional Neural Networks (ConvNets) have successfully contributed to\n",
            "improve the accuracy of regression-based methods for computer vision tasks such\n",
            "as human pose estimation, landmark localization, and object detection. The\n",
            "network optimization has been usually performed with L2 loss and without\n",
            "considering the impact of outliers on the training process, where an outlier in\n",
            "this context is defined by a sample estimation that lies at an abnormal\n",
            "distance from the other training sample estimations in the objective space. In\n",
            "this work, we propose a regression model with ConvNets that achieves robustness\n",
            "to such outliers by minimizing Tukey's biweight function, an M-estimator robust\n",
            "to outliers, as the loss function for the ConvNet. In addition to the robust\n",
            "loss, we introduce a coarse-to-fine model, which processes input images of\n",
            "progressively higher resolutions for improving the accuracy of the regressed\n",
            "values. In our experiments, we demonstrate faster convergence and better\n",
            "generalization of our robust loss function for the tasks of human pose\n",
            "estimation and age estimation from face images. We also show that the\n",
            "combination of the robust loss function with the coarse-to-fine model produces\n",
            "comparable or better results than current state-of-the-art approaches in four\n",
            "publicly available human pose estimation datasets.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Previous work on adversarially robust neural networks for image\n",
            "classification requires large training sets and computationally expensive\n",
            "training procedures. On the other hand, few-shot learning methods are highly\n",
            "vulnerable to adversarial examples. The goal of our work is to produce networks\n",
            "which both perform well at few-shot classification tasks and are simultaneously\n",
            "robust to adversarial examples. We develop an algorithm, called Adversarial\n",
            "Querying (AQ), for producing adversarially robust meta-learners, and we\n",
            "thoroughly investigate the causes for adversarial vulnerability. Moreover, our\n",
            "method achieves far superior robust performance on few-shot image\n",
            "classification tasks, such as Mini-ImageNet and CIFAR-FS, than robust transfer\n",
            "learning.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Experience replay is widely used in deep reinforcement learning algorithms\n",
            "and allows agents to remember and learn from experiences from the past. In an\n",
            "effort to learn more efficiently, researchers proposed prioritized experience\n",
            "replay (PER) which samples important transitions more frequently. In this\n",
            "paper, we propose Prioritized Sequence Experience Replay (PSER) a framework for\n",
            "prioritizing sequences of experience in an attempt to both learn more\n",
            "efficiently and to obtain better performance. We compare the performance of PER\n",
            "and PSER sampling techniques in a tabular Q-learning environment and in DQN on\n",
            "the Atari 2600 benchmark. We prove theoretically that PSER is guaranteed to\n",
            "converge faster than PER and empirically show PSER substantially improves upon\n",
            "PER.\n",
            "Label(s): [' ', '.', 'A', 'G', 'I', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Edge preserving filters preserve the edges and its information while blurring\n",
            "an image. In other words they are used to smooth an image, while reducing the\n",
            "edge blurring effects across the edge like halos, phantom etc. They are\n",
            "nonlinear in nature. Examples are bilateral filter, anisotropic diffusion\n",
            "filter, guided filter, trilateral filter etc. Hence these family of filters are\n",
            "very useful in reducing the noise in an image making it very demanding in\n",
            "computer vision and computational photography applications like denoising,\n",
            "video abstraction, demosaicing, optical-flow estimation, stereo matching, tone\n",
            "mapping, style transfer, relighting etc. This paper provides a concrete\n",
            "introduction to edge preserving filters starting from the heat diffusion\n",
            "equation in olden to recent eras, an overview of its numerous applications, as\n",
            "well as mathematical analysis, various efficient and optimized ways of\n",
            "implementation and their interrelationships, keeping focus on preserving the\n",
            "boundaries, spikes and canyons in presence of noise. Furthermore it provides a\n",
            "realistic notion for efficient implementation with a research scope for\n",
            "hardware realization for further acceleration.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: We propose a novel universal detector for detecting images generated by using\n",
            "CNNs. In this paper, properties of checkerboard artifacts in CNN-generated\n",
            "images are considered, and the spectrum of images is enhanced in accordance\n",
            "with the properties. Next, a classifier is trained by using the enhanced\n",
            "spectrums to judge a query image to be a CNN-generated ones or not. In\n",
            "addition, an ensemble of the proposed detector with emphasized spectrums and a\n",
            "conventional detector is proposed to improve the performance of these methods.\n",
            "In an experiment, the proposed ensemble is demonstrated to outperform a\n",
            "state-of-the-art method under some conditions.\n",
            "Label(s): [' ', '.', 'C', 'I', 'V', 'c', 'e', 's']\n",
            " \n",
            "Abstract: Scene Designer is a novel method for searching and generating images using\n",
            "free-hand sketches of scene compositions; i.e. drawings that describe both the\n",
            "appearance and relative positions of objects. Our core contribution is a single\n",
            "unified model to learn both a cross-modal search embedding for matching\n",
            "sketched compositions to images, and an object embedding for layout synthesis.\n",
            "We show that a graph neural network (GNN) followed by Transformer under our\n",
            "novel contrastive learning setting is required to allow learning correlations\n",
            "between object type, appearance and arrangement, driving a mask generation\n",
            "module that synthesises coherent scene layouts, whilst also delivering state of\n",
            "the art sketch based visual search of scenes.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Model-free deep reinforcement learning (RL) algorithms have been successfully\n",
            "applied to a range of challenging sequential decision making and control tasks.\n",
            "However, these methods typically suffer from two major challenges: high sample\n",
            "complexity and brittleness to hyperparameters. Both of these challenges limit\n",
            "the applicability of such methods to real-world domains. In this paper, we\n",
            "describe Soft Actor-Critic (SAC), our recently introduced off-policy\n",
            "actor-critic algorithm based on the maximum entropy RL framework. In this\n",
            "framework, the actor aims to simultaneously maximize expected return and\n",
            "entropy. That is, to succeed at the task while acting as randomly as possible.\n",
            "We extend SAC to incorporate a number of modifications that accelerate training\n",
            "and improve stability with respect to the hyperparameters, including a\n",
            "constrained formulation that automatically tunes the temperature\n",
            "hyperparameter. We systematically evaluate SAC on a range of benchmark tasks,\n",
            "as well as real-world challenging tasks such as locomotion for a quadrupedal\n",
            "robot and robotic manipulation with a dexterous hand. With these improvements,\n",
            "SAC achieves state-of-the-art performance, outperforming prior on-policy and\n",
            "off-policy methods in sample-efficiency and asymptotic performance.\n",
            "Furthermore, we demonstrate that, in contrast to other off-policy algorithms,\n",
            "our approach is very stable, achieving similar performance across different\n",
            "random seeds. These results suggest that SAC is a promising candidate for\n",
            "learning in real-world robotics tasks.\n",
            "Label(s): [' ', '.', 'A', 'G', 'I', 'L', 'M', 'O', 'R', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: We propose an end-to-end deep learning architecture that produces a 3D shape\n",
            "in triangular mesh from a single color image. Limited by the nature of deep\n",
            "neural network, previous methods usually represent a 3D shape in volume or\n",
            "point cloud, and it is non-trivial to convert them to the more ready-to-use\n",
            "mesh model. Unlike the existing methods, our network represents 3D mesh in a\n",
            "graph-based convolutional neural network and produces correct geometry by\n",
            "progressively deforming an ellipsoid, leveraging perceptual features extracted\n",
            "from the input image. We adopt a coarse-to-fine strategy to make the whole\n",
            "deformation procedure stable, and define various of mesh related losses to\n",
            "capture properties of different levels to guarantee visually appealing and\n",
            "physically accurate 3D geometry. Extensive experiments show that our method not\n",
            "only qualitatively produces mesh model with better details, but also achieves\n",
            "higher 3D shape estimation accuracy compared to the state-of-the-art.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: We present an approach to labeling short video clips with English verbs as\n",
            "event descriptions. A key distinguishing aspect of this work is that it labels\n",
            "videos with verbs that describe the spatiotemporal interaction between event\n",
            "participants, humans and objects interacting with each other, abstracting away\n",
            "all object-class information and fine-grained image characteristics, and\n",
            "relying solely on the coarse-grained motion of the event participants. We apply\n",
            "our approach to a large set of 22 distinct verb classes and a corpus of 2,584\n",
            "videos, yielding two surprising outcomes. First, a classification accuracy of\n",
            "greater than 70% on a 1-out-of-22 labeling task and greater than 85% on a\n",
            "variety of 1-out-of-10 subsets of this labeling task is independent of the\n",
            "choice of which of two different time-series classifiers we employ. Second, we\n",
            "achieve this level of accuracy using a highly impoverished intermediate\n",
            "representation consisting solely of the bounding boxes of one or two event\n",
            "participants as a function of time. This indicates that successful event\n",
            "recognition depends more on the choice of appropriate features that\n",
            "characterize the linguistic invariants of the event classes than on the\n",
            "particular classifier algorithms.\n",
            "Label(s): [' ', '.', 'A', 'C', 'I', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Modeling financial time series is challenging due to their high volatility\n",
            "and unexpected happenings on the market. Most financial models and algorithms\n",
            "trying to fill the lack of historical financial time series struggle to perform\n",
            "and are highly vulnerable to overfitting. As an alternative, we introduce in\n",
            "this paper a deep neural network called the WGAN-GP, a data-driven model that\n",
            "focuses on sample generation. The WGAN-GP consists of a generator and\n",
            "discriminator function which utilize an LSTM architecture. The WGAN-GP is\n",
            "supposed to learn the underlying structure of the input data, which in our\n",
            "case, is the Bitcoin. Bitcoin is unique in its behavior; the prices fluctuate\n",
            "what makes guessing the price trend hardly impossible. Through adversarial\n",
            "training, the WGAN-GP should learn the underlying structure of the bitcoin and\n",
            "generate very similar samples of the bitcoin distribution. The generated\n",
            "synthetic time series are visually indistinguishable from the real data. But\n",
            "the numerical results show that the generated data were close to the real data\n",
            "distribution but distinguishable. The model mainly shows a stable learning\n",
            "behavior. However, the model has space for optimization, which could be\n",
            "achieved by adjusting the hyperparameters.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: We propose a real-time RGB-based pipeline for object detection and 6D pose\n",
            "estimation. Our novel 3D orientation estimation is based on a variant of the\n",
            "Denoising Autoencoder that is trained on simulated views of a 3D model using\n",
            "Domain Randomization. This so-called Augmented Autoencoder has several\n",
            "advantages over existing methods: It does not require real, pose-annotated\n",
            "training data, generalizes to various test sensors and inherently handles\n",
            "object and view symmetries. Instead of learning an explicit mapping from input\n",
            "images to object poses, it provides an implicit representation of object\n",
            "orientations defined by samples in a latent space. Our pipeline achieves\n",
            "state-of-the-art performance on the T-LESS dataset both in the RGB and RGB-D\n",
            "domain. We also evaluate on the LineMOD dataset where we can compete with other\n",
            "synthetically trained approaches. We further increase performance by correcting\n",
            "3D orientation estimates to account for perspective errors when the object\n",
            "deviates from the image center and show extended results.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Traditional generalization bounds are based on analyzing the limits of the\n",
            "model capacity. Therefore, they become vacuous in the \\emph{interpolation}\n",
            "(over-parameterized) regime of modern machine learning models where training\n",
            "data can be fitted perfectly. This paper proposes a new approach to meaningful\n",
            "generalization bounds in the interpolation regime by decomposing the\n",
            "generalization gap into a notion of \\emph{representativeness} and \\emph{feature\n",
            "robustness}. Representativeness captures properties of the data distribution\n",
            "and mitigates the dependence on the data dimension by exploiting the\n",
            "low-dimensional feature representation used implicitly by the model, and\n",
            "feature robustness captures the expected change in loss resulting from\n",
            "perturbations of these implicit features. We show that feature robustness can\n",
            "be bounded by a relative flatness measure of the empirical loss surface for\n",
            "models that locally minimize the training loss. This yields an\n",
            "algorithm-agnostic bound potentially explaining the abundance of empirical\n",
            "observations that flatness of the loss surface is correlated with\n",
            "generalization.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: We present a novel method to learn temporally consistent 3D reconstruction of\n",
            "clothed people from a monocular video. Recent methods for 3D human\n",
            "reconstruction from monocular video using volumetric, implicit or parametric\n",
            "human shape models, produce per frame reconstructions giving temporally\n",
            "inconsistent output and limited performance when applied to video. In this\n",
            "paper, we introduce an approach to learn temporally consistent features for\n",
            "textured reconstruction of clothed 3D human sequences from monocular video by\n",
            "proposing two advances: a novel temporal consistency loss function; and hybrid\n",
            "representation learning for implicit 3D reconstruction from 2D images and\n",
            "coarse 3D geometry. The proposed advances improve the temporal consistency and\n",
            "accuracy of both the 3D reconstruction and texture prediction from a monocular\n",
            "video. Comprehensive comparative performance evaluation on images of people\n",
            "demonstrates that the proposed method significantly outperforms the\n",
            "state-of-the-art learning-based single image 3D human shape estimation\n",
            "approaches achieving significant improvement of reconstruction accuracy,\n",
            "completeness, quality and temporal consistency.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Neural architecture search (NAS) is proposed to automate the architecture\n",
            "design process and attracts overwhelming interest from both academia and\n",
            "industry. However, it is confronted with overfitting issue due to the\n",
            "high-dimensional search space composed by operator selection and skip\n",
            "connection of each layer. This paper explores the architecture overfitting\n",
            "issue in depth based on the reinforcement learning-based NAS framework. We show\n",
            "that the policy gradient method has deep correlations with the cross entropy\n",
            "minimization. Based on this correlation, we further demonstrate that, though\n",
            "the reward of NAS is sparse, the policy gradient method implicitly assign the\n",
            "reward to all operations and skip connections based on the sampling frequency.\n",
            "However, due to the inaccurate reward estimation, curse of dimensionality\n",
            "problem and the hierachical structure of neural networks, reward charateristics\n",
            "for operators and skip connections have intrinsic differences, the assigned\n",
            "rewards for the skip connections are extremely noisy and inaccurate. To\n",
            "alleviate this problem, we propose a neural architecture refinement approach\n",
            "that working with an initial state-of-the-art network structure and only\n",
            "refining its operators. Extensive experiments have demonstrated that the\n",
            "proposed method can achieve fascinated results, including classification, face\n",
            "recognition etc.\n",
            "Label(s): [' ', '.', 'E', 'G', 'L', 'N', 'c', 's']\n",
            " \n",
            "Abstract: Although the human visual system is surprisingly robust to extreme distortion\n",
            "when recognizing objects, most evaluations of computer object detection methods\n",
            "focus only on robustness to natural form deformations such as people's pose\n",
            "changes. To determine whether algorithms truly mirror the flexibility of human\n",
            "vision, they must be compared against human vision at its limits. For example,\n",
            "in Cubist abstract art, painted objects are distorted by object fragmentation\n",
            "and part-reorganization, to the point that human vision often fails to\n",
            "recognize them. In this paper, we evaluate existing object detection methods on\n",
            "these abstract renditions of objects, comparing human annotators to four\n",
            "state-of-the-art object detectors on a corpus of Picasso paintings. Our results\n",
            "demonstrate that while human perception significantly outperforms current\n",
            "methods, human perception and part-based models exhibit a similarly graceful\n",
            "degradation in object detection performance as the objects become increasingly\n",
            "abstract and fragmented, corroborating the theory of part-based object\n",
            "representation in the brain.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: After thirty years of the GIF file format, today is becoming more popular\n",
            "than ever: being a great way of communication for friends and communities on\n",
            "Instant Messengers and Social Networks. While being so popular, the original\n",
            "compression method to encode GIF images have not changed a bit. On the other\n",
            "hand popularity means that storage saving becomes an issue for hosting\n",
            "platforms. In this paper a parametric optimization technique for animated GIFs\n",
            "will be presented. The proposed technique is based on Local Color Table\n",
            "selection and color remapping in order to create optimized animated GIFs while\n",
            "preserving the original format. The technique achieves good results in terms of\n",
            "byte reduction with limited or no loss of perceived color quality. Tests\n",
            "carried out on 1000 GIF files demonstrate the effectiveness of the proposed\n",
            "optimization strategy.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Recent contributions have demonstrated that it is possible to recognize the\n",
            "pose of humans densely and accurately given a large dataset of poses annotated\n",
            "in detail. In principle, the same approach could be extended to any animal\n",
            "class, but the effort required for collecting new annotations for each case\n",
            "makes this strategy impractical, despite important applications in natural\n",
            "conservation, science and business. We show that, at least for proximal animal\n",
            "classes such as chimpanzees, it is possible to transfer the knowledge existing\n",
            "in dense pose recognition for humans, as well as in more general object\n",
            "detectors and segmenters, to the problem of dense pose recognition in other\n",
            "classes. We do this by (1) establishing a DensePose model for the new animal\n",
            "which is also geometrically aligned to humans (2) introducing a multi-head\n",
            "R-CNN architecture that facilitates transfer of multiple recognition tasks\n",
            "between classes, (3) finding which combination of known classes can be\n",
            "transferred most effectively to the new animal and (4) using self-calibrated\n",
            "uncertainty heads to generate pseudo-labels graded by quality for training a\n",
            "model for this class. We also introduce two benchmark datasets labelled in the\n",
            "manner of DensePose for the class chimpanzee and use them to evaluate our\n",
            "approach, showing excellent transfer learning performance.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: In this paper, we analyze the statistics of error signals to assess the\n",
            "perceived quality of images. Specifically, we focus on the magnitude spectrum\n",
            "of error images obtained from the difference of reference and distorted images.\n",
            "Analyzing spectral statistics over grayscale images partially models\n",
            "interference in spatial harmonic distortion exhibited by the visual system but\n",
            "it overlooks color information, selective and hierarchical nature of visual\n",
            "system. To overcome these shortcomings, we introduce an image quality\n",
            "assessment algorithm based on the Spectral Understanding of Multi-scale and\n",
            "Multi-channel Error Representations, denoted as SUMMER. We validate the quality\n",
            "assessment performance over 3 databases with around 30 distortion types. These\n",
            "distortion types are grouped into 7 main categories as compression artifact,\n",
            "image noise, color artifact, communication error, blur, global and local\n",
            "distortions. In total, we benchmark the performance of 17 algorithms along with\n",
            "the proposed algorithm using 5 performance metrics that measure linearity,\n",
            "monotonicity, accuracy, and consistency. In addition to experiments with\n",
            "standard performance metrics, we analyze the distribution of objective and\n",
            "subjective scores with histogram difference metrics and scatter plots.\n",
            "Moreover, we analyze the classification performance of quality assessment\n",
            "algorithms along with their statistical significance tests. Based on our\n",
            "experiments, SUMMER significantly outperforms majority of the compared methods\n",
            "in all benchmark categories\n",
            "Label(s): [' ', '.', '4', 'C', 'I', 'V', 'c', 'e', 's']\n",
            " \n",
            "Abstract: We consider the exploration-exploitation dilemma in finite-horizon\n",
            "reinforcement learning (RL). When the state space is large or continuous,\n",
            "traditional tabular approaches are unfeasible and some form of function\n",
            "approximation is mandatory. In this paper, we introduce an\n",
            "optimistically-initialized variant of the popular randomized least-squares\n",
            "value iteration (RLSVI), a model-free algorithm where exploration is induced by\n",
            "perturbing the least-squares approximation of the action-value function. Under\n",
            "the assumption that the Markov decision process has low-rank transition\n",
            "dynamics, we prove that the frequentist regret of RLSVI is upper-bounded by\n",
            "$\\widetilde O(d^2 H^2 \\sqrt{T})$ where $ d $ are the feature dimension, $ H $\n",
            "is the horizon, and $ T $ is the total number of steps. To the best of our\n",
            "knowledge, this is the first frequentist regret analysis for randomized\n",
            "exploration with function approximation.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Generating high fidelity identity-preserving faces with different facial\n",
            "attributes has a wide range of applications. Although a number of generative\n",
            "models have been developed to tackle this problem, there is still much room for\n",
            "further improvement.In paticular, the current solutions usually ignore the\n",
            "perceptual information of images, which we argue that it benefits the output of\n",
            "a high-quality image while preserving the identity information, especially in\n",
            "facial attributes learning area.To this end, we propose to train GAN\n",
            "iteratively via regularizing the min-max process with an integrated loss, which\n",
            "includes not only the per-pixel loss but also the perceptual loss. In contrast\n",
            "to the existing methods only deal with either image generation or\n",
            "transformation, our proposed iterative architecture can achieve both of them.\n",
            "Experiments on the multi-label facial dataset CelebA demonstrate that the\n",
            "proposed model has excellent performance on recognizing multiple attributes,\n",
            "generating a high-quality image, and transforming image with controllable\n",
            "attributes.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Large datasets represented by multidimensional data point clouds often\n",
            "possess non-trivial distributions with branching trajectories and excluded\n",
            "regions, with the recent single-cell transcriptomic studies of developing\n",
            "embryo being notable examples. Reducing the complexity and producing compact\n",
            "and interpretable representations of such data remains a challenging task. Most\n",
            "of the existing computational methods are based on exploring the local data\n",
            "point neighbourhood relations, a step that can perform poorly in the case of\n",
            "multidimensional and noisy data. Here we present ElPiGraph, a scalable and\n",
            "robust method for approximation of datasets with complex structures which does\n",
            "not require computing the complete data distance matrix or the data point\n",
            "neighbourhood graph. This method is able to withstand high levels of noise and\n",
            "is capable of approximating complex topologies via principal graph ensembles\n",
            "that can be combined into a consensus principal graph. ElPiGraph deals\n",
            "efficiently with large and complex datasets in various fields from biology,\n",
            "where it can be used to infer gene dynamics from single-cell RNA-Seq, to\n",
            "astronomy, where it can be used to explore complex structures in the\n",
            "distribution of galaxies.\n",
            "Label(s): [' ', '-', '.', 'G', 'L', 'M', 'Q', 'a', 'b', 'c', 'i', 'o', 'q', 's', 't']\n",
            " \n",
            "Abstract: Contrastive self-supervised learning (SSL) has achieved great success in\n",
            "unsupervised visual representation learning by maximizing the similarity\n",
            "between two augmented views of the same image (positive pairs) and\n",
            "simultaneously contrasting other different images (negative pairs). However,\n",
            "this type of methods, such as SimCLR and MoCo, relies heavily on a large number\n",
            "of negative pairs and thus requires either large batches or memory banks. In\n",
            "contrast, some recent non-contrastive SSL methods, such as BYOL and SimSiam,\n",
            "attempt to discard negative pairs by introducing asymmetry and show remarkable\n",
            "performance. Unfortunately, to avoid collapsed solutions caused by not using\n",
            "negative pairs, these methods require sophisticated asymmetry designs. In this\n",
            "paper, we argue that negative pairs are still necessary but one is sufficient,\n",
            "i.e., triplet is all you need. A simple triplet-based loss can achieve\n",
            "surprisingly good performance without requiring large batches or asymmetry.\n",
            "Moreover, we observe that unsupervised visual representation learning can gain\n",
            "significantly from randomness. Based on this observation, we propose a simple\n",
            "plug-in RandOm MApping (ROMA) strategy by randomly mapping samples into other\n",
            "spaces and enforcing these randomly projected samples to satisfy the same\n",
            "correlation requirement. The proposed ROMA strategy not only achieves the\n",
            "state-of-the-art performance in conjunction with the triplet-based loss, but\n",
            "also can further effectively boost other SSL methods.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Localizing moments in untrimmed videos via language queries is a new and\n",
            "interesting task that requires the ability to accurately ground language into\n",
            "video. Previous works have approached this task by processing the entire video,\n",
            "often more than once, to localize relevant activities. In the real world\n",
            "applications of this approach, such as video surveillance, efficiency is a key\n",
            "system requirement. In this paper, we present TripNet, an end-to-end system\n",
            "that uses a gated attention architecture to model fine-grained textual and\n",
            "visual representations in order to align text and video content. Furthermore,\n",
            "TripNet uses reinforcement learning to efficiently localize relevant activity\n",
            "clips in long videos, by learning how to intelligently skip around the video.\n",
            "It extracts visual features for few frames to perform activity classification.\n",
            "In our evaluation over Charades-STA, ActivityNet Captions and the TACoS\n",
            "dataset, we find that TripNet achieves high accuracy and saves processing time\n",
            "by only looking at 32-41% of the entire video.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Intelligent agents must pursue their goals in complex environments with\n",
            "partial information and often limited computational capacity. Reinforcement\n",
            "learning methods have achieved great success by creating agents that optimize\n",
            "engineered reward functions, but which often struggle to learn in sparse-reward\n",
            "environments, generally require many environmental interactions to perform\n",
            "well, and are typically computationally very expensive. Active inference is a\n",
            "model-based approach that directs agents to explore uncertain states while\n",
            "adhering to a prior model of their goal behaviour. This paper introduces an\n",
            "active inference agent which minimizes the novel free energy of the expected\n",
            "future. Our model is capable of solving sparse-reward problems with a very high\n",
            "sample efficiency due to its objective function, which encourages directed\n",
            "exploration of uncertain states. Moreover, our model is computationally very\n",
            "light and can operate in a fully online manner while achieving comparable\n",
            "performance to offline RL methods. We showcase the capabilities of our model by\n",
            "solving the mountain car problem, where we demonstrate its superior exploration\n",
            "properties and its robustness to observation noise, which in fact improves\n",
            "performance. We also introduce a novel method for approximating the prior model\n",
            "from the reward function, which simplifies the expression of complex objectives\n",
            "and improves performance over previous active inference approaches.\n",
            "Label(s): [' ', '.', '2', 'A', 'G', 'I', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Learning to hash has been widely applied to approximate nearest neighbor\n",
            "search for large-scale multimedia retrieval, due to its computation efficiency\n",
            "and retrieval quality. Deep learning to hash, which improves retrieval quality\n",
            "by end-to-end representation learning and hash encoding, has received\n",
            "increasing attention recently. Subject to the ill-posed gradient difficulty in\n",
            "the optimization with sign activations, existing deep learning to hash methods\n",
            "need to first learn continuous representations and then generate binary hash\n",
            "codes in a separated binarization step, which suffer from substantial loss of\n",
            "retrieval quality. This work presents HashNet, a novel deep architecture for\n",
            "deep learning to hash by continuation method with convergence guarantees, which\n",
            "learns exactly binary hash codes from imbalanced similarity data. The key idea\n",
            "is to attack the ill-posed gradient problem in optimizing deep networks with\n",
            "non-smooth binary activations by continuation method, in which we begin from\n",
            "learning an easier network with smoothed activation function and let it evolve\n",
            "during the training, until it eventually goes back to being the original,\n",
            "difficult to optimize, deep network with the sign activation function.\n",
            "Comprehensive empirical evidence shows that HashNet can generate exactly binary\n",
            "hash codes and yield state-of-the-art multimedia retrieval performance on\n",
            "standard benchmarks.\n",
            "Label(s): [' ', '.', 'C', 'G', 'L', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Graph neural networks get significant attention for graph representation and\n",
            "classification in machine learning community. Attention mechanism applied on\n",
            "the neighborhood of a node improves the performance of graph neural networks.\n",
            "Typically, it helps to identify a neighbor node which plays more important role\n",
            "to determine the label of the node under consideration. But in real world\n",
            "scenarios, a particular subset of nodes together, but not the individual pairs\n",
            "in the subset, may be important to determine the label of the graph. To address\n",
            "this problem, we introduce the concept of subgraph attention for graphs. On the\n",
            "other hand, hierarchical graph pooling has been shown to be promising in recent\n",
            "literature. But due to noisy hierarchical structure of real world graphs, not\n",
            "all the hierarchies of a graph play equal role for graph classification.\n",
            "Towards this end, we propose a graph classification algorithm called\n",
            "SubGattPool which jointly learns the subgraph attention and employs two\n",
            "different types of hierarchical attention mechanisms to find the important\n",
            "nodes in a hierarchy and the importance of individual hierarchies in a graph.\n",
            "Experimental evaluation with different types of graph classification algorithms\n",
            "shows that SubGattPool is able to improve the state-of-the-art or remains\n",
            "competitive on multiple publicly available graph classification datasets. We\n",
            "conduct further experiments on both synthetic and real world graph datasets to\n",
            "justify the usefulness of different components of SubGattPool and to show its\n",
            "consistent performance on other downstream tasks.\n",
            "Label(s): [' ', '.', 'G', 'I', 'L', 'S', 'c', 's']\n",
            " \n",
            "Abstract: Increasing the mini-batch size for stochastic gradient descent offers\n",
            "significant opportunities to reduce wall-clock training time, but there are a\n",
            "variety of theoretical and systems challenges that impede the widespread\n",
            "success of this technique. We investigate these issues, with an emphasis on\n",
            "time to convergence and total computational cost, through an extensive\n",
            "empirical analysis of network training across several architectures and problem\n",
            "domains, including image classification, image segmentation, and language\n",
            "modeling. Although it is common practice to increase the batch size in order to\n",
            "fully exploit available computational resources, we find a substantially more\n",
            "nuanced picture. Our main finding is that across a wide range of network\n",
            "architectures and problem domains, increasing the batch size beyond a certain\n",
            "point yields no decrease in wall-clock time to convergence for \\emph{either}\n",
            "train or test loss. This batch size is usually substantially below the capacity\n",
            "of current systems. We show that popular training strategies for large batch\n",
            "size optimization begin to fail before we can populate all available compute\n",
            "resources, and we show that the point at which these methods break down depends\n",
            "more on attributes like model architecture and data complexity than it does\n",
            "directly on the size of the dataset.\n",
            "Label(s): [' ', '.', 'C', 'D', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Reconstruction based on the stereo camera has received considerable attention\n",
            "recently, but two particular challenges still remain. The first concerns the\n",
            "need to aggregate similar pixels in an effective approach, and the second is to\n",
            "maintain as much of the available information as possible while ensuring\n",
            "sufficient accuracy. To overcome these issues, we propose a new 3D\n",
            "representation method, namely, planecell, that extracts planarity from the\n",
            "depth-assisted image segmentation and then projects these depth planes into the\n",
            "3D world. An energy function formulated from Conditional Random Field that\n",
            "generalizes the planar relationships is maximized to merge coplanar segments.\n",
            "We evaluate our method with a variety of reconstruction baselines on both KITTI\n",
            "and Middlebury datasets, and the results indicate the superiorities compared to\n",
            "other 3D space representation methods in accuracy, memory requirements and\n",
            "further applications.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: High-definition map (HD map) construction is a crucial problem for autonomous\n",
            "driving. This problem typically involves collecting high-quality point clouds,\n",
            "fusing multiple point clouds of the same scene, annotating map elements, and\n",
            "updating maps constantly. This pipeline, however, requires a vast amount of\n",
            "human efforts and resources which limits its scalability. Additionally,\n",
            "traditional HD maps are coupled with centimeter-level accurate localization\n",
            "which is unreliable in many scenarios. In this paper, we argue that online map\n",
            "learning, which dynamically constructs the HD maps based on local sensor\n",
            "observations, is a more scalable way to provide semantic and geometry priors to\n",
            "self-driving vehicles than traditional pre-annotated HD maps. Meanwhile, we\n",
            "introduce an online map learning method, titled HDMapNet. It encodes image\n",
            "features from surrounding cameras and/or point clouds from LiDAR, and predicts\n",
            "vectorized map elements in the bird's-eye view. We benchmark HDMapNet on the\n",
            "nuScenes dataset and show that in all settings, it performs better than\n",
            "baseline methods. Of note, our fusion-based HDMapNet outperforms existing\n",
            "methods by more than 50% in all metrics. To accelerate future research, we\n",
            "develop customized metrics to evaluate map learning performance, including both\n",
            "semantic-level and instance-level ones. By introducing this method and metrics,\n",
            "we invite the community to study this novel map learning problem. We will\n",
            "release our code and evaluation kit to facilitate future development.\n",
            "Label(s): [' ', '.', 'A', 'C', 'I', 'V', 'c', 's']\n",
            " \n",
            "Abstract: In this work we consider UAVs as cooperative agents supporting human users in\n",
            "their operations. In this context, the 3D localisation of the UAV assistant is\n",
            "an important task that can facilitate the exchange of spatial information\n",
            "between the user and the UAV. To address this in a data-driven manner, we\n",
            "design a data synthesis pipeline to create a realistic multimodal dataset that\n",
            "includes both the exocentric user view, and the egocentric UAV view. We then\n",
            "exploit the joint availability of photorealistic and synthesized inputs to\n",
            "train a single-shot monocular pose estimation model. During training we\n",
            "leverage differentiable rendering to supplement a state-of-the-art direct\n",
            "regression objective with a novel smooth silhouette loss. Our results\n",
            "demonstrate its qualitative and quantitative performance gains over traditional\n",
            "silhouette objectives. Our data and code are available at\n",
            "https://vcl3d.github.io/DronePose\n",
            "Label(s): [' ', '.', 'C', 'G', 'I', 'O', 'R', 'V', 'c', 'e', 's']\n",
            " \n",
            "Abstract: Due to the statistical complexity of video, the high degree of inherent\n",
            "stochasticity, and the sheer amount of data, generating natural video remains a\n",
            "challenging task. State-of-the-art video generation models often attempt to\n",
            "address these issues by combining sometimes complex, usually video-specific\n",
            "neural network architectures, latent variable models, adversarial training and\n",
            "a range of other methods. Despite their often high complexity, these approaches\n",
            "still fall short of generating high quality video continuations outside of\n",
            "narrow domains and often struggle with fidelity. In contrast, we show that\n",
            "conceptually simple autoregressive video generation models based on a\n",
            "three-dimensional self-attention mechanism achieve competitive results across\n",
            "multiple metrics on popular benchmark datasets, for which they produce\n",
            "continuations of high fidelity and realism. We also present results from\n",
            "training our models on Kinetics, a large scale action recognition dataset\n",
            "comprised of YouTube videos exhibiting phenomena such as camera movement,\n",
            "complex object interactions and diverse human movement. While modeling these\n",
            "phenomena consistently remains elusive, we hope that our results, which include\n",
            "occasional realistic continuations encourage further research on comparatively\n",
            "complex, large scale datasets such as Kinetics.\n",
            "Label(s): [' ', '.', 'A', 'C', 'G', 'I', 'L', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Access to electronic health record (EHR) data has motivated computational\n",
            "advances in medical research. However, various concerns, particularly over\n",
            "privacy, can limit access to and collaborative use of EHR data. Sharing\n",
            "synthetic EHR data could mitigate risk. In this paper, we propose a new\n",
            "approach, medical Generative Adversarial Network (medGAN), to generate\n",
            "realistic synthetic patient records. Based on input real patient records,\n",
            "medGAN can generate high-dimensional discrete variables (e.g., binary and count\n",
            "features) via a combination of an autoencoder and generative adversarial\n",
            "networks. We also propose minibatch averaging to efficiently avoid mode\n",
            "collapse, and increase the learning efficiency with batch normalization and\n",
            "shortcut connections. To demonstrate feasibility, we showed that medGAN\n",
            "generates synthetic patient records that achieve comparable performance to real\n",
            "data on many experiments including distribution statistics, predictive modeling\n",
            "tasks and a medical expert review. We also empirically observe a limited\n",
            "privacy risk in both identity and attribute disclosure using medGAN.\n",
            "Label(s): [' ', '.', 'E', 'G', 'L', 'N', 'c', 's']\n",
            " \n",
            "Abstract: This report presents the application of object detection on a database of\n",
            "underwater images of different species of crabs, as well as aerial images of\n",
            "sea lions and finally the Pascal VOC dataset. The model is an end-to-end object\n",
            "detection neural network based on a convolutional network base and a Long\n",
            "Short-Term Memory detector.\n",
            "Label(s): [' ', '.', 'C', 'G', 'L', 'V', 'c', 's']\n",
            " \n",
            "Abstract: To address the problem of data inconsistencies among different facial\n",
            "expression recognition (FER) datasets, many cross-domain FER methods (CD-FERs)\n",
            "have been extensively devised in recent years. Although each declares to\n",
            "achieve superior performance, fair comparisons are lacking due to the\n",
            "inconsistent choices of the source/target datasets and feature extractors. In\n",
            "this work, we first analyze the performance effect caused by these inconsistent\n",
            "choices, and then re-implement some well-performing CD-FER and recently\n",
            "published domain adaptation algorithms. We ensure that all these algorithms\n",
            "adopt the same source datasets and feature extractors for fair CD-FER\n",
            "evaluations. We find that most of the current leading algorithms use\n",
            "adversarial learning to learn holistic domain-invariant features to mitigate\n",
            "domain shifts. However, these algorithms ignore local features, which are more\n",
            "transferable across different datasets and carry more detailed content for\n",
            "fine-grained adaptation. To address these issues, we integrate graph\n",
            "representation propagation with adversarial learning for cross-domain\n",
            "holistic-local feature co-adaptation by developing a novel adversarial graph\n",
            "representation adaptation (AGRA) framework. Specifically, it first builds two\n",
            "graphs to correlate holistic and local regions within each domain and across\n",
            "different domains, respectively. Then, it extracts holistic-local features from\n",
            "the input image and uses learnable per-class statistical distributions to\n",
            "initialize the corresponding graph nodes. Finally, two stacked graph\n",
            "convolution networks (GCNs) are adopted to propagate holistic-local features\n",
            "within each domain to explore their interaction and across different domains\n",
            "for holistic-local feature co-adaptation. We conduct extensive and fair\n",
            "evaluations on several popular benchmarks and show that the proposed AGRA\n",
            "framework outperforms previous state-of-the-art methods.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: We present our latest experiment results of object recognition from 3D point\n",
            "cloud data collected through moving car.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: In multiagent environments, several decision-making individuals interact\n",
            "while adhering to the dynamics constraints imposed by the environment. These\n",
            "interactions, combined with the potential stochasticity of the agents'\n",
            "decision-making processes, make such systems complex and interesting to study\n",
            "from a dynamical perspective. Significant research has been conducted on\n",
            "learning models for forward-direction estimation of agent behaviors, for\n",
            "example, pedestrian predictions used for collision-avoidance in self-driving\n",
            "cars. However, in many settings, only sporadic observations of agents may be\n",
            "available in a given trajectory sequence. For instance, in football, subsets of\n",
            "players may come in and out of view of broadcast video footage, while\n",
            "unobserved players continue to interact off-screen. In this paper, we study the\n",
            "problem of multiagent time-series imputation, where available past and future\n",
            "observations of subsets of agents are used to estimate missing observations for\n",
            "other agents. Our approach, called the Graph Imputer, uses forward- and\n",
            "backward-information in combination with graph networks and variational\n",
            "autoencoders to enable learning of a distribution of imputed trajectories. We\n",
            "evaluate our approach on a dataset of football matches, using a projective\n",
            "camera module to train and evaluate our model for the off-screen player state\n",
            "estimation setting. We illustrate that our method outperforms several\n",
            "state-of-the-art approaches, including those hand-crafted for football.\n",
            "Label(s): [' ', '.', 'A', 'G', 'I', 'L', 'M', 'c', 's']\n",
            " \n",
            "Abstract: Weight sharing, as an approach to speed up architecture performance\n",
            "estimation has received wide attention. Instead of training each architecture\n",
            "separately, weight sharing builds a supernet that assembles all the\n",
            "architectures as its submodels. However, there has been debate over whether the\n",
            "NAS process actually benefits from weight sharing, due to the gap between\n",
            "supernet optimization and the objective of NAS. To further understand the\n",
            "effect of weight sharing on NAS, we conduct a comprehensive analysis on five\n",
            "search spaces, including NAS-Bench-101, NAS-Bench-201, DARTS-CIFAR10,\n",
            "DARTS-PTB, and ProxylessNAS. We find that weight sharing works well on some\n",
            "search spaces but fails on others. Taking a step forward, we further identified\n",
            "biases accounting for such phenomenon and the capacity of weight sharing. Our\n",
            "work is expected to inspire future NAS researchers to better leverage the power\n",
            "of weight sharing.\n",
            "Label(s): [' ', '.', 'C', 'E', 'G', 'L', 'N', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Modern machine learning workloads use large models, with complex structures,\n",
            "that are very expensive to execute. The devices that execute complex models are\n",
            "becoming increasingly heterogeneous as we see a flourishing of domain-specific\n",
            "accelerators being offered as hardware accelerators in addition to CPUs. These\n",
            "trends necessitate distributing the workload across multiple devices. Recent\n",
            "work has shown that significant gains can be obtained with model parallelism,\n",
            "i.e, partitioning a neural network's computational graph onto multiple devices.\n",
            "In particular, this form of parallelism assumes a pipeline of devices, which is\n",
            "fed a stream of samples and yields high throughput for training and inference\n",
            "of DNNs. However, for such settings (large models and multiple heterogeneous\n",
            "devices), we require automated algorithms and toolchains that can partition the\n",
            "ML workload across devices. In this paper, we identify and isolate the\n",
            "structured optimization problem at the core of device placement of DNN\n",
            "operators, for both inference and training, especially in modern pipelined\n",
            "settings. We then provide algorithms that solve this problem to optimality. We\n",
            "demonstrate the applicability and efficiency of our approaches using several\n",
            "contemporary DNN computation graphs.\n",
            "Label(s): [' ', '.', 'C', 'D', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Robot control problems are often structured with a policy function that maps\n",
            "state values into control values, but in many dynamic problems the observed\n",
            "state can have a difficult to characterize relationship with useful policy\n",
            "actions. In this paper we present a new method for learning state embeddings\n",
            "from plans or other forms of demonstrations such that the embedding space has a\n",
            "specified geometric relationship with the demonstrations. We present a novel\n",
            "variational framework for learning these embeddings that attempts to optimize\n",
            "trajectory linearity in the learned embedding space. We show how these\n",
            "embedding spaces can then be used as an augmentation to the robot state in\n",
            "reinforcement learning problems. We use kinodynamic planning to generate\n",
            "training trajectories for some example environments, and then train embedding\n",
            "spaces for these environments. We show empirically that observing a system in\n",
            "the learned embedding space improves the performance of policy gradient\n",
            "reinforcement learning algorithms, particularly by reducing the variance\n",
            "between training runs. Our technique is limited to environments where\n",
            "demonstration data is available, but places no limits on how that data is\n",
            "collected. Our embedding technique provides a way to transfer domain knowledge\n",
            "from existing technologies such as planning and control algorithms, into more\n",
            "flexible policy learning algorithms, by creating an abstract representation of\n",
            "the robot state with meaningful geometry.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'O', 'R', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Image super-resolution (SR) research has witnessed impressive progress thanks\n",
            "to the advance of convolutional neural networks (CNNs) in recent years.\n",
            "However, most existing SR methods are non-blind and assume that degradation has\n",
            "a single fixed and known distribution (e.g., bicubic) which struggle while\n",
            "handling degradation in real-world data that usually follows a multi-modal,\n",
            "spatially variant, and unknown distribution. The recent blind SR studies\n",
            "address this issue via degradation estimation, but they do not generalize well\n",
            "to multi-source degradation and cannot handle spatially variant degradation. We\n",
            "design CRL-SR, a contrastive representation learning network that focuses on\n",
            "blind SR of images with multi-modal and spatially variant distributions. CRL-SR\n",
            "addresses the blind SR challenges from two perspectives. The first is\n",
            "contrastive decoupling encoding which introduces contrastive learning to\n",
            "extract resolution-invariant embedding and discard resolution-variant embedding\n",
            "under the guidance of a bidirectional contrastive loss. The second is\n",
            "contrastive feature refinement which generates lost or corrupted high-frequency\n",
            "details under the guidance of a conditional contrastive loss. Extensive\n",
            "experiments on synthetic datasets and real images show that the proposed CRL-SR\n",
            "can handle multi-modal and spatially variant degradation effectively under\n",
            "blind settings and it also outperforms state-of-the-art SR methods\n",
            "qualitatively and quantitatively.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Graph convolutional networks (GCNs) have gained popularity due to high\n",
            "performance achievable on several downstream tasks including node\n",
            "classification. Several architectural variants of these networks have been\n",
            "proposed and investigated with experimental studies in the literature.\n",
            "Motivated by a recent work on simplifying GCNs, we study the problem of\n",
            "designing other variants and propose a framework to compose networks using\n",
            "building blocks of GCN. The framework offers flexibility to compose and\n",
            "evaluate different networks using feature and/or label propagation networks,\n",
            "linear or non-linear networks, with each composition having different\n",
            "computational complexity. We conduct a detailed experimental study on several\n",
            "benchmark datasets with many variants and present observations from our\n",
            "evaluation. Our empirical experimental results suggest that several newly\n",
            "composed variants are useful alternatives to consider because they are as\n",
            "competitive as, or better than the original GCN.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: In recent decades, 3D morphable model (3DMM) has been commonly used in\n",
            "image-based photorealistic 3D face reconstruction. However, face images are\n",
            "often corrupted by serious occlusion by non-face objects including eyeglasses,\n",
            "masks, and hands. Such objects block the correct capture of landmarks and\n",
            "shading information. Therefore, the reconstructed 3D face model is hardly\n",
            "reusable. In this paper, a novel method is proposed to restore de-occluded face\n",
            "images based on inverse use of 3DMM and generative adversarial network. We\n",
            "utilize the 3DMM prior to the proposed adversarial network and combine a global\n",
            "and local adversarial convolutional neural network to learn face de-occlusion\n",
            "model. The 3DMM serves not only as geometric prior but also proposes the face\n",
            "region for the local discriminator. Experiment results confirm the\n",
            "effectiveness and robustness of the proposed algorithm in removing challenging\n",
            "types of occlusions with various head poses and illumination. Furthermore, the\n",
            "proposed method reconstructs the correct 3D face model with de-occluded\n",
            "textures.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: To alleviate the cost of collecting and annotating large-scale point cloud\n",
            "datasets, we propose an unsupervised learning approach to learn features from\n",
            "unlabeled point cloud \"3D object\" dataset by using part contrasting and object\n",
            "clustering with deep graph neural networks (GNNs). In the contrast learning\n",
            "step, all the samples in the 3D object dataset are cut into two parts and put\n",
            "into a \"part\" dataset. Then a contrast learning GNN (ContrastNet) is trained to\n",
            "verify whether two randomly sampled parts from the part dataset belong to the\n",
            "same object. In the cluster learning step, the trained ContrastNet is applied\n",
            "to all the samples in the original 3D object dataset to extract features, which\n",
            "are used to group the samples into clusters. Then another GNN for clustering\n",
            "learning (ClusterNet) is trained to predict the cluster ID of all the training\n",
            "samples. The contrasting learning forces the ContrastNet to learn high-level\n",
            "semantic features of objects but probably ignores low-level features, while the\n",
            "ClusterNet improves the quality of learned features by being trained to\n",
            "discover objects that probably belong to the same semantic categories by the\n",
            "use of cluster IDs. We have conducted extensive experiments to evaluate the\n",
            "proposed framework on point cloud classification tasks. The proposed\n",
            "unsupervised learning approach obtained comparable performance to the\n",
            "state-of-the-art unsupervised learning methods that used much more complicated\n",
            "network structures. The code of this work is publicly available via:\n",
            "https://github.com/lingzhang1/ContrastNet.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: The generalization capability of deep neural networks has been substantially\n",
            "improved by applying a wide spectrum of regularization methods, e.g.,\n",
            "restricting function space, injecting randomness during training, augmenting\n",
            "data, etc. In this work, we propose a simple yet effective regularization\n",
            "method named \\textit{progressive self-knowledge distillation} (PS-KD), which\n",
            "progressively distills a model's own knowledge to soften hard targets (i.e.,\n",
            "one-hot vectors) during training. Hence, it can be interpreted within a\n",
            "framework of knowledge distillation as a student becomes a teacher itself.\n",
            "Specifically, targets are adjusted adaptively by combining the ground-truth and\n",
            "past predictions from the model itself. We show that PS-KD provides an effect\n",
            "of hard example mining by rescaling gradients according to difficulty in\n",
            "classifying examples. The proposed method is applicable to any supervised\n",
            "learning tasks with hard targets and can be easily combined with existing\n",
            "regularization methods to further enhance the generalization performance.\n",
            "Furthermore, it is confirmed that PS-KD achieves not only better accuracy, but\n",
            "also provides high quality of confidence estimates in terms of calibration as\n",
            "well as ordinal ranking. Extensive experimental results on three different\n",
            "tasks, image classification, object detection, and machine translation,\n",
            "demonstrate that our method consistently improves the performance of the\n",
            "state-of-the-art baselines. The code will be released.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Explaining AI systems is fundamental both to the development of high\n",
            "performing models and to the trust placed in them by their users. The Shapley\n",
            "framework for explainability has strength in its general applicability combined\n",
            "with its precise, rigorous foundation: it provides a common, model-agnostic\n",
            "language for AI explainability and uniquely satisfies a set of intuitive\n",
            "mathematical axioms. However, Shapley values are too restrictive in one\n",
            "significant regard: they ignore all causal structure in the data. We introduce\n",
            "a less restrictive framework, Asymmetric Shapley values (ASVs), which are\n",
            "rigorously founded on a set of axioms, applicable to any AI system, and\n",
            "flexible enough to incorporate any causal structure known to be respected by\n",
            "the data. We demonstrate that ASVs can (i) improve model explanations by\n",
            "incorporating causal information, (ii) provide an unambiguous test for unfair\n",
            "discrimination in model predictions, (iii) enable sequentially incremental\n",
            "explanations in time-series models, and (iv) support feature-selection studies\n",
            "without the need for model retraining.\n",
            "Label(s): [' ', '.', 'A', 'G', 'I', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: It is now possible to synthesize highly realistic images of people who don't\n",
            "exist. Such content has, for example, been implicated in the creation of\n",
            "fraudulent social-media profiles responsible for dis-information campaigns.\n",
            "Significant efforts are, therefore, being deployed to detect\n",
            "synthetically-generated content. One popular forensic approach trains a neural\n",
            "network to distinguish real from synthetic content.\n",
            "  We show that such forensic classifiers are vulnerable to a range of attacks\n",
            "that reduce the classifier to near-0% accuracy. We develop five attack case\n",
            "studies on a state-of-the-art classifier that achieves an area under the ROC\n",
            "curve (AUC) of 0.95 on almost all existing image generators, when only trained\n",
            "on one generator. With full access to the classifier, we can flip the lowest\n",
            "bit of each pixel in an image to reduce the classifier's AUC to 0.0005; perturb\n",
            "1% of the image area to reduce the classifier's AUC to 0.08; or add a single\n",
            "noise pattern in the synthesizer's latent space to reduce the classifier's AUC\n",
            "to 0.17. We also develop a black-box attack that, with no access to the target\n",
            "classifier, reduces the AUC to 0.22. These attacks reveal significant\n",
            "vulnerabilities of certain image-forensic classifiers.\n",
            "Label(s): [' ', '.', 'C', 'R', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Finding a suitable data representation for a specific task has been shown to\n",
            "be crucial in many applications. The success of subspace clustering depends on\n",
            "the assumption that the data can be separated into different subspaces.\n",
            "However, this simple assumption does not always hold since the raw data might\n",
            "not be separable into subspaces. To recover the ``clustering-friendly''\n",
            "representation and facilitate the subsequent clustering, we propose a graph\n",
            "filtering approach by which a smooth representation is achieved. Specifically,\n",
            "it injects graph similarity into data features by applying a low-pass filter to\n",
            "extract useful data representations for clustering. Extensive experiments on\n",
            "image and document clustering datasets demonstrate that our method improves\n",
            "upon state-of-the-art subspace clustering techniques. Especially, its\n",
            "comparable performance with deep learning methods emphasizes the effectiveness\n",
            "of the simple graph filtering scheme for many real-world applications. An\n",
            "ablation study shows that graph filtering can remove noise, preserve structure\n",
            "in the image, and increase the separability of classes.\n",
            "Label(s): [' ', '.', 'A', 'C', 'G', 'I', 'L', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Deep CNN-based methods have so far achieved the state of the art results in\n",
            "multi-view 3D object reconstruction. Despite the considerable progress, the two\n",
            "core modules of these methods - multi-view feature extraction and fusion, are\n",
            "usually investigated separately, and the object relations in different views\n",
            "are rarely explored. In this paper, inspired by the recent great success in\n",
            "self-attention-based Transformer models, we reformulate the multi-view 3D\n",
            "reconstruction as a sequence-to-sequence prediction problem and propose a new\n",
            "framework named 3D Volume Transformer (VolT) for such a task. Unlike previous\n",
            "CNN-based methods using a separate design, we unify the feature extraction and\n",
            "view fusion in a single Transformer network. A natural advantage of our design\n",
            "lies in the exploration of view-to-view relationships using self-attention\n",
            "among multiple unordered inputs. On ShapeNet - a large-scale 3D reconstruction\n",
            "benchmark dataset, our method achieves a new state-of-the-art accuracy in\n",
            "multi-view reconstruction with fewer parameters ($70\\%$ less) than other\n",
            "CNN-based methods. Experimental results also suggest the strong scaling\n",
            "capability of our method. Our code will be made publicly available.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Learning based approaches have not yet achieved their full potential in\n",
            "optical flow estimation, where their performance still trails heuristic\n",
            "approaches. In this paper, we present a CNN based patch matching approach for\n",
            "optical flow estimation. An important contribution of our approach is a novel\n",
            "thresholded loss for Siamese networks. We demonstrate that our loss performs\n",
            "clearly better than existing losses. It also allows to speed up training by a\n",
            "factor of 2 in our tests. Furthermore, we present a novel way for calculating\n",
            "CNN based features for different image scales, which performs better than\n",
            "existing methods. We also discuss new ways of evaluating the robustness of\n",
            "trained features for the application of patch matching for optical flow. An\n",
            "interesting discovery in our paper is that low-pass filtering of feature maps\n",
            "can increase the robustness of features created by CNNs. We proved the\n",
            "competitive performance of our approach by submitting it to the KITTI 2012,\n",
            "KITTI 2015 and MPI-Sintel evaluation portals where we obtained state-of-the-art\n",
            "results on all three datasets.\n",
            "Label(s): [' ', '.', 'C', 'E', 'G', 'L', 'N', 'V', 'c', 's']\n",
            " \n",
            "Abstract: We introduce Point2Skeleton, an unsupervised method to learn skeletal\n",
            "representations from point clouds. Existing skeletonization methods are limited\n",
            "to tubular shapes and the stringent requirement of watertight input, while our\n",
            "method aims to produce more generalized skeletal representations for complex\n",
            "structures and handle point clouds. Our key idea is to use the insights of the\n",
            "medial axis transform (MAT) to capture the intrinsic geometric and topological\n",
            "natures of the original input points. We first predict a set of skeletal points\n",
            "by learning a geometric transformation, and then analyze the connectivity of\n",
            "the skeletal points to form skeletal mesh structures. Extensive evaluations and\n",
            "comparisons show our method has superior performance and robustness. The\n",
            "learned skeletal representation will benefit several unsupervised tasks for\n",
            "point clouds, such as surface reconstruction and segmentation.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Partial person re-identification (ReID) is a challenging task because only\n",
            "partial information of person images is available for matching target persons.\n",
            "Few studies, especially on deep learning, have focused on matching partial\n",
            "person images with holistic person images. This study presents a novel deep\n",
            "partial ReID framework based on pairwise spatial transformer networks\n",
            "(STNReID), which can be trained on existing holistic person datasets. STNReID\n",
            "includes a spatial transformer network (STN) module and a ReID module. The STN\n",
            "module samples an affined image (a semantically corresponding patch) from the\n",
            "holistic image to match the partial image. The ReID module extracts the\n",
            "features of the holistic, partial, and affined images. Competition (or\n",
            "confrontation) is observed between the STN module and the ReID module, and\n",
            "two-stage training is applied to acquire a strong STNReID for partial ReID.\n",
            "Experimental results show that our STNReID obtains 66.7% and 54.6% rank-1\n",
            "accuracies on partial ReID and partial iLIDS datasets, respectively. These\n",
            "values are at par with those obtained with state-of-the-art methods.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Recognition of Handwritten Mathematical Expressions (HMEs) is a challenging\n",
            "problem because of the ambiguity and complexity of two-dimensional handwriting.\n",
            "Moreover, the lack of large training data is a serious issue, especially for\n",
            "academic recognition systems. In this paper, we propose pattern generation\n",
            "strategies that generate shape and structural variations to improve the\n",
            "performance of recognition systems based on a small training set. For data\n",
            "generation, we employ the public databases: CROHME 2014 and 2016 of online\n",
            "HMEs. The first strategy employs local and global distortions to generate shape\n",
            "variations. The second strategy decomposes an online HME into sub-online HMEs\n",
            "to get more structural variations. The hybrid strategy combines both these\n",
            "strategies to maximize shape and structural variations. The generated online\n",
            "HMEs are converted to images for offline HME recognition. We tested our\n",
            "strategies in an end-to-end recognition system constructed from a recent deep\n",
            "learning model: Convolutional Neural Network and attention-based\n",
            "encoder-decoder. The results of experiments on the CROHME 2014 and 2016\n",
            "databases demonstrate the superiority and effectiveness of our strategies: our\n",
            "hybrid strategy achieved classification rates of 48.78% and 45.60%,\n",
            "respectively, on these databases. These results are competitive compared to\n",
            "others reported in recent literature. Our generated datasets are openly\n",
            "available for research community and constitute a useful resource for the HME\n",
            "recognition research in future.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Recently, pre-trained language representation flourishes as the mainstay of\n",
            "the natural language understanding community, e.g., BERT. These pre-trained\n",
            "language representations can create state-of-the-art results on a wide range of\n",
            "downstream tasks. Along with continuous significant performance improvement,\n",
            "the size and complexity of these pre-trained neural models continue to increase\n",
            "rapidly. Is it possible to compress these large-scale language representation\n",
            "models? How will the pruned language representation affect the downstream\n",
            "multi-task transfer learning objectives? In this paper, we propose Reweighted\n",
            "Proximal Pruning (RPP), a new pruning method specifically designed for a\n",
            "large-scale language representation model. Through experiments on SQuAD and the\n",
            "GLUE benchmark suite, we show that proximal pruned BERT keeps high accuracy for\n",
            "both the pre-training task and the downstream multiple fine-tuning tasks at\n",
            "high prune ratio. RPP provides a new perspective to help us analyze what\n",
            "large-scale language representation might learn. Additionally, RPP makes it\n",
            "possible to deploy a large state-of-the-art language representation model such\n",
            "as BERT on a series of distinct devices (e.g., online servers, mobile phones,\n",
            "and edge devices).\n",
            "Label(s): [' ', '.', 'C', 'E', 'G', 'L', 'M', 'N', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Echocardiography is a powerful prenatal examination tool for early diagnosis\n",
            "of fetal congenital heart diseases (CHDs). The four-chamber (FC) view is a\n",
            "crucial and easily accessible ultrasound (US) image among echocardiography\n",
            "images. Automatic analysis of FC views contributes significantly to the early\n",
            "diagnosis of CHDs. The first step to automatically analyze fetal FC views is\n",
            "locating the fetal four crucial chambers of heart in a US image. However, it is\n",
            "a greatly challenging task due to several key factors, such as numerous\n",
            "speckles in US images, the fetal cardiac chambers with small size and unfixed\n",
            "positions, and category indistinction caused by the similarity of cardiac\n",
            "chambers. These factors hinder the process of capturing robust and\n",
            "discriminative features, hence destroying fetal cardiac anatomical chambers\n",
            "precise localization. Therefore, we first propose a multistage residual hybrid\n",
            "attention module (MRHAM) to improve the feature learning. Then, we present an\n",
            "improved YOLOv4 detection model, namely MRHAM-YOLOv4-Slim. Specially, the\n",
            "residual identity mapping is replaced with the MRHAM in the backbone of\n",
            "MRHAM-YOLOv4-Slim, accurately locating the four important chambers in fetal FC\n",
            "views. Extensive experiments demonstrate that our proposed method outperforms\n",
            "current state-of-the-art, including the precision of 0.919, the recall of\n",
            "0.971, the F1 score of 0.944, the mAP of 0.953, and the frames per second (FPS)\n",
            "of 43.\n",
            "Label(s): [' ', '.', 'C', 'I', 'V', 'c', 'e', 's']\n",
            " \n",
            "Abstract: While most steps in the modern object detection methods are learnable, the\n",
            "region feature extraction step remains largely hand-crafted, featured by RoI\n",
            "pooling methods. This work proposes a general viewpoint that unifies existing\n",
            "region feature extraction methods and a novel method that is end-to-end\n",
            "learnable. The proposed method removes most heuristic choices and outperforms\n",
            "its RoI pooling counterparts. It moves further towards fully learnable object\n",
            "detection.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Input transformation based defense strategies fall short in defending against\n",
            "strong adversarial attacks. Some successful defenses adopt approaches that\n",
            "either increase the randomness within the applied transformations, or make the\n",
            "defense computationally intensive, making it substantially more challenging for\n",
            "the attacker. However, it limits the applicability of such defenses as a\n",
            "pre-processing step, similar to computationally heavy approaches that use\n",
            "retraining and network modifications to achieve robustness to perturbations. In\n",
            "this work, we propose a defense strategy that applies random image corruptions\n",
            "to the input image alone, constructs a self-correlation based subspace followed\n",
            "by a projection operation to suppress the adversarial perturbation. Due to its\n",
            "simplicity, the proposed defense is computationally efficient as compared to\n",
            "the state-of-the-art, and yet can withstand huge perturbations. Further, we\n",
            "develop proximity relationships between the projection operator of a clean\n",
            "image and of its adversarially perturbed version, via bounds relating geodesic\n",
            "distance on the Grassmannian to matrix Frobenius norms. We empirically show\n",
            "that our strategy is complementary to other weak defenses like JPEG compression\n",
            "and can be seamlessly integrated with them to create a stronger defense. We\n",
            "present extensive experiments on the ImageNet dataset across four different\n",
            "models namely InceptionV3, ResNet50, VGG16 and MobileNet models with\n",
            "perturbation magnitude set to {\\epsilon} = 16. Unlike state-of-the-art\n",
            "approaches, even without any retraining, the proposed strategy achieves an\n",
            "absolute improvement of ~ 4.5% in defense accuracy on ImageNet.\n",
            "Label(s): [' ', '.', 'C', 'G', 'L', 'R', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Unsupervised domain mapping aims to learn a function to translate domain X to\n",
            "Y by a function GXY in the absence of paired examples. Finding the optimal GXY\n",
            "without paired data is an ill-posed problem, so appropriate constraints are\n",
            "required to obtain reasonable solutions. One of the most prominent constraints\n",
            "is cycle consistency, which enforces the translated image by GXY to be\n",
            "translated back to the input image by an inverse mapping GYX. While cycle\n",
            "consistency requires the simultaneous training of GXY and GY X, recent studies\n",
            "have shown that one-sided domain mapping can be achieved by preserving pairwise\n",
            "distances between images. Although cycle consistency and distance preservation\n",
            "successfully constrain the solution space, they overlook the special properties\n",
            "that simple geometric transformations do not change the semantic structure of\n",
            "images. Based on this special property, we develop a geometry-consistent\n",
            "generative adversarial network (GcGAN), which enables one-sided unsupervised\n",
            "domain mapping. GcGAN takes the original image and its counterpart image\n",
            "transformed by a predefined geometric transformation as inputs and generates\n",
            "two images in the new domain coupled with the corresponding\n",
            "geometry-consistency constraint. The geometry-consistency constraint reduces\n",
            "the space of possible solutions while keep the correct solutions in the search\n",
            "space. Quantitative and qualitative comparisons with the baseline (GAN alone)\n",
            "and the state-of-the-art methods including CycleGAN and DistanceGAN demonstrate\n",
            "the effectiveness of our method.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: The enactive approach to cognition is typically proposed as a viable\n",
            "alternative to traditional cognitive science. Enactive cognition displaces the\n",
            "explanatory focus from the internal representations of the agent to the direct\n",
            "sensorimotor interaction with its environment. In this paper, we investigate\n",
            "enactive learning through means of artificial agent simulations. We compare the\n",
            "performances of the enactive agent to an agent operating on classical\n",
            "reinforcement learning in foraging tasks within maze environments. The\n",
            "characteristics of the agents are analysed in terms of the accessibility of the\n",
            "environmental states, goals, and exploration/exploitation tradeoffs. We confirm\n",
            "that the enactive agent can successfully interact with its environment and\n",
            "learn to avoid unfavourable interactions using intrinsically defined goals. The\n",
            "performance of the enactive agent is shown to be limited by the number of\n",
            "affordable actions.\n",
            "Label(s): [' ', '.', 'A', 'G', 'I', 'L', 'M', 'O', 'R', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Modern lane detection methods have achieved remarkable performances in\n",
            "complex real-world scenarios, but many have issues maintaining real-time\n",
            "efficiency, which is important for autonomous vehicles. In this work, we\n",
            "propose LaneATT: an anchor-based deep lane detection model, which, akin to\n",
            "other generic deep object detectors, uses the anchors for the feature pooling\n",
            "step. Since lanes follow a regular pattern and are highly correlated, we\n",
            "hypothesize that in some cases global information may be crucial to infer their\n",
            "positions, especially in conditions such as occlusion, missing lane markers,\n",
            "and others. Thus, this work proposes a novel anchor-based attention mechanism\n",
            "that aggregates global information. The model was evaluated extensively on\n",
            "three of the most widely used datasets in the literature. The results show that\n",
            "our method outperforms the current state-of-the-art methods showing both higher\n",
            "efficacy and efficiency. Moreover, an ablation study is performed along with a\n",
            "discussion on efficiency trade-off options that are useful in practice.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Incompleteness is a common problem for existing knowledge graphs (KGs), and\n",
            "the completion of KG which aims to predict links between entities is\n",
            "challenging. Most existing KG completion methods only consider the direct\n",
            "relation between nodes and ignore the relation paths which contain useful\n",
            "information for link prediction. Recently, a few methods take relation paths\n",
            "into consideration but pay less attention to the order of relations in paths\n",
            "which is important for reasoning. In addition, these path-based models always\n",
            "ignore nonlinear contributions of path features for link prediction. To solve\n",
            "these problems, we propose a novel KG completion method named OPTransE. Instead\n",
            "of embedding both entities of a relation into the same latent space as in\n",
            "previous methods, we project the head entity and the tail entity of each\n",
            "relation into different spaces to guarantee the order of relations in the path.\n",
            "Meanwhile, we adopt a pooling strategy to extract nonlinear and complex\n",
            "features of different paths to further improve the performance of link\n",
            "prediction. Experimental results on two benchmark datasets show that the\n",
            "proposed model OPTransE performs better than state-of-the-art methods.\n",
            "Label(s): [' ', '.', 'A', 'G', 'I', 'L', 'c', 's']\n",
            " \n",
            "Abstract: Over the past years, computer vision community has contributed to enormous\n",
            "progress in semantic image segmentation, a per-pixel classification task,\n",
            "crucial for dense scene understanding and rapidly becoming vital in lots of\n",
            "real-world applications, including driverless cars and medical imaging. Most\n",
            "recent models are now reaching previously unthinkable numbers (e.g., 89% mean\n",
            "iou on PASCAL VOC, 83% on CityScapes), and, while intersection-over-union and a\n",
            "range of other metrics provide the general picture of model performance, in\n",
            "this paper we aim to extend them into other meaningful and important for\n",
            "applications characteristics, answering such questions as 'how accurate the\n",
            "model segmentation is on small objects in the general scene?', or 'what are the\n",
            "sources of uncertainty that cause the model to make an erroneous prediction?'.\n",
            "Besides establishing a methodology that covers the performance of a single\n",
            "model from different perspectives, we also showcase several extensions that can\n",
            "be worth pursuing in order to further improve current results in semantic\n",
            "segmentation.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Event cameras, which are asynchronous bio-inspired vision sensors, have shown\n",
            "great potential in computer vision and artificial intelligence. However, the\n",
            "application of event cameras to object-level motion estimation or tracking is\n",
            "still in its infancy. The main idea behind this work is to propose a novel deep\n",
            "neural network to learn and regress a parametric object-level motion/transform\n",
            "model for event-based object tracking. To achieve this goal, we propose a\n",
            "synchronous Time-Surface with Linear Time Decay (TSLTD) representation, which\n",
            "effectively encodes the spatio-temporal information of asynchronous retinal\n",
            "events into TSLTD frames with clear motion patterns. We feed the sequence of\n",
            "TSLTD frames to a novel Retinal Motion Regression Network (RMRNet) to perform\n",
            "an end-to-end 5-DoF object motion regression. Our method is compared with\n",
            "state-of-the-art object tracking methods, that are based on conventional\n",
            "cameras or event cameras. The experimental results show the superiority of our\n",
            "method in handling various challenging environments such as fast motion and low\n",
            "illumination conditions.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Convolutional Neural Networks (CNNs) have performed extremely well on data\n",
            "represented by regularly arranged grids such as images. However, directly\n",
            "leveraging the classic convolution kernels or parameter sharing mechanisms on\n",
            "sparse 3D point clouds is inefficient due to their irregular and unordered\n",
            "nature. We propose a point attention network that learns rich local shape\n",
            "features and their contextual correlations for 3D point cloud semantic\n",
            "segmentation. Since the geometric distribution of the neighboring points is\n",
            "invariant to the point ordering, we propose a Local Attention-Edge Convolution\n",
            "(LAE Conv) to construct a local graph based on the neighborhood points searched\n",
            "in multi-directions. We assign attention coefficients to each edge and then\n",
            "aggregate the point features as a weighted sum of its neighbors. The learned\n",
            "LAE-Conv layer features are then given to a point-wise spatial attention module\n",
            "to generate an interdependency matrix of all points regardless of their\n",
            "distances, which captures long-range spatial contextual features contributing\n",
            "to more precise semantic information. The proposed point attention network\n",
            "consists of an encoder and decoder which, together with the LAE-Conv layers and\n",
            "the point-wise spatial attention modules, make it an end-to-end trainable\n",
            "network for predicting dense labels for 3D point cloud segmentation.\n",
            "Experiments on challenging benchmarks of 3D point clouds show that our\n",
            "algorithm can perform at par or better than the existing state of the art\n",
            "methods.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Contrastive Learning (CL) is a recent representation learning approach, which\n",
            "encourages inter-class separability and intra-class compactness in learned\n",
            "image representations. Since medical images often contain multiple semantic\n",
            "classes in an image, using CL to learn representations of local features (as\n",
            "opposed to global) is important. In this work, we present a novel\n",
            "semi-supervised 2D medical segmentation solution that applies CL on image\n",
            "patches, instead of full images. These patches are meaningfully constructed\n",
            "using the semantic information of different classes obtained via pseudo\n",
            "labeling. We also propose a novel consistency regularization (CR) scheme, which\n",
            "works in synergy with CL. It addresses the problem of confirmation bias, and\n",
            "encourages better clustering in the feature space. We evaluate our method on\n",
            "four public medical segmentation datasets and a novel histopathology dataset\n",
            "that we introduce. Our method obtains consistent improvements over\n",
            "state-of-the-art semi-supervised segmentation approaches for all datasets.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Paper-intensive industries like insurance, law, and government have long\n",
            "leveraged optical character recognition (OCR) to automatically transcribe\n",
            "hordes of scanned documents into text strings for downstream processing. Even\n",
            "in 2019, there are still many scanned documents and mail that come into\n",
            "businesses in non-digital format. Text to be extracted from real world\n",
            "documents is often nestled inside rich formatting, such as tabular structures\n",
            "or forms with fill-in-the-blank boxes or underlines whose ink often touches or\n",
            "even strikes through the ink of the text itself. Further, the text region could\n",
            "have random ink smudges or spurious strokes. Such ink artifacts can severely\n",
            "interfere with the performance of recognition algorithms or other downstream\n",
            "processing tasks. In this work, we propose DeepErase, a neural-based\n",
            "preprocessor to erase ink artifacts from text images. We devise a method to\n",
            "programmatically assemble real text images and real artifacts into\n",
            "realistic-looking \"dirty\" text images, and use them to train an artifact\n",
            "segmentation network in a weakly supervised manner, since pixel-level\n",
            "annotations are automatically obtained during the assembly process. In addition\n",
            "to high segmentation accuracy, we show that our cleansed images achieve a\n",
            "significant boost in recognition accuracy by popular OCR software such as\n",
            "Tesseract 4.0. Finally, we test DeepErase on out-of-distribution datasets (NIST\n",
            "SDB) of scanned IRS tax return forms and achieve double-digit improvements in\n",
            "accuracy. All experiments are performed on both printed and handwritten text.\n",
            "Code for all experiments is available at https://github.com/yikeqicn/DeepErase\n",
            "Label(s): [' ', '.', 'C', 'E', 'G', 'L', 'N', 'V', 'c', 's']\n",
            " \n",
            "Abstract: We present a cross-modality generation framework that learns to generate\n",
            "translated modalities from given modalities in MR images without real\n",
            "acquisition. Our proposed method performs NeuroImage-to-NeuroImage translation\n",
            "(abbreviated as N2N) by means of a deep learning model that leverages\n",
            "conditional generative adversarial networks (cGANs). Our framework jointly\n",
            "exploits the low-level features (pixel-wise information) and high-level\n",
            "representations (e.g. brain tumors, brain structure like gray matter, etc.)\n",
            "between cross modalities which are important for resolving the challenging\n",
            "complexity in brain structures. Our framework can serve as an auxiliary method\n",
            "in clinical diagnosis and has great application potential. Based on our\n",
            "proposed framework, we first propose a method for cross-modality registration\n",
            "by fusing the deformation fields to adopt the cross-modality information from\n",
            "translated modalities. Second, we propose an approach for MRI segmentation,\n",
            "translated multichannel segmentation (TMS), where given modalities, along with\n",
            "translated modalities, are segmented by fully convolutional networks (FCN) in a\n",
            "multichannel manner. Both of these two methods successfully adopt the\n",
            "cross-modality information to improve the performance without adding any extra\n",
            "data. Experiments demonstrate that our proposed framework advances the\n",
            "state-of-the-art on five brain MRI datasets. We also observe encouraging\n",
            "results in cross-modality registration and segmentation on some widely adopted\n",
            "brain datasets. Overall, our work can serve as an auxiliary method in clinical\n",
            "diagnosis and be applied to various tasks in medical fields.\n",
            "  Keywords: image-to-image, cross-modality, registration, segmentation, brain\n",
            "MRI\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: In this paper, we propose several dictionary learning algorithms for sparse\n",
            "representations that also impose specific structures on the learned\n",
            "dictionaries such that they are numerically efficient to use: reduced number of\n",
            "addition/multiplications and even avoiding multiplications altogether. We base\n",
            "our work on factorizations of the dictionary in highly structured basic\n",
            "building blocks (binary orthonormal, scaling and shear transformations) for\n",
            "which we can write closed-form solutions to the optimization problems that we\n",
            "consider. We show the effectiveness of our methods on image data where we can\n",
            "compare against well-known numerically efficient transforms such as the fast\n",
            "Fourier and the fast discrete cosine transforms.\n",
            "Label(s): [' ', '.', 'A', 'G', 'L', 'M', 'N', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Most existing single image deraining methods require learning supervised\n",
            "models from a large set of paired synthetic training data, which limits their\n",
            "generality, scalability and practicality in real-world multimedia applications.\n",
            "Besides, due to lack of labeled-supervised constraints, directly applying\n",
            "existing unsupervised frameworks to the image deraining task will suffer from\n",
            "low-quality recovery. Therefore, we propose an Unsupervised Deraining\n",
            "Generative Adversarial Network (UD-GAN) to tackle above problems by introducing\n",
            "self-supervised constraints from the intrinsic statistics of unpaired rainy and\n",
            "clean images. Specifically, we firstly design two collaboratively optimized\n",
            "modules, namely Rain Guidance Module (RGM) and Background Guidance Module\n",
            "(BGM), to take full advantage of rainy image characteristics: The RGM is\n",
            "designed to discriminate real rainy images from fake rainy images which are\n",
            "created based on outputs of the generator with BGM. Simultaneously, the BGM\n",
            "exploits a hierarchical Gaussian-Blur gradient error to ensure background\n",
            "consistency between rainy input and de-rained output. Secondly, a novel\n",
            "luminance-adjusting adversarial loss is integrated into the clean image\n",
            "discriminator considering the built-in luminance difference between real clean\n",
            "images and derained images. Comprehensive experiment results on various\n",
            "benchmarking datasets and different training settings show that UD-GAN\n",
            "outperforms existing image deraining methods in both quantitative and\n",
            "qualitative comparisons.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: In this work we consider the application of convolutional neural networks\n",
            "(CNNs) for pixel-wise labeling (a.k.a., semantic segmentation) of remote\n",
            "sensing imagery (e.g., aerial color or hyperspectral imagery). Remote sensing\n",
            "imagery is usually stored in the form of very large images, referred to as\n",
            "\"tiles\", which are too large to be segmented directly using most CNNs and their\n",
            "associated hardware. As a result, during label inference, smaller sub-images,\n",
            "called \"patches\", are processed individually and then \"stitched\" (concatenated)\n",
            "back together to create a tile-sized label map. This approach suffers from\n",
            "computational ineffiency and can result in discontinuities at output\n",
            "boundaries. We propose a simple alternative approach in which the input size of\n",
            "the CNN is dramatically increased only during label inference. This does not\n",
            "avoid stitching altogether, but substantially mitigates its limitations. We\n",
            "evaluate the performance of the proposed approach against a vonventional\n",
            "stitching approach using two popular segmentation CNN models and two\n",
            "large-scale remote sensing imagery datasets. The results suggest that the\n",
            "proposed approach substantially reduces label inference time, while also\n",
            "yielding modest overall label accuracy increases. This approach contributed to\n",
            "our wining entry (overall performance) in the INRIA building labeling\n",
            "competition.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Access to high-quality education at scale is limited by the difficulty of\n",
            "providing student feedback on open-ended assignments in structured domains like\n",
            "computer programming, graphics, and short response questions. This problem has\n",
            "proven to be exceptionally difficult: for humans, it requires large amounts of\n",
            "manual work, and for computers, until recently, achieving anything near\n",
            "human-level accuracy has been unattainable. In this paper, we present\n",
            "generative grading: a novel computational approach for providing feedback at\n",
            "scale that is capable of accurately grading student work and providing nuanced,\n",
            "interpretable feedback. Our approach uses generative descriptions of student\n",
            "cognition, written as probabilistic programs, to synthesise millions of\n",
            "labelled example solutions to a problem; we then learn to infer feedback for\n",
            "real student solutions based on this cognitive model.\n",
            "  We apply our methods to three settings. In block-based coding, we achieve a\n",
            "50% improvement upon the previous best results for feedback, achieving\n",
            "super-human accuracy. In two other widely different domains -- graphical tasks\n",
            "and short text answers -- we achieve major improvement over the previous state\n",
            "of the art by about 4x and 1.5x respectively, approaching human accuracy. In a\n",
            "real classroom, we ran an experiment where we used our system to augment human\n",
            "graders, yielding doubled grading accuracy while halving grading time.\n",
            "Label(s): [' ', '.', 'C', 'G', 'L', 'M', 'Y', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Several AutoML approaches have been proposed to automate the machine learning\n",
            "(ML) process, such as searching for the ML model architectures and\n",
            "hyper-parameters. However, these AutoML pipelines only focus on improving the\n",
            "learning accuracy of benign samples while ignoring the ML model robustness\n",
            "under adversarial attacks. As ML systems are increasingly being used in a\n",
            "variety of mission-critical applications, improving the robustness of ML\n",
            "systems has become of utmost importance. In this paper, we propose the first\n",
            "robust AutoML framework, Robusta--based on reinforcement learning (RL)--to\n",
            "perform feature selection, aiming to select features that lead to both accurate\n",
            "and robust ML systems. We show that a variation of the 0-1 robust loss can be\n",
            "directly optimized via an RL-based combinatorial search in the feature\n",
            "selection scenario. In addition, we employ heuristics to accelerate the search\n",
            "procedure based on feature scoring metrics, which are mutual information\n",
            "scores, tree-based classifiers feature importance scores, F scores, and\n",
            "Integrated Gradient (IG) scores, as well as their combinations. We conduct\n",
            "extensive experiments and show that the proposed framework is able to improve\n",
            "the model robustness by up to 22% while maintaining competitive accuracy on\n",
            "benign samples compared with other feature selection methods.\n",
            "Label(s): [' ', '.', 'A', 'G', 'I', 'L', 'c', 's']\n",
            " \n",
            "Abstract: The goal of many computer vision systems is to transform image pixels into 3D\n",
            "representations. Recent popular models use neural networks to regress directly\n",
            "from pixels to 3D object parameters. Such an approach works well when\n",
            "supervision is available, but in problems like human pose and shape estimation,\n",
            "it is difficult to obtain natural images with 3D ground truth. To go one step\n",
            "further, we propose a new architecture that facilitates unsupervised, or\n",
            "lightly supervised, learning. The idea is to break the problem into a series of\n",
            "transformations between increasingly abstract representations. Each step\n",
            "involves a cycle designed to be learnable without annotated training data, and\n",
            "the chain of cycles delivers the final solution. Specifically, we use 2D body\n",
            "part segments as an intermediate representation that contains enough\n",
            "information to be lifted to 3D, and at the same time is simple enough to be\n",
            "learned in an unsupervised way. We demonstrate the method by learning 3D human\n",
            "pose and shape from un-paired and un-annotated images. We also explore varying\n",
            "amounts of paired data and show that cycling greatly alleviates the need for\n",
            "paired data. While we present results for modeling humans, our formulation is\n",
            "general and can be applied to other vision problems.\n",
            "Label(s): [' ', '.', 'C', 'G', 'L', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Learning classifiers that are robust to adversarial examples has received a\n",
            "great deal of recent attention. A major drawback of the standard robust\n",
            "learning framework is the imposition of an artificial robustness radius $r$\n",
            "that applies to all inputs, and ignores the fact that data may be highly\n",
            "heterogeneous. In this paper, we address this limitation by proposing a new\n",
            "framework for adaptive robustness, called neighborhood preserving robustness.\n",
            "We present sufficient conditions under which general non-parametric methods\n",
            "that can be represented as weight functions satisfy our notion of robustness,\n",
            "and show that both nearest neighbors and kernel classifiers satisfy these\n",
            "conditions in the large sample limit.\n",
            "Label(s): [' ', '.', 'G', 'L', 'c', 's']\n",
            " \n",
            "Abstract: Software developers routinely search for code using general-purpose search\n",
            "engines. However, these search engines cannot find code semantically unless it\n",
            "has an accompanying description. We propose a technique for semantic code\n",
            "search: A Convolutional Neural Network approach to code retrieval (CoNCRA). Our\n",
            "technique aims to find the code snippet that most closely matches the\n",
            "developer's intent, expressed in natural language. We evaluated our approach's\n",
            "efficacy on a dataset composed of questions and code snippets collected from\n",
            "Stack Overflow. Our preliminary results showed that our technique, which\n",
            "prioritizes local interactions (words nearby), improved the state-of-the-art\n",
            "(SOTA) by 5% on average, retrieving the most relevant code snippets in the top\n",
            "3 (three) positions by almost 80% of the time. Therefore, our technique is\n",
            "promising and can improve the efficacy of semantic code retrieval.\n",
            "Label(s): [' ', '.', 'C', 'E', 'G', 'L', 'M', 'S', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: A quantum edge detector for image segmentation in optical environments is\n",
            "presented in this work. A Boolean version of the same detector is presented\n",
            "too. The quantum version of the new edge detector works with computational\n",
            "basis states, exclusively. This way, we can easily avoid the problem of quantum\n",
            "measurement retrieving the result of applying the new detector on the image.\n",
            "Besides, a new criterion and logic based on projections onto vertical axis of\n",
            "Bloch's Sphere exclusively are presented too. This approach will allow us: 1) a\n",
            "simpler development of logic quantum operations, where they will closer to\n",
            "those used in the classical logic operations, 2) building simple and robust\n",
            "classical-to-quantum and quantum-to-classical interfaces. Said so far is\n",
            "extended to quantum algorithms outside image processing too. In a special\n",
            "section on metric and simulations, a new metric based on the comparison between\n",
            "the classical and quantum versions algorithms for edge detection of images is\n",
            "presented. Notable differences between the results of classical and quantum\n",
            "versions of such algorithms (outside and inside of quantum computer,\n",
            "respectively) show the existence of implementation problems involved in the\n",
            "experiment, and that they have not been properly modeled for optical\n",
            "environments. However, although they are different, the quantum results are\n",
            "equally valid. The latter is clearly seen in the computer simulations\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Heterogeneous graphs are pervasive in practical scenarios, where each graph\n",
            "consists of multiple types of nodes and edges. Representation learning on\n",
            "heterogeneous graphs aims to obtain low-dimensional node representations that\n",
            "could preserve both node attributes and relation information. However, most of\n",
            "the existing graph convolution approaches were designed for homogeneous graphs,\n",
            "and therefore cannot handle heterogeneous graphs. Some recent methods designed\n",
            "for heterogeneous graphs are also faced with several issues, including the\n",
            "insufficient utilization of heterogeneous properties, structural information\n",
            "loss, and lack of interpretability. In this paper, we propose HGConv, a novel\n",
            "Heterogeneous Graph Convolution approach, to learn comprehensive node\n",
            "representations on heterogeneous graphs with a hybrid micro/macro level\n",
            "convolutional operation. Different from existing methods, HGConv could perform\n",
            "convolutions on the intrinsic structure of heterogeneous graphs directly at\n",
            "both micro and macro levels: A micro-level convolution to learn the importance\n",
            "of nodes within the same relation, and a macro-level convolution to distinguish\n",
            "the subtle difference across different relations. The hybrid strategy enables\n",
            "HGConv to fully leverage heterogeneous information with proper\n",
            "interpretability. Moreover, a weighted residual connection is designed to\n",
            "aggregate both inherent attributes and neighbor information of the focal node\n",
            "adaptively. Extensive experiments on various tasks demonstrate not only the\n",
            "superiority of HGConv over existing methods, but also the intuitive\n",
            "interpretability of our approach for graph analysis.\n",
            "Label(s): [' ', '.', 'G', 'L', 'c', 's']\n",
            " \n",
            "Abstract: Most recent transformer-based models show impressive performance on vision\n",
            "tasks, even better than Convolution Neural Networks (CNN). In this work, we\n",
            "present a novel, flexible, and effective transformer-based model for\n",
            "high-quality instance segmentation. The proposed method, Segmenting Objects\n",
            "with TRansformers (SOTR), simplifies the segmentation pipeline, building on an\n",
            "alternative CNN backbone appended with two parallel subtasks: (1) predicting\n",
            "per-instance category via transformer and (2) dynamically generating\n",
            "segmentation mask with the multi-level upsampling module. SOTR can effectively\n",
            "extract lower-level feature representations and capture long-range context\n",
            "dependencies by Feature Pyramid Network (FPN) and twin transformer,\n",
            "respectively. Meanwhile, compared with the original transformer, the proposed\n",
            "twin transformer is time- and resource-efficient since only a row and a column\n",
            "attention are involved to encode pixels. Moreover, SOTR is easy to be\n",
            "incorporated with various CNN backbones and transformer model variants to make\n",
            "considerable improvements for the segmentation accuracy and training\n",
            "convergence. Extensive experiments show that our SOTR performs well on the MS\n",
            "COCO dataset and surpasses state-of-the-art instance segmentation approaches.\n",
            "We hope our simple but strong framework could serve as a preferment baseline\n",
            "for instance-level recognition. Our code is available at\n",
            "https://github.com/easton-cau/SOTR.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: We consider a set of probabilistic functions of some input variables as a\n",
            "representation of the inputs. We present bounds on how informative a\n",
            "representation is about input data. We extend these bounds to hierarchical\n",
            "representations so that we can quantify the contribution of each layer towards\n",
            "capturing the information in the original data. The special form of these\n",
            "bounds leads to a simple, bottom-up optimization procedure to construct\n",
            "hierarchical representations that are also maximally informative about the\n",
            "data. This optimization has linear computational complexity and constant sample\n",
            "complexity in the number of variables. These results establish a new approach\n",
            "to unsupervised learning of deep representations that is both principled and\n",
            "practical. We demonstrate the usefulness of the approach on both synthetic and\n",
            "real-world data.\n",
            "Label(s): [' ', '-', '.', 'G', 'L', 'M', 'a', 'c', 'd', 'h', 'i', 'n', 'p', 's', 't', 'y']\n",
            " \n",
            "Abstract: Distillation is the technique of training a \"student\" model based on examples\n",
            "that are labeled by a separate \"teacher\" model, which itself is trained on a\n",
            "labeled dataset. The most common explanations for why distillation \"works\" are\n",
            "predicated on the assumption that student is provided with \\emph{soft} labels,\n",
            "\\eg probabilities or confidences, from the teacher model. In this work, we\n",
            "show, that, even when the teacher model is highly overparameterized, and\n",
            "provides \\emph{hard} labels, using a very large held-out unlabeled dataset to\n",
            "train the student model can result in a model that outperforms more\n",
            "\"traditional\" approaches.\n",
            "  Our explanation for this phenomenon is based on recent work on \"double\n",
            "descent\". It has been observed that, once a model's complexity roughly exceeds\n",
            "the amount required to memorize the training data, increasing the complexity\n",
            "\\emph{further} can, counterintuitively, result in \\emph{better} generalization.\n",
            "Researchers have identified several settings in which it takes place, while\n",
            "others have made various attempts to explain it (thus far, with only partial\n",
            "success). In contrast, we avoid these questions, and instead seek to\n",
            "\\emph{exploit} this phenomenon by demonstrating that a highly-overparameterized\n",
            "teacher can avoid overfitting via double descent, while a student trained on a\n",
            "larger independent dataset labeled by this teacher will avoid overfitting due\n",
            "to the size of its training set.\n",
            "Label(s): [' ', '.', 'A', 'G', 'I', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Segmentation of magnetic resonance (MR) images is a fundamental step in many\n",
            "medical imaging-based applications. The recent implementation of deep\n",
            "convolutional neural networks (CNNs) in image processing has been shown to have\n",
            "significant impacts on medical image segmentation. Network training of\n",
            "segmentation CNNs typically requires images and paired annotation data\n",
            "representing pixel-wise tissue labels referred to as masks. However, the\n",
            "supervised training of highly efficient CNNs with deeper structure and more\n",
            "network parameters requires a large number of training images and paired tissue\n",
            "masks. Thus, there is great need to develop a generalized CNN-based\n",
            "segmentation method which would be applicable for a wide variety of MR image\n",
            "datasets with different tissue contrasts. The purpose of this study was to\n",
            "develop and evaluate a generalized CNN-based method for fully-automated\n",
            "segmentation of different MR image datasets using a single set of annotated\n",
            "training data. A technique called cycle-consistent generative adversarial\n",
            "network (CycleGAN) is applied as the core of the proposed method to perform\n",
            "image-to-image translation between MR image datasets with different tissue\n",
            "contrasts. A joint segmentation network is incorporated into the adversarial\n",
            "network to obtain additional segmentation functionality. The proposed method\n",
            "was evaluated for segmenting bone and cartilage on two clinical knee MR image\n",
            "datasets acquired at our institution using only a single set of annotated data\n",
            "from a publicly available knee MR image dataset. The new technique may further\n",
            "improve the applicability and efficiency of CNN-based segmentation of medical\n",
            "images while eliminating the need for large amounts of annotated training data.\n",
            "Label(s): [' ', '.', 'A', 'C', 'I', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Efficient dispatching rule in manufacturing industry is key to ensure product\n",
            "on-time delivery and minimum past-due and inventory cost. Manufacturing,\n",
            "especially in the developed world, is moving towards on-demand manufacturing\n",
            "meaning a high mix, low volume product mix. This requires efficient dispatching\n",
            "that can work in dynamic and stochastic environments, meaning it allows for\n",
            "quick response to new orders received and can work over a disparate set of shop\n",
            "floor settings. In this paper we address this problem of dispatching in\n",
            "manufacturing. Using reinforcement learning (RL), we propose a new design to\n",
            "formulate the shop floor state as a 2-D matrix, incorporate job slack time into\n",
            "state representation, and design lateness and tardiness rewards function for\n",
            "dispatching purpose. However, maintaining a separate RL model for each\n",
            "production line on a manufacturing shop floor is costly and often infeasible.\n",
            "To address this, we enhance our deep RL model with an approach for dispatching\n",
            "policy transfer. This increases policy generalization and saves time and cost\n",
            "for model training and data collection. Experiments show that: (1) our approach\n",
            "performs the best in terms of total discounted reward and average lateness,\n",
            "tardiness, (2) the proposed policy transfer approach reduces training time and\n",
            "increases policy generalization.\n",
            "Label(s): [' ', '.', 'A', 'G', 'I', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Vehicles, pedestrians, and riders are the most important and interesting\n",
            "objects for the perception modules of self-driving vehicles and video\n",
            "surveillance. However, the state-of-the-art performance of detecting such\n",
            "important objects (esp. small objects) is far from satisfying the demand of\n",
            "practical systems. Large-scale, rich-diversity, and high-resolution datasets\n",
            "play an important role in developing better object detection methods to satisfy\n",
            "the demand. Existing public large-scale datasets such as MS COCO collected from\n",
            "websites do not focus on the specific scenarios. Moreover, the popular datasets\n",
            "(e.g., KITTI and Citypersons) collected from the specific scenarios are limited\n",
            "in the number of images and instances, the resolution, and the diversity. To\n",
            "attempt to solve the problem, we build a diverse high-resolution dataset\n",
            "(called TJU-DHD). The dataset contains 115,354 high-resolution images (52%\n",
            "images have a resolution of 1624$\\times$1200 pixels and 48% images have a\n",
            "resolution of at least 2,560$\\times$1,440 pixels) and 709,330 labeled objects\n",
            "in total with a large variance in scale and appearance. Meanwhile, the dataset\n",
            "has a rich diversity in season variance, illumination variance, and weather\n",
            "variance. In addition, a new diverse pedestrian dataset is further built. With\n",
            "the four different detectors (i.e., the one-stage RetinaNet, anchor-free FCOS,\n",
            "two-stage FPN, and Cascade R-CNN), experiments about object detection and\n",
            "pedestrian detection are conducted. We hope that the newly built dataset can\n",
            "help promote the research on object detection and pedestrian detection in these\n",
            "two scenes. The dataset is available at https://github.com/tjubiit/TJU-DHD.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: We address the novel task of jointly reconstructing the 3D shape, texture,\n",
            "and motion of an object from a single motion-blurred image. While previous\n",
            "approaches address the deblurring problem only in the 2D image domain, our\n",
            "proposed rigorous modeling of all object properties in the 3D domain enables\n",
            "the correct description of arbitrary object motion. This leads to significantly\n",
            "better image decomposition and sharper deblurring results. We model the\n",
            "observed appearance of a motion-blurred object as a combination of the\n",
            "background and a 3D object with constant translation and rotation. Our method\n",
            "minimizes a loss on reconstructing the input image via differentiable rendering\n",
            "with suitable regularizers. This enables estimating the textured 3D mesh of the\n",
            "blurred object with high fidelity. Our method substantially outperforms\n",
            "competing approaches on several benchmarks for fast moving objects deblurring.\n",
            "Qualitative results show that the reconstructed 3D mesh generates high-quality\n",
            "temporal super-resolution and novel views of the deblurred object.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Recently, skeleton-based approaches have achieved rapid progress on the basis\n",
            "of great success in skeleton representation. Plenty of researches focus on\n",
            "solving specific problems according to skeleton features. Some skeleton-based\n",
            "approaches have been mentioned in several overviews on object detection as a\n",
            "non-essential part. Nevertheless, there has not been any thorough analysis of\n",
            "skeleton-based approaches attentively. Instead of describing these techniques\n",
            "in terms of theoretical constructs, we devote to summarizing skeleton-based\n",
            "approaches with regard to application fields and given tasks as comprehensively\n",
            "as possible. This paper is conducive to further understanding of skeleton-based\n",
            "application and dealing with particular issues.\n",
            "Label(s): [' ', '.', 'A', 'C', 'I', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Unsupervised person re-identification (Re-ID) is a promising and very\n",
            "challenging research problem in computer vision. Learning robust and\n",
            "discriminative features with unlabeled data is of central importance to Re-ID.\n",
            "Recently, more attention has been paid to unsupervised Re-ID algorithms based\n",
            "on clustered pseudo-label. However, the previous approaches did not fully\n",
            "exploit information of hard samples, simply using cluster centroid or all\n",
            "instances for contrastive learning. In this paper, we propose a Hard-sample\n",
            "Guided Hybrid Contrast Learning (HHCL) approach combining cluster-level loss\n",
            "with instance-level loss for unsupervised person Re-ID. Our approach applies\n",
            "cluster centroid contrastive loss to ensure that the network is updated in a\n",
            "more stable way. Meanwhile, introduction of a hard instance contrastive loss\n",
            "further mines the discriminative information. Extensive experiments on two\n",
            "popular large-scale Re-ID benchmarks demonstrate that our HHCL outperforms\n",
            "previous state-of-the-art methods and significantly improves the performance of\n",
            "unsupervised person Re-ID. The code of our work is available soon at\n",
            "https://github.com/bupt-ai-cz/HHCL-ReID.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Road extraction in remote sensing images is of great importance for a wide\n",
            "range of applications. Because of the complex background, and high density,\n",
            "most of the existing methods fail to accurately extract a road network that\n",
            "appears correct and complete. Moreover, they suffer from either insufficient\n",
            "training data or high costs of manual annotation. To address these problems, we\n",
            "introduce a new model to apply structured domain adaption for synthetic image\n",
            "generation and road segmentation. We incorporate a feature pyramid network into\n",
            "generative adversarial networks to minimize the difference between the source\n",
            "and target domains. A generator is learned to produce quality synthetic images,\n",
            "and the discriminator attempts to distinguish them. We also propose a feature\n",
            "pyramid network that improves the performance of the proposed model by\n",
            "extracting effective features from all the layers of the network for describing\n",
            "different scales objects. Indeed, a novel scale-wise architecture is introduced\n",
            "to learn from the multi-level feature maps and improve the semantics of the\n",
            "features. For optimization, the model is trained by a joint reconstruction loss\n",
            "function, which minimizes the difference between the fake images and the real\n",
            "ones. A wide range of experiments on three datasets prove the superior\n",
            "performance of the proposed approach in terms of accuracy and efficiency. In\n",
            "particular, our model achieves state-of-the-art 78.86 IOU on the Massachusetts\n",
            "dataset with 14.89M parameters and 86.78B FLOPs, with 4x fewer FLOPs but higher\n",
            "accuracy (+3.47% IOU) than the top performer among state-of-the-art approaches\n",
            "used in the evaluation.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Hashing has been a widely-adopted technique for nearest neighbor search in\n",
            "large-scale image retrieval tasks. Recent research has shown that leveraging\n",
            "supervised information can lead to high quality hashing. However, the cost of\n",
            "annotating data is often an obstacle when applying supervised hashing to a new\n",
            "domain. Moreover, the results can suffer from the robustness problem as the\n",
            "data at training and test stage could come from similar but different\n",
            "distributions. This paper studies the exploration of generating synthetic data\n",
            "through semi-supervised generative adversarial networks (GANs), which leverages\n",
            "largely unlabeled and limited labeled training data to produce highly\n",
            "compelling data with intrinsic invariance and global coherence, for better\n",
            "understanding statistical structures of natural data. We demonstrate that the\n",
            "above two limitations can be well mitigated by applying the synthetic data for\n",
            "hashing. Specifically, a novel deep semantic hashing with GANs (DSH-GANs) is\n",
            "presented, which mainly consists of four components: a deep convolution neural\n",
            "networks (CNN) for learning image representations, an adversary stream to\n",
            "distinguish synthetic images from real ones, a hash stream for encoding image\n",
            "representations to hash codes and a classification stream. The whole\n",
            "architecture is trained end-to-end by jointly optimizing three losses, i.e.,\n",
            "adversarial loss to correct label of synthetic or real for each sample, triplet\n",
            "ranking loss to preserve the relative similarity ordering in the input\n",
            "real-synthetic triplets and classification loss to classify each sample\n",
            "accurately. Extensive experiments conducted on both CIFAR-10 and NUS-WIDE image\n",
            "benchmarks validate the capability of exploiting synthetic images for hashing.\n",
            "Our framework also achieves superior results when compared to state-of-the-art\n",
            "deep hash models.\n",
            "Label(s): [' ', '.', 'C', 'I', 'R', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Augmenting an agent's control with useful higher-level behaviors called\n",
            "options can greatly reduce the sample complexity of reinforcement learning, but\n",
            "manually designing options is infeasible in high-dimensional and abstract state\n",
            "spaces. While recent work has proposed several techniques for automated option\n",
            "discovery, they do not scale to multi-level hierarchies and to expressive\n",
            "representations such as deep networks. We present Discovery of Deep Options\n",
            "(DDO), a policy-gradient algorithm that discovers parametrized options from a\n",
            "set of demonstration trajectories, and can be used recursively to discover\n",
            "additional levels of the hierarchy. The scalability of our approach to\n",
            "multi-level hierarchies stems from the decoupling of low-level option discovery\n",
            "from high-level meta-control policy learning, facilitated by\n",
            "under-parametrization of the high level. We demonstrate that using the\n",
            "discovered options to augment the action space of Deep Q-Network agents can\n",
            "accelerate learning by guiding exploration in tasks where random actions are\n",
            "unlikely to reach valuable states. We show that DDO is effective in adding\n",
            "options that accelerate learning in 4 out of 5 Atari RAM environments chosen in\n",
            "our experiments. We also show that DDO can discover structure in robot-assisted\n",
            "surgical videos and kinematics that match expert annotation with 72% accuracy.\n",
            "Label(s): [' ', '.', 'G', 'L', 'c', 's']\n",
            " \n",
            "Abstract: Graph Drawing techniques have been developed in the last few years with the\n",
            "purpose of producing aesthetically pleasing node-link layouts. Recently, the\n",
            "employment of differentiable loss functions has paved the road to the massive\n",
            "usage of Gradient Descent and related optimization algorithms. In this paper,\n",
            "we propose a novel framework for the development of Graph Neural Drawers (GND),\n",
            "machines that rely on neural computation for constructing efficient and complex\n",
            "maps. GND are Graph Neural Networks (GNNs) whose learning process can be driven\n",
            "by any provided loss function, such as the ones commonly employed in Graph\n",
            "Drawing. Moreover, we prove that this mechanism can be guided by loss functions\n",
            "computed by means of Feedforward Neural Networks, on the basis of supervision\n",
            "hints that express beauty properties, like the minimization of crossing edges.\n",
            "In this context, we show that GNNs can nicely be enriched by positional\n",
            "features to deal also with unlabelled vertexes. We provide a proof-of-concept\n",
            "by constructing a loss function for the edge-crossing and provide quantitative\n",
            "and qualitative comparisons among different GNN models working under the\n",
            "proposed framework.\n",
            "Label(s): [' ', '.', 'G', 'L', 'c', 's']\n",
            " \n",
            "Abstract: Graph convolutional networks (GCNs) have been widely used and achieved\n",
            "remarkable results in skeleton-based action recognition. In GCNs, graph\n",
            "topology dominates feature aggregation and therefore is the key to extracting\n",
            "representative features. In this work, we propose a novel Channel-wise Topology\n",
            "Refinement Graph Convolution (CTR-GC) to dynamically learn different topologies\n",
            "and effectively aggregate joint features in different channels for\n",
            "skeleton-based action recognition. The proposed CTR-GC models channel-wise\n",
            "topologies through learning a shared topology as a generic prior for all\n",
            "channels and refining it with channel-specific correlations for each channel.\n",
            "Our refinement method introduces few extra parameters and significantly reduces\n",
            "the difficulty of modeling channel-wise topologies. Furthermore, via\n",
            "reformulating graph convolutions into a unified form, we find that CTR-GC\n",
            "relaxes strict constraints of graph convolutions, leading to stronger\n",
            "representation capability. Combining CTR-GC with temporal modeling modules, we\n",
            "develop a powerful graph convolutional network named CTR-GCN which notably\n",
            "outperforms state-of-the-art methods on the NTU RGB+D, NTU RGB+D 120, and\n",
            "NW-UCLA datasets.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Meta-reinforcement learning (meta-RL) algorithms allow for agents to learn\n",
            "new behaviors from small amounts of experience, mitigating the sample\n",
            "inefficiency problem in RL. However, while meta-RL agents can adapt quickly to\n",
            "new tasks at test time after experiencing only a few trajectories, the\n",
            "meta-training process is still sample-inefficient. Prior works have found that\n",
            "in the multi-task RL setting, relabeling past transitions and thus sharing\n",
            "experience among tasks can improve sample efficiency and asymptotic\n",
            "performance. We apply this idea to the meta-RL setting and devise a new\n",
            "relabeling method called Hindsight Foresight Relabeling (HFR). We construct a\n",
            "relabeling distribution using the combination of \"hindsight\", which is used to\n",
            "relabel trajectories using reward functions from the training task\n",
            "distribution, and \"foresight\", which takes the relabeled trajectories and\n",
            "computes the utility of each trajectory for each task. HFR is easy to implement\n",
            "and readily compatible with existing meta-RL algorithms. We find that HFR\n",
            "improves performance when compared to other relabeling methods on a variety of\n",
            "meta-RL tasks.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Existing video stabilization methods often generate visible distortion or\n",
            "require aggressive cropping of frame boundaries, resulting in smaller field of\n",
            "views. In this work, we present a frame synthesis algorithm to achieve\n",
            "full-frame video stabilization. We first estimate dense warp fields from\n",
            "neighboring frames and then synthesize the stabilized frame by fusing the\n",
            "warped contents. Our core technical novelty lies in the learning-based\n",
            "hybrid-space fusion that alleviates artifacts caused by optical flow inaccuracy\n",
            "and fast-moving objects. We validate the effectiveness of our method on the\n",
            "NUS, selfie, and DeepStab video datasets. Extensive experiment results\n",
            "demonstrate the merits of our approach over prior video stabilization methods.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: In this paper, we propose a novel end-to-end model, namely Single-Stage\n",
            "Grounding network (SSG), to localize the referent given a referring expression\n",
            "within an image. Different from previous multi-stage models which rely on\n",
            "object proposals or detected regions, our proposed model aims to comprehend a\n",
            "referring expression through one single stage without resorting to region\n",
            "proposals as well as the subsequent region-wise feature extraction.\n",
            "Specifically, a multimodal interactor is proposed to summarize the local region\n",
            "features regarding the referring expression attentively. Subsequently, a\n",
            "grounder is proposed to localize the referring expression within the given\n",
            "image directly. For further improving the localization accuracy, a guided\n",
            "attention mechanism is proposed to enforce the grounder to focus on the central\n",
            "region of the referent. Moreover, by exploiting and predicting visual attribute\n",
            "information, the grounder can further distinguish the referent objects within\n",
            "an image and thereby improve the model performance. Experiments on RefCOCO,\n",
            "RefCOCO+, and RefCOCOg datasets demonstrate that our proposed SSG without\n",
            "relying on any region proposals can achieve comparable performance with other\n",
            "advanced models. Furthermore, our SSG outperforms the previous models and\n",
            "achieves the state-of-art performance on the ReferItGame dataset. More\n",
            "importantly, our SSG is time efficient and can ground a referring expression in\n",
            "a 416*416 image from the RefCOCO dataset in 25ms (40 referents per second) on\n",
            "average with a Nvidia Tesla P40, accomplishing more than 9* speedups over the\n",
            "existing multi-stage models.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Training robust deep learning (DL) systems for disease detection from medical\n",
            "images is challenging due to limited images covering different disease types\n",
            "and severity. The problem is especially acute, where there is a severe class\n",
            "imbalance. We propose an active learning (AL) framework to select most\n",
            "informative samples for training our model using a Bayesian neural network.\n",
            "Informative samples are then used within a novel class aware generative\n",
            "adversarial network (CAGAN) to generate realistic chest xray images for data\n",
            "augmentation by transferring characteristics from one class label to another.\n",
            "Experiments show our proposed AL framework is able to achieve state-of-the-art\n",
            "performance by using about $35\\%$ of the full dataset, thus saving significant\n",
            "time and effort over conventional methods.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: This paper proposes a novel simultaneous localization and mapping (SLAM)\n",
            "approach, namely Attention-SLAM, which simulates human navigation mode by\n",
            "combining a visual saliency model (SalNavNet) with traditional monocular visual\n",
            "SLAM. Most SLAM methods treat all the features extracted from the images as\n",
            "equal importance during the optimization process. However, the salient feature\n",
            "points in scenes have more significant influence during the human navigation\n",
            "process. Therefore, we first propose a visual saliency model called SalVavNet\n",
            "in which we introduce a correlation module and propose an adaptive Exponential\n",
            "Moving Average (EMA) module. These modules mitigate the center bias to enable\n",
            "the saliency maps generated by SalNavNet to pay more attention to the same\n",
            "salient object. Moreover, the saliency maps simulate the human behavior for the\n",
            "refinement of SLAM results. The feature points extracted from the salient\n",
            "regions have greater importance in optimization process. We add semantic\n",
            "saliency information to the Euroc dataset to generate an open-source saliency\n",
            "SLAM dataset. Comprehensive test results prove that Attention-SLAM outperforms\n",
            "benchmarks such as Direct Sparse Odometry (DSO), ORB-SLAM, and Salient DSO in\n",
            "terms of efficiency, accuracy, and robustness in most test cases.\n",
            "Label(s): [' ', '.', 'C', 'O', 'R', 'V', 'c', 's']\n",
            " \n",
            "Abstract: This paper proposes combining spatio-temporal appearance (STA) descriptors\n",
            "with optical flow for human action recognition. The STA descriptors are local\n",
            "histogram-based descriptors of space-time, suitable for building a partial\n",
            "representation of arbitrary spatio-temporal phenomena. Because of the\n",
            "possibility of iterative refinement, they are interesting in the context of\n",
            "online human action recognition. We investigate the use of dense optical flow\n",
            "as the image function of the STA descriptor for human action recognition, using\n",
            "two different algorithms for computing the flow: the Farneb\\\"ack algorithm and\n",
            "the TVL1 algorithm. We provide a detailed analysis of the influencing optical\n",
            "flow algorithm parameters on the produced optical flow fields. An extensive\n",
            "experimental validation of optical flow-based STA descriptors in human action\n",
            "recognition is performed on the KTH human action dataset. The encouraging\n",
            "experimental results suggest the potential of our approach in online human\n",
            "action recognition.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Human Activity Recognition (HAR) based on motion sensors has drawn a lot of\n",
            "attention over the last few years, since perceiving the human status enables\n",
            "context-aware applications to adapt their services on users' needs. However,\n",
            "motion sensor fusion and feature extraction have not reached their full\n",
            "potentials, remaining still an open issue. In this paper, we introduce\n",
            "PerceptionNet, a deep Convolutional Neural Network (CNN) that applies a late 2D\n",
            "convolution to multimodal time-series sensor data, in order to extract\n",
            "automatically efficient features for HAR. We evaluate our approach on two\n",
            "public available HAR datasets to demonstrate that the proposed model fuses\n",
            "effectively multimodal sensors and improves the performance of HAR. In\n",
            "particular, PerceptionNet surpasses the performance of state-of-the-art HAR\n",
            "methods based on: (i) features extracted from humans, (ii) deep CNNs exploiting\n",
            "early fusion approaches, and (iii) Long Short-Term Memory (LSTM), by an average\n",
            "accuracy of more than 3%.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: In many vision-based reinforcement learning (RL) problems, the agent controls\n",
            "a movable object in its visual field, e.g., the player's avatar in video games\n",
            "and the robotic arm in visual grasping and manipulation. Leveraging\n",
            "action-conditioned video prediction, we propose an end-to-end learning\n",
            "framework to disentangle the controllable object from the observation signal.\n",
            "The disentangled representation is shown to be useful for RL as additional\n",
            "observation channels to the agent. Experiments on a set of Atari games with the\n",
            "popular Double DQN algorithm demonstrate improved sample efficiency and game\n",
            "performance (from 222.8% to 261.4% measured in normalized game scores, with\n",
            "prediction bonus reward).\n",
            "Label(s): [' ', '.', 'C', 'G', 'L', 'M', 'V', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: The task of searching certain people in videos has seen increasing potential\n",
            "in real-world applications, such as video organization and editing. Most\n",
            "existing approaches are devised to work in an offline manner, where identities\n",
            "can only be inferred after an entire video is examined. This working manner\n",
            "precludes such methods from being applied to online services or those\n",
            "applications that require real-time responses. In this paper, we propose an\n",
            "online person search framework, which can recognize people in a video on the\n",
            "fly. This framework maintains a multimodal memory bank at its heart as the\n",
            "basis for person recognition, and updates it dynamically with a policy obtained\n",
            "by reinforcement learning. Our experiments on a large movie dataset show that\n",
            "the proposed method is effective, not only achieving remarkable improvements\n",
            "over online schemes but also outperforming offline methods.\n",
            "Label(s): [' ', '.', 'A', 'C', 'G', 'I', 'L', 'M', 'V', 'c', 'e', 's']\n",
            " \n",
            "Abstract: This position paper proposes a fresh look at Reinforcement Learning (RL) from\n",
            "the perspective of data-efficiency. Data-efficient RL has gone through three\n",
            "major stages: pure on-line RL where every data-point is considered only once,\n",
            "RL with a replay buffer where additional learning is done on a portion of the\n",
            "experience, and finally transition memory based RL, where, conceptually, all\n",
            "transitions are stored and re-used in every update step. While inferring\n",
            "knowledge from all explicitly stored experience has lead to a tremendous gain\n",
            "in data-efficiency, the question of how this data is collected has been vastly\n",
            "understudied. We argue that data-efficiency can only be achieved through\n",
            "careful consideration of both aspects. We propose to make this insight explicit\n",
            "via a paradigm that we call 'Collect and Infer', which explicitly models RL as\n",
            "two separate but interconnected processes, concerned with data collection and\n",
            "knowledge inference respectively. We discuss implications of the paradigm, how\n",
            "its ideas are reflected in the literature, and how it can guide future research\n",
            "into data efficient RL.\n",
            "Label(s): [' ', '.', 'G', 'L', 'c', 's']\n",
            " \n",
            "Abstract: Models based on deep convolutional networks have dominated recent image\n",
            "interpretation tasks; we investigate whether models which are also recurrent,\n",
            "or \"temporally deep\", are effective for tasks involving sequences, visual and\n",
            "otherwise. We develop a novel recurrent convolutional architecture suitable for\n",
            "large-scale visual learning which is end-to-end trainable, and demonstrate the\n",
            "value of these models on benchmark video recognition tasks, image description\n",
            "and retrieval problems, and video narration challenges. In contrast to current\n",
            "models which assume a fixed spatio-temporal receptive field or simple temporal\n",
            "averaging for sequential processing, recurrent convolutional models are \"doubly\n",
            "deep\"' in that they can be compositional in spatial and temporal \"layers\". Such\n",
            "models may have advantages when target concepts are complex and/or training\n",
            "data are limited. Learning long-term dependencies is possible when\n",
            "nonlinearities are incorporated into the network state updates. Long-term RNN\n",
            "models are appealing in that they directly can map variable-length inputs\n",
            "(e.g., video frames) to variable length outputs (e.g., natural language text)\n",
            "and can model complex temporal dynamics; yet they can be optimized with\n",
            "backpropagation. Our recurrent long-term models are directly connected to\n",
            "modern visual convnet models and can be jointly trained to simultaneously learn\n",
            "temporal dynamics and convolutional perceptual representations. Our results\n",
            "show such models have distinct advantages over state-of-the-art models for\n",
            "recognition or generation which are separately defined and/or optimized.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: We present evidence that many common convolutional neural networks (CNNs)\n",
            "trained for face verification learn functions that are nearly equivalent under\n",
            "rotation. More specifically, we demonstrate that one face verification model's\n",
            "embeddings (i.e. last--layer activations) can be compared directly to another\n",
            "model's embeddings after only a rotation or linear transformation, with little\n",
            "performance penalty. This finding is demonstrated using IJB-C 1:1 verification\n",
            "across the combinations of ten modern off-the-shelf CNN-based face verification\n",
            "models which vary in training dataset, CNN architecture, way of using angular\n",
            "loss, or some combination of the 3, and achieve a mean true accept rate of 0.96\n",
            "at a false accept rate of 0.01. When instead evaluating embeddings generated\n",
            "from two CNNs, where one CNN's embeddings are mapped with a linear\n",
            "transformation, the mean true accept rate drops to 0.95 using the same\n",
            "verification paradigm. Restricting these linear maps to only perform rotation\n",
            "produces a mean true accept rate of 0.91. These mappings' existence suggests\n",
            "that a common representation is learned by models with variation in training or\n",
            "structure. A discovery such as this likely has broad implications, and we\n",
            "provide an application in which face embeddings can be de-anonymized using a\n",
            "limited number of samples.\n",
            "Label(s): [' ', '.', 'A', 'C', 'I', 'V', 'c', 's']\n",
            " \n",
            "Abstract: A deep neural networks based method is proposed to convert single\n",
            "polarization grayscale SAR image to fully polarimetric. It consists of two\n",
            "components: a feature extractor network to extract hierarchical multi-scale\n",
            "spatial features of grayscale SAR image, followed by a feature translator\n",
            "network to map spatial feature to polarimetric feature with which the\n",
            "polarimetric covariance matrix of each pixel can be reconstructed. Both\n",
            "qualitative and quantitative experiments with real fully polarimetric data are\n",
            "conducted to show the efficacy of the proposed method. The reconstructed\n",
            "full-pol SAR image agrees well with the true full-pol image. Existing PolSAR\n",
            "applications such as model-based decomposition and unsupervised classification\n",
            "can be applied directly to the reconstructed full-pol SAR images. This\n",
            "framework can be easily extended to reconstruction of full-pol data from\n",
            "compact-pol data. The experiment results also show that the proposed method\n",
            "could be potentially used for interference removal on the cross-polarization\n",
            "channel.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Generative adversarial network (GAN) has become one of the most important\n",
            "neural network models for classical unsupervised machine learning. A variety of\n",
            "discriminator loss functions have been developed to train GAN's discriminators\n",
            "and they all have a common structure: a sum of real and fake losses that only\n",
            "depends on the actual and generated data respectively. One challenge associated\n",
            "with an equally weighted sum of two losses is that the training may benefit one\n",
            "loss but harm the other, which we show causes instability and mode collapse. In\n",
            "this paper, we introduce a new family of discriminator loss functions that\n",
            "adopts a weighted sum of real and fake parts, which we call adaptive weighted\n",
            "loss functions or aw-loss functions. Using the gradients of the real and fake\n",
            "parts of the loss, we can adaptively choose weights to train a discriminator in\n",
            "the direction that benefits the GAN's stability. Our method can be potentially\n",
            "applied to any discriminator model with a loss that is a sum of the real and\n",
            "fake parts. Experiments validated the effectiveness of our loss functions on an\n",
            "unconditional image generation task, improving the baseline results by a\n",
            "significant margin on CIFAR-10, STL-10, and CIFAR-100 datasets in Inception\n",
            "Scores and FID.\n",
            "Label(s): [' ', '.', 'G', 'L', 'c', 's']\n",
            " \n",
            "Abstract: Reconstruction of seismic data with missing traces is a long-standing issue\n",
            "in seismic data processing. In recent years, rank reduction operations are\n",
            "being commonly utilized to overcome this problem, which require the rank of\n",
            "seismic data to be a prior. However, the rank of field data is unknown; usually\n",
            "it requires much time to manually adjust the rank and just obtain an\n",
            "approximated rank. Methods based on deep learning require very large datasets\n",
            "for training; however acquiring large datasets is difficult owing to physical\n",
            "or financial constraints in practice. Therefore, in this work, we developed a\n",
            "novel method based on unsupervised learning using the intrinsic properties of a\n",
            "convolutional neural network known as U-net, without training datasets. Only\n",
            "one undersampled seismic data was needed, and the deep seismic prior of input\n",
            "data could be exploited by the network itself, thus making the reconstruction\n",
            "convenient. Furthermore, this method can handle both irregular and regular\n",
            "seismic data. Synthetic and field data were tested to assess the performance of\n",
            "the proposed algorithm (DSPRecon algorithm); the advantages of using our method\n",
            "were evaluated by comparing it with the singular spectrum analysis (SSA) method\n",
            "for irregular data reconstruction and de-aliased Cadzow method for regular data\n",
            "reconstruction. Experimental results showed that our method provided better\n",
            "reconstruction performance than the SSA or Cadzow methods. The recovered\n",
            "signal-to-noise ratios (SNRs) were 32.68 dB and 19.11 dB for the DSPRecon and\n",
            "SSA algorithms, respectively. Those for the DSPRecon and Cadzow methods were\n",
            "35.91 dB and 15.32 dB, respectively.\n",
            "Label(s): [' ', '-', '.', 'G', 'L', 'M', 'a', 'c', 'd', 'h', 'i', 'n', 'p', 's', 't', 'y']\n",
            " \n",
            "Abstract: Segmentation algorithms are prone to make topological errors on fine-scale\n",
            "structures, e.g., broken connections. We propose a novel method that learns to\n",
            "segment with correct topology. In particular, we design a continuous-valued\n",
            "loss function that enforces a segmentation to have the same topology as the\n",
            "ground truth, i.e., having the same Betti number. The proposed\n",
            "topology-preserving loss function is differentiable and we incorporate it into\n",
            "end-to-end training of a deep neural network. Our method achieves much better\n",
            "performance on the Betti number error, which directly accounts for the\n",
            "topological correctness. It also performs superiorly on other topology-relevant\n",
            "metrics, e.g., the Adjusted Rand Index and the Variation of Information. We\n",
            "illustrate the effectiveness of the proposed method on a broad spectrum of\n",
            "natural and biomedical datasets.\n",
            "Label(s): [' ', '.', 'C', 'G', 'V', 'c', 's']\n",
            " \n",
            "Abstract: In this work, we have developed a robust lane detection and departure warning\n",
            "technique. Our system is based on single camera sensor. For lane detection a\n",
            "modified Inverse Perspective Mapping using only a few extrinsic camera\n",
            "parameters and illuminant Invariant techniques is used. Lane markings are\n",
            "represented using a combination of 2nd and 4th order steerable filters, robust\n",
            "to shadowing. Effect of shadowing and extra sun light are removed using Lab\n",
            "color space, and illuminant invariant representation. Lanes are assumed to be\n",
            "cubic curves and fitted using robust RANSAC. This method can reliably detect\n",
            "lanes of the road and its boundary. This method has been experimented in Indian\n",
            "road conditions under different challenging situations and the result obtained\n",
            "were very good. For lane departure angle an optical flow based method were\n",
            "used.\n",
            "Label(s): [' ', '.', '4', '5', '6', '8', 'C', 'T', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Exploration is essential for reinforcement learning (RL). To face the\n",
            "challenges of exploration, we consider a reward-free RL framework that\n",
            "completely separates exploration from exploitation and brings new challenges\n",
            "for exploration algorithms. In the exploration phase, the agent learns an\n",
            "exploratory policy by interacting with a reward-free environment and collects a\n",
            "dataset of transitions by executing the policy. In the planning phase, the\n",
            "agent computes a good policy for any reward function based on the dataset\n",
            "without further interacting with the environment. This framework is suitable\n",
            "for the meta RL setting where there are many reward functions of interest. In\n",
            "the exploration phase, we propose to maximize the Renyi entropy over the\n",
            "state-action space and justify this objective theoretically. The success of\n",
            "using Renyi entropy as the objective results from its encouragement to explore\n",
            "the hard-to-reach state-actions. We further deduce a policy gradient\n",
            "formulation for this objective and design a practical exploration algorithm\n",
            "that can deal with complex environments. In the planning phase, we solve for\n",
            "good policies given arbitrary reward functions using a batch RL algorithm.\n",
            "Empirically, we show that our exploration algorithm is effective and sample\n",
            "efficient, and results in superior policies for arbitrary reward functions in\n",
            "the planning phase.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Unsupervised person re-identification (Re-Id) has attracted increasing\n",
            "attention due to its practical application in the read-world video surveillance\n",
            "system. The traditional unsupervised Re-Id are mostly based on the method\n",
            "alternating between clustering and fine-tuning with the classification or\n",
            "metric learning objectives on the grouped clusters. However, since person Re-Id\n",
            "is an open-set problem, the clustering based methods often leave out lots of\n",
            "outlier instances or group the instances into the wrong clusters, thus they can\n",
            "not make full use of the training samples as a whole. To solve these problems,\n",
            "we present the hybrid dynamic cluster contrast and probability distillation\n",
            "algorithm. It formulates the unsupervised Re-Id problem into an unified\n",
            "local-to-global dynamic contrastive learning and self-supervised probability\n",
            "distillation framework. Specifically, the proposed method can make the utmost\n",
            "of the self-supervised signals of all the clustered and un-clustered instances,\n",
            "from both the instances' self-contrastive level and the probability\n",
            "distillation respective, in the memory-based non-parametric manner. Besides,\n",
            "the proposed hybrid local-to-global contrastive learning can take full\n",
            "advantage of the informative and valuable training examples for effective and\n",
            "robust training. Extensive experiment results show that the proposed method\n",
            "achieves superior performances to state-of-the-art methods, under both the\n",
            "purely unsupervised and unsupervised domain adaptation experiment settings.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Elastography ultrasound (EUS) provides additional bio-mechanical in-formation\n",
            "about lesion for B-mode ultrasound (BUS) in the diagnosis of breast cancers.\n",
            "However, joint utilization of both BUS and EUS is not popular due to the lack\n",
            "of EUS devices in rural hospitals, which arouses a novel modality im-balance\n",
            "problem in computer-aided diagnosis (CAD) for breast cancers. Current transfer\n",
            "learning (TL) pay little attention to this special issue of clinical modality\n",
            "imbalance, that is, the source domain (EUS modality) has fewer labeled samples\n",
            "than those in the target domain (BUS modality). Moreover, these TL methods\n",
            "cannot fully use the label information to explore the intrinsic relation\n",
            "between two modalities and then guide the promoted knowledge transfer. To this\n",
            "end, we propose a novel doubly supervised TL network (DDSTN) that integrates\n",
            "the Learning Using Privileged Information (LUPI) paradigm and the Maximum Mean\n",
            "Discrepancy (MMD) criterion into a unified deep TL framework. The proposed\n",
            "algorithm can not only make full use of the shared labels to effectively guide\n",
            "knowledge transfer by LUPI paradigm, but also perform additional super-vised\n",
            "transfer between unpaired data. We further introduce the MMD criterion to\n",
            "enhance the knowledge transfer. The experimental results on the breast\n",
            "ultra-sound dataset indicate that the proposed DDSTN outperforms all the\n",
            "compared state-of-the-art algorithms for the BUS-based CAD.\n",
            "Label(s): [' ', '.', 'C', 'G', 'L', 'M', 'V', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Deep residual networks (ResNets) and their variants are widely used in many\n",
            "computer vision applications and natural language processing tasks. However,\n",
            "the theoretical principles for designing and training ResNets are still not\n",
            "fully understood. Recently, several points of view have emerged to try to\n",
            "interpret ResNet theoretically, such as unraveled view, unrolled iterative\n",
            "estimation and dynamical systems view. In this paper, we adopt the dynamical\n",
            "systems point of view, and analyze the lesioning properties of ResNet both\n",
            "theoretically and experimentally. Based on these analyses, we additionally\n",
            "propose a novel method for accelerating ResNet training. We apply the proposed\n",
            "method to train ResNets and Wide ResNets for three image classification\n",
            "benchmarks, reducing training time by more than 40% with superior or on-par\n",
            "accuracy.\n",
            "Label(s): [' ', '.', 'C', 'L', 'M', 'V', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Recent studies of generalization in deep learning have observed a puzzling\n",
            "trend: accuracies of models on one data distribution are approximately linear\n",
            "functions of the accuracies on another distribution. We explain this trend\n",
            "under an intuitive assumption on model similarity, which was verified\n",
            "empirically in prior work. More precisely, we assume the probability that two\n",
            "models agree in their predictions is higher than what we can infer from their\n",
            "accuracy levels alone. Then, we show that a linear trend must occur when\n",
            "evaluating models on two distributions unless the size of the distribution\n",
            "shift is large. This work emphasizes the value of understanding model\n",
            "similarity, which can have an impact on the generalization and robustness of\n",
            "classification models.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Object detection or localization is an incremental step in progression from\n",
            "coarse to fine digital image inference. It not only provides the classes of the\n",
            "image objects, but also provides the location of the image objects which have\n",
            "been classified. The location is given in the form of bounding boxes or\n",
            "centroids. Semantic segmentation gives fine inference by predicting labels for\n",
            "every pixel in the input image. Each pixel is labelled according to the object\n",
            "class within which it is enclosed. Furthering this evolution, instance\n",
            "segmentation gives different labels for separate instances of objects belonging\n",
            "to the same class. Hence, instance segmentation may be defined as the technique\n",
            "of simultaneously solving the problem of object detection as well as that of\n",
            "semantic segmentation. In this survey paper on instance segmentation -- its\n",
            "background, issues, techniques, evolution, popular datasets, related work up to\n",
            "the state of the art and future scope have been discussed. The paper provides\n",
            "valuable information for those who want to do research in the field of instance\n",
            "segmentation.\n",
            "Label(s): [' ', '.', 'C', 'G', 'I', 'L', 'V', 'c', 'e', 's']\n",
            " \n",
            "Abstract: Modern deep learning methods are very sensitive to many hyperparameters, and,\n",
            "due to the long training times of state-of-the-art models, vanilla Bayesian\n",
            "hyperparameter optimization is typically computationally infeasible. On the\n",
            "other hand, bandit-based configuration evaluation approaches based on random\n",
            "search lack guidance and do not converge to the best configurations as quickly.\n",
            "Here, we propose to combine the benefits of both Bayesian optimization and\n",
            "bandit-based methods, in order to achieve the best of both worlds: strong\n",
            "anytime performance and fast convergence to optimal configurations. We propose\n",
            "a new practical state-of-the-art hyperparameter optimization method, which\n",
            "consistently outperforms both Bayesian optimization and Hyperband on a wide\n",
            "range of problem types, including high-dimensional toy functions, support\n",
            "vector machines, feed-forward neural networks, Bayesian neural networks, deep\n",
            "reinforcement learning, and convolutional neural networks. Our method is robust\n",
            "and versatile, while at the same time being conceptually simple and easy to\n",
            "implement.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Recently, several optimization methods have been successfully applied to the\n",
            "hyperparameter optimization of deep neural networks (DNNs). The methods work by\n",
            "modeling the joint distribution of hyperparameter values and corresponding\n",
            "error. Those methods become less practical when applied to modern DNNs whose\n",
            "training may take a few days and thus one cannot collect sufficient\n",
            "observations to accurately model the distribution. To address this challenging\n",
            "issue, we propose a method that learns to transfer optimal hyperparameter\n",
            "values for a small source dataset to hyperparameter values with comparable\n",
            "performance on a dataset of interest. As opposed to existing transfer learning\n",
            "methods, our proposed method does not use hand-designed features. Instead, it\n",
            "uses surrogates to model the hyperparameter-error distributions of the two\n",
            "datasets and trains a neural network to learn the transfer function. Extensive\n",
            "experiments on three CV benchmark datasets clearly demonstrate the efficiency\n",
            "of our method.\n",
            "Label(s): [' ', '.', 'C', 'E', 'G', 'L', 'M', 'N', 'V', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Recovering the 3D representation of an object from single-view or multi-view\n",
            "RGB images by deep neural networks has attracted increasing attention in the\n",
            "past few years. Several mainstream works (e.g., 3D-R2N2) use recurrent neural\n",
            "networks (RNNs) to fuse multiple feature maps extracted from input images\n",
            "sequentially. However, when given the same set of input images with different\n",
            "orders, RNN-based approaches are unable to produce consistent reconstruction\n",
            "results. Moreover, due to long-term memory loss, RNNs cannot fully exploit\n",
            "input images to refine reconstruction results. To solve these problems, we\n",
            "propose a novel framework for single-view and multi-view 3D reconstruction,\n",
            "named Pix2Vox. By using a well-designed encoder-decoder, it generates a coarse\n",
            "3D volume from each input image. Then, a context-aware fusion module is\n",
            "introduced to adaptively select high-quality reconstructions for each part\n",
            "(e.g., table legs) from different coarse 3D volumes to obtain a fused 3D\n",
            "volume. Finally, a refiner further refines the fused 3D volume to generate the\n",
            "final output. Experimental results on the ShapeNet and Pix3D benchmarks\n",
            "indicate that the proposed Pix2Vox outperforms state-of-the-arts by a large\n",
            "margin. Furthermore, the proposed method is 24 times faster than 3D-R2N2 in\n",
            "terms of backward inference time. The experiments on ShapeNet unseen 3D\n",
            "categories have shown the superior generalization abilities of our method.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Computational color constancy refers to the problem of computing the\n",
            "illuminant color so that the images of a scene under varying illumination can\n",
            "be normalized to an image under the canonical illumination. In this paper, we\n",
            "adopt a deep learning framework for the illumination estimation problem. The\n",
            "proposed method works under the assumption of uniform illumination over the\n",
            "scene and aims for the accurate illuminant color computation. Specifically, we\n",
            "trained the convolutional neural network to solve the problem by casting the\n",
            "color constancy problem as an illumination classification problem. We designed\n",
            "the deep learning architecture so that the output of the network can be\n",
            "directly used for computing the color of the illumination. Experimental results\n",
            "show that our deep network is able to extract useful features for the\n",
            "illumination estimation and our method outperforms all previous color constancy\n",
            "methods on multiple test datasets.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Detecting communities on graphs has received significant interest in recent\n",
            "literature. Current state-of-the-art community embedding approach called\n",
            "\\textit{ComE} tackles this problem by coupling graph embedding with community\n",
            "detection. Considering the success of hyperbolic representations of\n",
            "graph-structured data in last years, an ongoing challenge is to set up a\n",
            "hyperbolic approach for the community detection problem. The present paper\n",
            "meets this challenge by introducing a Riemannian equivalent of \\textit{ComE}.\n",
            "Our proposed approach combines hyperbolic embeddings with Riemannian K-means or\n",
            "Riemannian mixture models to perform community detection. We illustrate the\n",
            "usefulness of this framework through several experiments on real-world social\n",
            "networks and comparisons with \\textit{ComE} and recent hyperbolic-based\n",
            "classification approaches.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Tabular data prediction (TDP) is one of the most popular industrial\n",
            "applications, and various methods have been designed to improve the prediction\n",
            "performance. However, existing works mainly focus on feature interactions and\n",
            "ignore sample relations, e.g., users with the same education level might have a\n",
            "similar ability to repay the debt. In this work, by explicitly and\n",
            "systematically modeling sample relations, we propose a novel framework TabGNN\n",
            "based on recently popular graph neural networks (GNN). Specifically, we firstly\n",
            "construct a multiplex graph to model the multifaceted sample relations, and\n",
            "then design a multiplex graph neural network to learn enhanced representation\n",
            "for each sample. To integrate TabGNN with the tabular solution in our company,\n",
            "we concatenate the learned embeddings and the original ones, which are then fed\n",
            "to prediction models inside the solution. Experiments on eleven TDP datasets\n",
            "from various domains, including classification and regression ones, show that\n",
            "TabGNN can consistently improve the performance compared to the tabular\n",
            "solution AutoFE in 4Paradigm.\n",
            "Label(s): [' ', '.', 'G', 'L', 'c', 's']\n",
            " \n",
            "Abstract: Point Cloud Sampling and Recovery (PCSR) is critical for massive real-time\n",
            "point cloud collection and processing since raw data usually requires large\n",
            "storage and computation. In this paper, we address a fundamental problem in\n",
            "PCSR: How to downsample the dense point cloud with arbitrary scales while\n",
            "preserving the local topology of discarding points in a case-agnostic manner\n",
            "(i.e. without additional storage for point relationship)? We propose a novel\n",
            "Locally Invertible Embedding for point cloud adaptive sampling and recovery\n",
            "(PointLIE). Instead of learning to predict the underlying geometry details in a\n",
            "seemingly plausible manner, PointLIE unifies point cloud sampling and\n",
            "upsampling to one single framework through bi-directional learning.\n",
            "Specifically, PointLIE recursively samples and adjusts neighboring points on\n",
            "each scale. Then it encodes the neighboring offsets of sampled points to a\n",
            "latent space and thus decouples the sampled points and the corresponding local\n",
            "geometric relationship. Once the latent space is determined and that the deep\n",
            "model is optimized, the recovery process could be conducted by passing the\n",
            "recover-pleasing sampled points and a randomly-drawn embedding to the same\n",
            "network through an invertible operation. Such a scheme could guarantee the\n",
            "fidelity of dense point recovery from sampled points. Extensive experiments\n",
            "demonstrate that the proposed PointLIE outperforms state-of-the-arts both\n",
            "quantitatively and qualitatively. Our code is released through\n",
            "https://github.com/zwb0/PointLIE.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: In this work, we present a novel approach for simultaneous knowledge transfer\n",
            "and model compression called Weight Squeezing. With this method, we perform\n",
            "knowledge transfer from a teacher model by learning the mapping from its\n",
            "weights to smaller student model weights.\n",
            "  We applied Weight Squeezing to a pre-trained text classification model based\n",
            "on BERT-Medium model and compared our method to various other knowledge\n",
            "transfer and model compression methods on GLUE multitask benchmark. We observed\n",
            "that our approach produces better results while being significantly faster than\n",
            "other methods for training student models.\n",
            "  We also proposed a variant of Weight Squeezing called Gated Weight Squeezing,\n",
            "for which we combined fine-tuning of BERT-Medium model and learning mapping\n",
            "from BERT-Base weights. We showed that fine-tuning with Gated Weight Squeezing\n",
            "outperforms plain fine-tuning of BERT-Medium model as well as other concurrent\n",
            "SoTA approaches while much being easier to implement.\n",
            "Label(s): [' ', '.', 'C', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: This paper introduces a new benchmark for large-scale image similarity\n",
            "detection. This benchmark is used for the Image Similarity Challenge at\n",
            "NeurIPS'21 (ISC2021). The goal is to determine whether a query image is a\n",
            "modified copy of any image in a reference corpus of size 1~million. The\n",
            "benchmark features a variety of image transformations such as automated\n",
            "transformations, hand-crafted image edits and machine-learning based\n",
            "manipulations. This mimics real-life cases appearing in social media, for\n",
            "example for integrity-related problems dealing with misinformation and\n",
            "objectionable content. The strength of the image manipulations, and therefore\n",
            "the difficulty of the benchmark, is calibrated according to the performance of\n",
            "a set of baseline approaches. Both the query and reference set contain a\n",
            "majority of \"distractor\" images that do not match, which corresponds to a\n",
            "real-life needle-in-haystack setting, and the evaluation metric reflects that.\n",
            "We expect the DISC21 benchmark to promote image copy detection as an important\n",
            "and challenging computer vision task and refresh the state of the art.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Lane marker extraction is a basic yet necessary task for autonomous driving.\n",
            "Although past years have witnessed major advances in lane marker extraction\n",
            "with deep learning models, they all aim at ordinary RGB images generated by\n",
            "frame-based cameras, which limits their performance in extreme cases, like huge\n",
            "illumination change. To tackle this problem, we introduce Dynamic Vision Sensor\n",
            "(DVS), a type of event-based sensor to lane marker extraction task and build a\n",
            "high-resolution DVS dataset for lane marker extraction. We collect the raw\n",
            "event data and generate 5,424 DVS images with a resolution of 1280$\\times$800\n",
            "pixels, the highest one among all DVS datasets available now. All images are\n",
            "annotated with multi-class semantic segmentation format. We then propose a\n",
            "structure-aware network for lane marker extraction in DVS images. It can\n",
            "capture directional information comprehensively with multidirectional slice\n",
            "convolution. We evaluate our proposed network with other state-of-the-art lane\n",
            "marker extraction models on this dataset. Experimental results demonstrate that\n",
            "our method outperforms other competitors. The dataset is made publicly\n",
            "available, including the raw event data, accumulated images and labels.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: In this paper, we propose a multi-generator extension to the adversarial\n",
            "training framework, in which the objective of each generator is to represent a\n",
            "unique component of a target mixture distribution. In the training phase, the\n",
            "generators cooperate to represent, as a mixture, the target distribution while\n",
            "maintaining distinct manifolds. As opposed to traditional generative models,\n",
            "inference from a particular generator after training resembles selective\n",
            "sampling from a unique component in the target distribution. We demonstrate the\n",
            "feasibility of the proposed architecture both analytically and with basic\n",
            "Multi-Layer Perceptron (MLP) models trained on the MNIST dataset.\n",
            "Label(s): [' ', '.', 'G', 'L', 'c', 's']\n",
            " \n",
            "Abstract: Autoencoders are among the earliest introduced nonlinear models for\n",
            "unsupervised learning. Although they are widely adopted beyond research, it has\n",
            "been a longstanding open problem to understand mathematically the feature\n",
            "extraction mechanism that trained nonlinear autoencoders provide.\n",
            "  In this work, we make progress in this problem by analyzing a class of\n",
            "two-layer weight-tied nonlinear autoencoders in the mean field framework. Upon\n",
            "a suitable scaling, in the regime of a large number of neurons, the models\n",
            "trained with stochastic gradient descent are shown to admit a mean field\n",
            "limiting dynamics. This limiting description reveals an asymptotically precise\n",
            "picture of feature learning by these models: their training dynamics exhibit\n",
            "different phases that correspond to the learning of different principal\n",
            "subspaces of the data, with varying degrees of nonlinear shrinkage dependent on\n",
            "the $\\ell_{2}$-regularization and stopping time. While we prove these results\n",
            "under an idealized assumption of (correlated) Gaussian data, experiments on\n",
            "real-life data demonstrate an interesting match with the theory.\n",
            "  The autoencoder setup of interests poses a nontrivial mathematical challenge\n",
            "to proving these results. In this setup, the \"Lipschitz\" constants of the\n",
            "models grow with the data dimension $d$. Consequently an adaptation of previous\n",
            "analyses requires a number of neurons $N$ that is at least exponential in $d$.\n",
            "Our main technical contribution is a new argument which proves that the\n",
            "required $N$ is only polynomial in $d$. We conjecture that $N\\gg d$ is\n",
            "sufficient and that $N$ is necessarily larger than a data-dependent intrinsic\n",
            "dimension, a behavior that is fundamentally different from previously studied\n",
            "setups.\n",
            "Label(s): [' ', '-', '.', 'G', 'H', 'L', 'M', 'S', 'T', 'a', 'c', 'd', 'h', 'i', 'm', 'n', 'o', 's', 't']\n",
            " \n",
            "Abstract: We explore the zero-shot setting for day-night domain adaptation. The\n",
            "traditional domain adaptation setting is to train on one domain and adapt to\n",
            "the target domain by exploiting unlabeled data samples from the test set. As\n",
            "gathering relevant test data is expensive and sometimes even impossible, we\n",
            "remove any reliance on test data imagery and instead exploit a visual inductive\n",
            "prior derived from physics-based reflection models for domain adaptation. We\n",
            "cast a number of color invariant edge detectors as trainable layers in a\n",
            "convolutional neural network and evaluate their robustness to illumination\n",
            "changes. We show that the color invariant layer reduces the day-night\n",
            "distribution shift in feature map activations throughout the network. We\n",
            "demonstrate improved performance for zero-shot day to night domain adaptation\n",
            "on both synthetic as well as natural datasets in various tasks, including\n",
            "classification, segmentation and place recognition.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Face detection methods have relied on face datasets for training. However,\n",
            "existing face datasets tend to be in small scales for face learning in both\n",
            "constrained and unconstrained environments. In this paper, we first introduce\n",
            "our large-scale image datasets, Large-scale Labeled Face (LSLF) and noisy\n",
            "Large-scale Labeled Non-face (LSLNF). Our LSLF dataset consists of a large\n",
            "number of unconstrained multi-view and partially occluded faces. The faces have\n",
            "many variations in color and grayscale, image quality, image resolution, image\n",
            "illumination, image background, image illusion, human face, cartoon face,\n",
            "facial expression, light and severe partial facial occlusion, make up, gender,\n",
            "age, and race. Many of these faces are partially occluded with accessories such\n",
            "as tattoos, hats, glasses, sunglasses, hands, hair, beards, scarves,\n",
            "microphones, or other objects or persons. The LSLF dataset is currently the\n",
            "largest labeled face image dataset in the literature in terms of the number of\n",
            "labeled images and the number of individuals compared to other existing labeled\n",
            "face image datasets. Second, we introduce our CrowedFaces and CrowedNonFaces\n",
            "image datasets. The crowedFaces and CrowedNonFaces datasets include faces and\n",
            "non-faces images from crowed scenes. These datasets essentially aim for\n",
            "researchers to provide a large number of training examples with many variations\n",
            "for large scale face learning and face recognition tasks.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Driven by the urgent demand for managing remote sensing big data, large-scale\n",
            "remote sensing image retrieval (RSIR) attracts increasing attention in the\n",
            "remote sensing field. In general, existing retrieval methods can be regarded as\n",
            "visual-based retrieval approaches which search and return a set of similar\n",
            "images from a database to a given query image. Although retrieval methods have\n",
            "achieved great success, there is still a question that needs to be responded\n",
            "to: Can we obtain the accurate semantic labels of the returned similar images\n",
            "to further help analyzing and processing imagery? Inspired by the above\n",
            "question, in this paper, we redefine the image retrieval problem as visual and\n",
            "semantic retrieval of images. Specifically, we propose a novel deep hashing\n",
            "convolutional neural network (DHCNN) to simultaneously retrieve the similar\n",
            "images and classify their semantic labels in a unified framework. In more\n",
            "detail, a convolutional neural network (CNN) is used to extract\n",
            "high-dimensional deep features. Then, a hash layer is perfectly inserted into\n",
            "the network to transfer the deep features into compact hash codes. In addition,\n",
            "a fully connected layer with a softmax function is performed on hash layer to\n",
            "generate class distribution. Finally, a loss function is elaborately designed\n",
            "to simultaneously consider the label loss of each image and similarity loss of\n",
            "pairs of images. Experimental results on two remote sensing datasets\n",
            "demonstrate that the proposed method achieves the state-of-art retrieval and\n",
            "classification performance.\n",
            "Label(s): [' ', '.', 'C', 'I', 'V', 'c', 'e', 's']\n",
            " \n",
            "Abstract: We propose a distributed deep learning model to successfully learn control\n",
            "policies directly from high-dimensional sensory input using reinforcement\n",
            "learning. The model is based on the deep Q-network, a convolutional neural\n",
            "network trained with a variant of Q-learning. Its input is raw pixels and its\n",
            "output is a value function estimating future rewards from taking an action\n",
            "given a system state. To distribute the deep Q-network training, we adapt the\n",
            "DistBelief software framework to the context of efficiently training\n",
            "reinforcement learning agents. As a result, the method is completely\n",
            "asynchronous and scales well with the number of machines. We demonstrate that\n",
            "the deep Q-network agent, receiving only the pixels and the game score as\n",
            "inputs, was able to achieve reasonable success on a simple game with minimal\n",
            "parameter tuning.\n",
            "Label(s): [' ', '.', 'A', 'C', 'D', 'E', 'G', 'I', 'L', 'N', 'c', 's']\n",
            " \n",
            "Abstract: Within the last decade, neural network based predictors have demonstrated\n",
            "impressive - and at times super-human - capabilities. This performance is often\n",
            "paid for with an intransparent prediction process and thus has sparked numerous\n",
            "contributions in the novel field of explainable artificial intelligence (XAI).\n",
            "In this paper, we focus on a popular and widely used method of XAI, the\n",
            "Layer-wise Relevance Propagation (LRP). Since its initial proposition LRP has\n",
            "evolved as a method, and a best practice for applying the method has tacitly\n",
            "emerged, based however on humanly observed evidence alone. In this paper we\n",
            "investigate - and for the first time quantify - the effect of this current best\n",
            "practice on feedforward neural networks in a visual object detection setting.\n",
            "The results verify that the layer-dependent approach to LRP applied in recent\n",
            "literature better represents the model's reasoning, and at the same time\n",
            "increases the object localization and class discriminativity of LRP.\n",
            "Label(s): [' ', '.', 'C', 'G', 'L', 'M', 'V', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: In this paper, we present a novel approach to perform deep neural networks\n",
            "layer-wise weight initialization using Linear Discriminant Analysis (LDA).\n",
            "Typically, the weights of a deep neural network are initialized with: random\n",
            "values, greedy layer-wise pre-training (usually as Deep Belief Network or as\n",
            "auto-encoder) or by re-using the layers from another network (transfer\n",
            "learning). Hence, many training epochs are needed before meaningful weights are\n",
            "learned, or a rather similar dataset is required for seeding a fine-tuning of\n",
            "transfer learning. In this paper, we describe how to turn an LDA into either a\n",
            "neural layer or a classification layer. We analyze the initialization technique\n",
            "on historical documents. First, we show that an LDA-based initialization is\n",
            "quick and leads to a very stable initialization. Furthermore, for the task of\n",
            "layout analysis at pixel level, we investigate the effectiveness of LDA-based\n",
            "initialization and show that it outperforms state-of-the-art random weight\n",
            "initialization methods.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Medical image segmentation is inherently an ambiguous task due to factors\n",
            "such as partial volumes and variations in anatomical definitions. While in most\n",
            "cases the segmentation uncertainty is around the border of structures of\n",
            "interest, there can also be considerable inter-rater differences. The class of\n",
            "conditional variational autoencoders (cVAE) offers a principled approach to\n",
            "inferring distributions over plausible segmentations that are conditioned on\n",
            "input images. Segmentation uncertainty estimated from samples of such\n",
            "distributions can be more informative than using pixel level probability\n",
            "scores. In this work, we propose a novel conditional generative model that is\n",
            "based on conditional Normalizing Flow (cFlow). The basic idea is to increase\n",
            "the expressivity of the cVAE by introducing a cFlow transformation step after\n",
            "the encoder. This yields improved approximations of the latent posterior\n",
            "distribution, allowing the model to capture richer segmentation variations.\n",
            "With this we show that the quality and diversity of samples obtained from our\n",
            "conditional generative model is enhanced. Performance of our model, which we\n",
            "call cFlow Net, is evaluated on two medical imaging datasets demonstrating\n",
            "substantial improvements in both qualitative and quantitative measures when\n",
            "compared to a recent cVAE based model.\n",
            "Label(s): [' ', '.', 'C', 'G', 'L', 'M', 'V', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Current dynamic networks and dynamic pruning methods have shown their\n",
            "promising capability in reducing theoretical computation complexity. However,\n",
            "dynamic sparse patterns on convolutional filters fail to achieve actual\n",
            "acceleration in real-world implementation, due to the extra burden of indexing,\n",
            "weight-copying, or zero-masking. Here, we explore a dynamic network slimming\n",
            "regime, named Dynamic Slimmable Network (DS-Net), which aims to achieve good\n",
            "hardware-efficiency via dynamically adjusting filter numbers of networks at\n",
            "test time with respect to different inputs, while keeping filters stored\n",
            "statically and contiguously in hardware to prevent the extra burden. Our DS-Net\n",
            "is empowered with the ability of dynamic inference by the proposed\n",
            "double-headed dynamic gate that comprises an attention head and a slimming head\n",
            "to predictively adjust network width with negligible extra computation cost. To\n",
            "ensure generality of each candidate architecture and the fairness of gate, we\n",
            "propose a disentangled two-stage training scheme inspired by one-shot NAS. In\n",
            "the first stage, a novel training technique for weight-sharing networks named\n",
            "In-place Ensemble Bootstrapping is proposed to improve the supernet training\n",
            "efficacy. In the second stage, Sandwich Gate Sparsification is proposed to\n",
            "assist the gate training by identifying easy and hard samples in an online way.\n",
            "Extensive experiments demonstrate our DS-Net consistently outperforms its\n",
            "static counterparts as well as state-of-the-art static and dynamic model\n",
            "compression methods by a large margin (up to 5.9%). Typically, DS-Net achieves\n",
            "2-4x computation reduction and 1.62x real-world acceleration over ResNet-50 and\n",
            "MobileNet with minimal accuracy drops on ImageNet. Code release:\n",
            "https://github.com/changlin31/DS-Net .\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Learning sensorimotor control policies from high-dimensional images crucially\n",
            "relies on the quality of the underlying visual representations. Prior works\n",
            "show that structured latent space such as visual keypoints often outperforms\n",
            "unstructured representations for robotic control. However, most of these\n",
            "representations, whether structured or unstructured are learned in a 2D space\n",
            "even though the control tasks are usually performed in a 3D environment. In\n",
            "this work, we propose a framework to learn such a 3D geometric structure\n",
            "directly from images in an end-to-end unsupervised manner. The input images are\n",
            "embedded into latent 3D keypoints via a differentiable encoder which is trained\n",
            "to optimize both a multi-view consistency loss and downstream task objective.\n",
            "These discovered 3D keypoints tend to meaningfully capture robot joints as well\n",
            "as object movements in a consistent manner across both time and 3D space. The\n",
            "proposed approach outperforms prior state-of-art methods across a variety of\n",
            "reinforcement learning benchmarks. Code and videos at\n",
            "https://buoyancy99.github.io/unsup-3d-keypoints/\n",
            "Label(s): [' ', '.', 'C', 'G', 'L', 'O', 'R', 'V', 'c', 's']\n",
            " \n",
            "Abstract: In this paper, we propose a model-driven method that reconstructs LoD-2\n",
            "building models following a \"decomposition-optimization-fitting\" paradigm. The\n",
            "proposed method starts building detection results through a deep learning-based\n",
            "detector and vectorizes individual segments into polygons using a \"three-step\"\n",
            "polygon extraction method, followed by a novel grid-based decomposition method\n",
            "that decomposes the complex and irregularly shaped building polygons to tightly\n",
            "combined elementary building rectangles ready to fit elementary building\n",
            "models. We have optionally introduced OpenStreetMap (OSM) and Graph-Cut (GC)\n",
            "labeling to further refine the orientation of 2D building rectangle. The 3D\n",
            "modeling step takes building-specific parameters such as hip lines, as well as\n",
            "non-rigid and regularized transformations to optimize the flexibility for using\n",
            "a minimal set of elementary models. Finally, roof type of building models s\n",
            "refined and adjacent building models in one building segment are merged into\n",
            "the complex polygonal model. Our proposed method has addressed a few technical\n",
            "caveats over existing methods, resulting in practically high-quality results,\n",
            "based on our evaluation and comparative study on a diverse set of experimental\n",
            "datasets of cities with different urban patterns.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Applications based on Deep Neural Networks (DNNs) have grown exponentially in\n",
            "the past decade. To match their increasing computational needs, several\n",
            "Non-Volatile Memory (NVM) crossbar-based accelerators have been proposed. Apart\n",
            "from improved energy efficiency and performance, these approximate hardware\n",
            "also possess intrinsic robustness for defense against Adversarial Attacks,\n",
            "which is an important security concern for DNNs. Prior works have focused on\n",
            "quantifying this intrinsic robustness for vanilla networks, that is DNNs\n",
            "trained on unperturbed inputs. However, adversarial training of DNNs is the\n",
            "benchmark technique for robustness, and sole reliance on intrinsic robustness\n",
            "of the hardware may not be sufficient. In this work, we explore the design of\n",
            "robust DNNs through the amalgamation of adversarial training and the intrinsic\n",
            "robustness offered by NVM crossbar-based analog hardware. First, we study the\n",
            "noise stability of such networks on unperturbed inputs and observe that\n",
            "internal activations of adversarially trained networks have lower\n",
            "Signal-to-Noise Ratio (SNR), and are sensitive to noise than vanilla networks.\n",
            "As a result, they suffer significantly higher performance degradation due to\n",
            "the non-ideal computations; on an average 2x accuracy drop. On the other hand,\n",
            "for adversarial images generated using Projected-Gradient-Descent (PGD)\n",
            "White-Box attacks, ResNet-10/20 adversarially trained on CIFAR-10/100 display a\n",
            "5-10% gain in robust accuracy due to the underlying NVM crossbar when the\n",
            "attack epsilon ($\\epsilon_{attack}$, the degree of input perturbations) is\n",
            "greater than the epsilon of the adversarial training ($\\epsilon_{train}$). Our\n",
            "results indicate that implementing adversarially trained networks on analog\n",
            "hardware requires careful calibration between hardware non-idealities and\n",
            "$\\epsilon_{train}$ to achieve optimum robustness and performance.\n",
            "Label(s): [' ', '.', 'G', 'L', 'c', 's']\n",
            " \n",
            "Abstract: Convolutional neural networks (CNNs) have been widely used over many areas in\n",
            "compute vision. Especially in classification. Recently, FlowNet and several\n",
            "works on opti- cal estimation using CNNs shows the potential ability of CNNs in\n",
            "doing per-pixel regression. We proposed several CNNs network architectures that\n",
            "can estimate optical flow, and fully unveiled the intrinsic different between\n",
            "these structures.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Neural painting refers to the procedure of producing a series of strokes for\n",
            "a given image and non-photo-realistically recreating it using neural networks.\n",
            "While reinforcement learning (RL) based agents can generate a stroke sequence\n",
            "step by step for this task, it is not easy to train a stable RL agent. On the\n",
            "other hand, stroke optimization methods search for a set of stroke parameters\n",
            "iteratively in a large search space; such low efficiency significantly limits\n",
            "their prevalence and practicality. Different from previous methods, in this\n",
            "paper, we formulate the task as a set prediction problem and propose a novel\n",
            "Transformer-based framework, dubbed Paint Transformer, to predict the\n",
            "parameters of a stroke set with a feed forward network. This way, our model can\n",
            "generate a set of strokes in parallel and obtain the final painting of size 512\n",
            "* 512 in near real time. More importantly, since there is no dataset available\n",
            "for training the Paint Transformer, we devise a self-training pipeline such\n",
            "that it can be trained without any off-the-shelf dataset while still achieving\n",
            "excellent generalization capability. Experiments demonstrate that our method\n",
            "achieves better painting performance than previous ones with cheaper training\n",
            "and inference costs. Codes and models are available.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: In this paper we present a system for the detection of immunogold particles\n",
            "and a Transfer Learning (TL) framework for the recognition of these immunogold\n",
            "particles. Immunogold particles are part of a high-magnification method for the\n",
            "selective localization of biological molecules at the subcellular level only\n",
            "visible through Electron Microscopy. The number of immunogold particles in the\n",
            "cell walls allows the assessment of the differences in their compositions\n",
            "providing a tool to analise the quality of different plants. For its\n",
            "quantization one requires a laborious manual labeling (or annotation) of images\n",
            "containing hundreds of particles. The system that is proposed in this paper can\n",
            "leverage significantly the burden of this manual task.\n",
            "  For particle detection we use a LoG filter coupled with a SDA. In order to\n",
            "improve the recognition, we also study the applicability of TL settings for\n",
            "immunogold recognition. TL reuses the learning model of a source problem on\n",
            "other datasets (target problems) containing particles of different sizes. The\n",
            "proposed system was developed to solve a particular problem on maize cells,\n",
            "namely to determine the composition of cell wall ingrowths in endosperm\n",
            "transfer cells. This novel dataset as well as the code for reproducing our\n",
            "experiments is made publicly available.\n",
            "  We determined that the LoG detector alone attained more than 84\\% of accuracy\n",
            "with the F-measure. Developing immunogold recognition with TL also provided\n",
            "superior performance when compared with the baseline models augmenting the\n",
            "accuracy rates by 10\\%.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Sleep disorder diagnosis relies on the analysis of polysomnography (PSG)\n",
            "records. As a preliminary step of this examination, sleep stages are\n",
            "systematically determined. In practice, sleep stage classification relies on\n",
            "the visual inspection of 30-second epochs of polysomnography signals. Numerous\n",
            "automatic approaches have been developed to replace this tedious and expensive\n",
            "task. Although these methods demonstrated better performance than human sleep\n",
            "experts on specific datasets, they remain largely unused in sleep clinics. The\n",
            "main reason is that each sleep clinic uses a specific PSG montage that most\n",
            "automatic approaches cannot handle out-of-the-box. Moreover, even when the PSG\n",
            "montage is compatible, publications have shown that automatic approaches\n",
            "perform poorly on unseen data with different demographics. To address these\n",
            "issues, we introduce RobustSleepNet, a deep learning model for automatic sleep\n",
            "stage classification able to handle arbitrary PSG montages. We trained and\n",
            "evaluated this model in a leave-one-out-dataset fashion on a large corpus of 8\n",
            "heterogeneous sleep staging datasets to make it robust to demographic changes.\n",
            "When evaluated on an unseen dataset, RobustSleepNet reaches 97% of the F1 of a\n",
            "model explicitly trained on this dataset. Hence, RobustSleepNet unlocks the\n",
            "possibility to perform high-quality out-of-the-box automatic sleep staging with\n",
            "any clinical setup. We further show that finetuning RobustSleepNet, using a\n",
            "part of the unseen dataset, increases the F1 by 2% when compared to a model\n",
            "trained specifically for this dataset. Therefore, finetuning might be used to\n",
            "reach a state-of-the-art level of performance on a specific population.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'P', 'S', 'a', 'c', 'e', 's', 't']\n",
            " \n",
            "Abstract: Intrinsic rewards are commonly applied to improve exploration in\n",
            "reinforcement learning. However, these approaches suffer from instability\n",
            "caused by non-stationary reward shaping and strong dependency on\n",
            "hyperparameters. In this work, we propose Decoupled RL (DeRL) which trains\n",
            "separate policies for exploration and exploitation. DeRL can be applied with\n",
            "on-policy and off-policy RL algorithms. We evaluate DeRL algorithms in two\n",
            "sparse-reward environments with multiple types of intrinsic rewards. We show\n",
            "that DeRL is more robust to scaling and speed of decay of intrinsic rewards and\n",
            "converges to the same evaluation returns than intrinsically motivated baselines\n",
            "in fewer interactions.\n",
            "Label(s): [' ', '.', 'A', 'G', 'I', 'L', 'c', 's']\n",
            " \n",
            "Abstract: The need for accurate yield estimates for viticulture is becoming more\n",
            "important due to increasing competition in the wine market worldwide. One of\n",
            "the most promising methods to estimate the harvest is berry counting, as it can\n",
            "be approached non-destructively, and its process can be automated. In this\n",
            "article, we present a method that addresses the challenge of occluded berries\n",
            "with leaves to obtain a more accurate estimate of the number of berries that\n",
            "will enable a better estimate of the harvest. We use generative adversarial\n",
            "networks, a deep learning-based approach that generates a likely scenario\n",
            "behind the leaves exploiting learned patterns from images with non-occluded\n",
            "berries. Our experiments show that the estimate of the number of berries after\n",
            "applying our method is closer to the manually counted reference. In contrast to\n",
            "applying a factor to the berry count, our approach better adapts to local\n",
            "conditions by directly involving the appearance of the visible berries.\n",
            "Furthermore, we show that our approach can identify which areas in the image\n",
            "should be changed by adding new berries without explicitly requiring\n",
            "information about hidden areas.\n",
            "Label(s): [' ', '.', 'C', 'E', 'G', 'L', 'N', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Policy gradients methods often achieve better performance when the change in\n",
            "policy is limited to a small Kullback-Leibler divergence. We derive policy\n",
            "gradients where the change in policy is limited to a small Wasserstein distance\n",
            "(or trust region). This is done in the discrete and continuous multi-armed\n",
            "bandit settings with entropy regularisation. We show that in the small steps\n",
            "limit with respect to the Wasserstein distance $W_2$, policy dynamics are\n",
            "governed by the Fokker-Planck (heat) equation, following the\n",
            "Jordan-Kinderlehrer-Otto result. This means that policies undergo diffusion and\n",
            "advection, concentrating near actions with high reward. This helps elucidate\n",
            "the nature of convergence in the probability matching setup, and provides\n",
            "justification for empirical practices such as Gaussian policy priors and\n",
            "additive gradient noise.\n",
            "Label(s): [' ', '.', 'G', 'L', 'c', 's']\n",
            " \n",
            "Abstract: Full-sampling (e.g., Q-learning) and pure-expectation (e.g., Expected Sarsa)\n",
            "algorithms are efficient and frequently used techniques in reinforcement\n",
            "learning. Q$(\\sigma,\\lambda)$ is the first approach unifies them with\n",
            "eligibility trace through the sampling degree $\\sigma$. However, it is limited\n",
            "to the tabular case, for large-scale learning, the Q$(\\sigma,\\lambda)$ is too\n",
            "expensive to require a huge volume of tables to accurately storage value\n",
            "functions. To address above problem, we propose a GQ$(\\sigma,\\lambda)$ that\n",
            "extends tabular Q$(\\sigma,\\lambda)$ with linear function approximation. We\n",
            "prove the convergence of GQ$(\\sigma,\\lambda)$. Empirical results on some\n",
            "standard domains show that GQ$(\\sigma,\\lambda)$ with a combination of\n",
            "full-sampling with pure-expectation reach a better performance than\n",
            "full-sampling and pure-expectation methods.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Detecting small objects is notoriously challenging due to their low\n",
            "resolution and noisy representation. Existing object detection pipelines\n",
            "usually detect small objects through learning representations of all the\n",
            "objects at multiple scales. However, the performance gain of such ad hoc\n",
            "architectures is usually limited to pay off the computational cost. In this\n",
            "work, we address the small object detection problem by developing a single\n",
            "architecture that internally lifts representations of small objects to\n",
            "\"super-resolved\" ones, achieving similar characteristics as large objects and\n",
            "thus more discriminative for detection. For this purpose, we propose a new\n",
            "Perceptual Generative Adversarial Network (Perceptual GAN) model that improves\n",
            "small object detection through narrowing representation difference of small\n",
            "objects from the large ones. Specifically, its generator learns to transfer\n",
            "perceived poor representations of the small objects to super-resolved ones that\n",
            "are similar enough to real large objects to fool a competing discriminator.\n",
            "Meanwhile its discriminator competes with the generator to identify the\n",
            "generated representation and imposes an additional perceptual requirement -\n",
            "generated representations of small objects must be beneficial for detection\n",
            "purpose - on the generator. Extensive evaluations on the challenging\n",
            "Tsinghua-Tencent 100K and the Caltech benchmark well demonstrate the\n",
            "superiority of Perceptual GAN in detecting small objects, including traffic\n",
            "signs and pedestrians, over well-established state-of-the-arts.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Robust point cloud registration in real-time is an important prerequisite for\n",
            "many mapping and localization algorithms. Traditional methods like ICP tend to\n",
            "fail without good initialization, insufficient overlap or in the presence of\n",
            "dynamic objects. Modern deep learning based registration approaches present\n",
            "much better results, but suffer from a heavy run-time. We overcome these\n",
            "drawbacks by introducing StickyPillars, a fast, accurate and extremely robust\n",
            "deep middle-end 3D feature matching method on point clouds. It uses graph\n",
            "neural networks and performs context aggregation on sparse 3D key-points with\n",
            "the aid of transformer based multi-head self and cross-attention. The network\n",
            "output is used as the cost for an optimal transport problem whose solution\n",
            "yields the final matching probabilities. The system does not rely on hand\n",
            "crafted feature descriptors or heuristic matching strategies. We present\n",
            "state-of-art art accuracy results on the registration problem demonstrated on\n",
            "the KITTI dataset while being four times faster then leading deep methods.\n",
            "Furthermore, we integrate our matching system into a LiDAR odometry pipeline\n",
            "yielding most accurate results on the KITTI odometry dataset. Finally, we\n",
            "demonstrate robustness on KITTI odometry. Our method remains stable in accuracy\n",
            "where state-of-the-art procedures fail on frame drops and higher speeds.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: To accelerate the training of machine learning models, distributed stochastic\n",
            "gradient descent (SGD) and its variants have been widely adopted, which apply\n",
            "multiple workers in parallel to speed up training. Among them, Local SGD has\n",
            "gained much attention due to its lower communication cost. Nevertheless, when\n",
            "the data distribution on workers is non-identical, Local SGD requires\n",
            "$O(T^{\\frac{3}{4}} N^{\\frac{3}{4}})$ communications to maintain its\n",
            "\\emph{linear iteration speedup} property, where $T$ is the total number of\n",
            "iterations and $N$ is the number of workers. In this paper, we propose Variance\n",
            "Reduced Local SGD (VRL-SGD) to further reduce the communication complexity.\n",
            "Benefiting from eliminating the dependency on the gradient variance among\n",
            "workers, we theoretically prove that VRL-SGD achieves a \\emph{linear iteration\n",
            "speedup} with a lower communication complexity $O(T^{\\frac{1}{2}}\n",
            "N^{\\frac{3}{2}})$ even if workers access non-identical datasets. We conduct\n",
            "experiments on three machine learning tasks, and the experimental results\n",
            "demonstrate that VRL-SGD performs impressively better than Local SGD when the\n",
            "data among workers are quite diverse.\n",
            "Label(s): [' ', '.', 'C', 'D', 'G', 'L', 'M', 'O', 'a', 'c', 'h', 'm', 's', 't']\n",
            " \n",
            "Abstract: We present KAMA, a 3D Keypoint Aware Mesh Articulation approach that allows\n",
            "us to estimate a human body mesh from the positions of 3D body keypoints. To\n",
            "this end, we learn to estimate 3D positions of 26 body keypoints and propose an\n",
            "analytical solution to articulate a parametric body model, SMPL, via a set of\n",
            "straightforward geometric transformations. Since keypoint estimation directly\n",
            "relies on image clues, our approach offers significantly better alignment to\n",
            "image content when compared to state-of-the-art approaches. Our proposed\n",
            "approach does not require any paired mesh annotations and is able to achieve\n",
            "state-of-the-art mesh fittings through 3D keypoint regression only. Results on\n",
            "the challenging 3DPW and Human3.6M demonstrate that our approach yields\n",
            "state-of-the-art body mesh fittings.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: In object detection, keypoint-based approaches often suffer a large number of\n",
            "incorrect object bounding boxes, arguably due to the lack of an additional look\n",
            "into the cropped regions. This paper presents an efficient solution which\n",
            "explores the visual patterns within each cropped region with minimal costs. We\n",
            "build our framework upon a representative one-stage keypoint-based detector\n",
            "named CornerNet. Our approach, named CenterNet, detects each object as a\n",
            "triplet, rather than a pair, of keypoints, which improves both precision and\n",
            "recall. Accordingly, we design two customized modules named cascade corner\n",
            "pooling and center pooling, which play the roles of enriching information\n",
            "collected by both top-left and bottom-right corners and providing more\n",
            "recognizable information at the central regions, respectively. On the MS-COCO\n",
            "dataset, CenterNet achieves an AP of 47.0%, which outperforms all existing\n",
            "one-stage detectors by at least 4.9%. Meanwhile, with a faster inference speed,\n",
            "CenterNet demonstrates quite comparable performance to the top-ranked two-stage\n",
            "detectors. Code is available at https://github.com/Duankaiwen/CenterNet.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Attribution methods have been developed to explain the decision of a machine\n",
            "learning model on a given input. We use the Integrated Gradient method for\n",
            "finding attributions to define the causal neighborhood of an input by\n",
            "incrementally masking high attribution features. We study the robustness of\n",
            "machine learning models on benign and adversarial inputs in this neighborhood.\n",
            "Our study indicates that benign inputs are robust to the masking of high\n",
            "attribution features but adversarial inputs generated by the state-of-the-art\n",
            "adversarial attack methods such as DeepFool, FGSM, CW and PGD, are not robust\n",
            "to such masking. Further, our study demonstrates that this concentration of\n",
            "high-attribution features responsible for the incorrect decision is more\n",
            "pronounced in physically realizable adversarial examples. This difference in\n",
            "attribution of benign and adversarial inputs can be used to detect adversarial\n",
            "examples. Such a defense approach is independent of training data and attack\n",
            "method, and we demonstrate its effectiveness on digital and physically\n",
            "realizable perturbations.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Although well-established in general reinforcement learning (RL), value-based\n",
            "methods are rarely explored in constrained RL (CRL) for their incapability of\n",
            "finding policies that can randomize among multiple actions. To apply\n",
            "value-based methods to CRL, a recent groundbreaking line of game-theoretic\n",
            "approaches uses the mixed policy that randomizes among a set of carefully\n",
            "generated policies to converge to the desired constraint-satisfying policy.\n",
            "However, these approaches require storing a large set of policies, which is not\n",
            "policy efficient, and may incur prohibitive memory costs in constrained deep\n",
            "RL. To address this problem, we propose an alternative approach. Our approach\n",
            "first reformulates the CRL to an equivalent distance optimization problem. With\n",
            "a specially designed linear optimization oracle, we derive a meta-algorithm\n",
            "that solves it using any off-the-shelf RL algorithm and any conditional\n",
            "gradient (CG) type algorithm as subroutines. We then propose a new variant of\n",
            "the CG-type algorithm, which generalizes the minimum norm point (MNP) method.\n",
            "The proposed method matches the convergence rate of the existing game-theoretic\n",
            "approaches and achieves the worst-case optimal policy efficiency. The\n",
            "experiments on a navigation task show that our method reduces the memory costs\n",
            "by an order of magnitude, and meanwhile achieves better performance,\n",
            "demonstrating both its effectiveness and efficiency.\n",
            "Label(s): [' ', '.', 'G', 'L', 'c', 's']\n",
            " \n",
            "Abstract: The goal of imitation learning (IL) is to learn a good policy from\n",
            "high-quality demonstrations. However, the quality of demonstrations in reality\n",
            "can be diverse, since it is easier and cheaper to collect demonstrations from a\n",
            "mix of experts and amateurs. IL in such situations can be challenging,\n",
            "especially when the level of demonstrators' expertise is unknown. We propose a\n",
            "new IL method called \\underline{v}ariational \\underline{i}mitation\n",
            "\\underline{l}earning with \\underline{d}iverse-quality demonstrations (VILD),\n",
            "where we explicitly model the level of demonstrators' expertise with a\n",
            "probabilistic graphical model and estimate it along with a reward function. We\n",
            "show that a naive approach to estimation is not suitable to large state and\n",
            "action spaces, and fix its issues by using a variational approach which can be\n",
            "easily implemented using existing reinforcement learning methods. Experiments\n",
            "on continuous-control benchmarks demonstrate that VILD outperforms\n",
            "state-of-the-art methods. Our work enables scalable and data-efficient IL under\n",
            "more realistic settings than before.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Anatomical landmark correspondences in medical images can provide additional\n",
            "guidance information for the alignment of two images, which, in turn, is\n",
            "crucial for many medical applications. However, manual landmark annotation is\n",
            "labor-intensive. Therefore, we propose an end-to-end deep learning approach to\n",
            "automatically detect landmark correspondences in pairs of two-dimensional (2D)\n",
            "images. Our approach consists of a Siamese neural network, which is trained to\n",
            "identify salient locations in images as landmarks and predict matching\n",
            "probabilities for landmark pairs from two different images. We trained our\n",
            "approach on 2D transverse slices from 168 lower abdominal Computed Tomography\n",
            "(CT) scans. We tested the approach on 22,206 pairs of 2D slices with varying\n",
            "levels of intensity, affine, and elastic transformations. The proposed approach\n",
            "finds an average of 639, 466, and 370 landmark matches per image pair for\n",
            "intensity, affine, and elastic transformations, respectively, with spatial\n",
            "matching errors of at most 1 mm. Further, more than 99% of the landmark pairs\n",
            "are within a spatial matching error of 2 mm, 4 mm, and 8 mm for image pairs\n",
            "with intensity, affine, and elastic transformations, respectively. To\n",
            "investigate the utility of our developed approach in a clinical setting, we\n",
            "also tested our approach on pairs of transverse slices selected from follow-up\n",
            "CT scans of three patients. Visual inspection of the results revealed landmark\n",
            "matches in both bony anatomical regions as well as in soft tissues lacking\n",
            "prominent intensity gradients.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: This paper presents a convenient graphical tool for encoding visual patterns\n",
            "(such as image patches and image atoms) as point constellations in a space\n",
            "spanned by perceptual features and with a clear geometrical interpretation.\n",
            "General theory and a practical pattern encoding scheme are presented, inspired\n",
            "by encoding polarization states of a light wave on the Poincare sphere. This\n",
            "new pattern encoding scheme can be useful for many applications in image\n",
            "processing and computer vision. Here, three possible applications are\n",
            "illustrated, in clustering perceptually similar patterns, visualizing\n",
            "properties of learned dictionaries of image atoms and generating new\n",
            "dictionaries of image atoms from spherical codes.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Temporal localization remains an important challenge in video understanding.\n",
            "In this work, we present our solution to the 3rd YouTube-8M Video Understanding\n",
            "Challenge organized by Google Research. Participants were required to build a\n",
            "segment-level classifier using a large-scale training data set with noisy\n",
            "video-level labels and a relatively small-scale validation data set with\n",
            "accurate segment-level labels. We formulated the problem as a multiple instance\n",
            "multi-label learning and developed an attention-based mechanism to selectively\n",
            "emphasize the important frames by attention weights. The model performance is\n",
            "further improved by constructing multiple sets of attention networks. We\n",
            "further fine-tuned the model using the segment-level data set. Our final model\n",
            "consists of an ensemble of attention/multi-attention networks, deep bag of\n",
            "frames models, recurrent neural networks and convolutional neural networks. It\n",
            "ranked 13th on the private leader board and stands out for its efficient usage\n",
            "of resources.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: The goal of meta-learning is to train a model on a variety of learning tasks,\n",
            "such that it can adapt to new problems within only a few iterations. Here we\n",
            "propose a principled information-theoretic model that optimally partitions the\n",
            "underlying problem space such that specialized expert decision-makers solve the\n",
            "resulting sub-problems. To drive this specialization we impose the same kind of\n",
            "information processing constraints both on the partitioning and the expert\n",
            "decision-makers. We argue that this specialization leads to efficient\n",
            "adaptation to new tasks. To demonstrate the generality of our approach we\n",
            "evaluate three meta-learning domains: image classification, regression, and\n",
            "reinforcement learning.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Deep convolutional neural networks (CNNs) have been widely applied for\n",
            "low-level vision over the past five years. According to nature of different\n",
            "applications, designing appropriate CNN architectures is developed. However,\n",
            "customized architectures gather different features via treating all pixel\n",
            "points as equal to improve the performance of given application, which ignores\n",
            "the effects of local power pixel points and results in low training efficiency.\n",
            "In this paper, we propose an asymmetric CNN (ACNet) comprising an asymmetric\n",
            "block (AB), a memory enhancement block (MEB) and a high-frequency feature\n",
            "enhancement block (HFFEB) for image super-resolution. The AB utilizes\n",
            "one-dimensional asymmetric convolutions to intensify the square convolution\n",
            "kernels in horizontal and vertical directions for promoting the influences of\n",
            "local salient features for SISR. The MEB fuses all hierarchical low-frequency\n",
            "features from the AB via residual learning (RL) technique to resolve the\n",
            "long-term dependency problem and transforms obtained low-frequency features\n",
            "into high-frequency features. The HFFEB exploits low- and high-frequency\n",
            "features to obtain more robust super-resolution features and address excessive\n",
            "feature enhancement problem. Addditionally, it also takes charge of\n",
            "reconstructing a high-resolution (HR) image. Extensive experiments show that\n",
            "our ACNet can effectively address single image super-resolution (SISR), blind\n",
            "SISR and blind SISR of blind noise problems. The code of the ACNet is shown at\n",
            "https://github.com/hellloxiaotian/ACNet.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: We introduce a new tool for interpreting neural net responses, namely\n",
            "full-gradients, which decomposes the neural net response into input sensitivity\n",
            "and per-neuron sensitivity components. This is the first proposed\n",
            "representation which satisfies two key properties: completeness and weak\n",
            "dependence, which provably cannot be satisfied by any saliency map-based\n",
            "interpretability method. For convolutional nets, we also propose an approximate\n",
            "saliency map representation, called FullGrad, obtained by aggregating the\n",
            "full-gradient components.\n",
            "  We experimentally evaluate the usefulness of FullGrad in explaining model\n",
            "behaviour with two quantitative tests: pixel perturbation and\n",
            "remove-and-retrain. Our experiments reveal that our method explains model\n",
            "behaviour correctly, and more comprehensively than other methods in the\n",
            "literature. Visual inspection also reveals that our saliency maps are sharper\n",
            "and more tightly confined to object regions than other methods.\n",
            "Label(s): [' ', '.', 'C', 'G', 'L', 'M', 'V', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: City Logistics is characterized by multiple stakeholders that often have\n",
            "different views of such a complex system. From a public policy perspective,\n",
            "identifying stakeholders, issues and trends is a daunting challenge, only\n",
            "partially addressed by traditional observation systems. Nowadays, social media\n",
            "is one of the biggest channels of public expression and is often used to\n",
            "communicate opinions and content related to City Logistics. The idea of this\n",
            "research is that analysing social media content could help in understanding the\n",
            "public perception of City logistics. This paper offers a methodology for\n",
            "collecting content from Twitter and implementing Machine Learning techniques\n",
            "(Unsupervised Learning and Natural Language Processing), to perform content and\n",
            "sentiment analysis. The proposed methodology is applied to more than 110 000\n",
            "tweets containing City Logistics key-terms. Results allowed the building of an\n",
            "Interest Map of concepts and a Sentiment Analysis to determine if City\n",
            "Logistics entries are positive, negative or neutral.\n",
            "Label(s): [' ', '.', 'C', 'G', 'I', 'L', 'M', 'S', 'Y', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Training large-scale question answering systems is complicated because\n",
            "training sources usually cover a small portion of the range of possible\n",
            "questions. This paper studies the impact of multitask and transfer learning for\n",
            "simple question answering; a setting for which the reasoning required to answer\n",
            "is quite easy, as long as one can retrieve the correct evidence given a\n",
            "question, which can be difficult in large-scale conditions. To this end, we\n",
            "introduce a new dataset of 100k questions that we use in conjunction with\n",
            "existing benchmarks. We conduct our study within the framework of Memory\n",
            "Networks (Weston et al., 2015) because this perspective allows us to eventually\n",
            "scale up to more complex reasoning, and show that Memory Networks can be\n",
            "successfully trained to achieve excellent performance.\n",
            "Label(s): [' ', '.', 'C', 'G', 'L', 'c', 's']\n",
            " \n",
            "Abstract: Zero-shot domain adaptation (ZSDA) is a category of domain adaptation\n",
            "problems where neither data sample nor label is available for parameter\n",
            "learning in the target domain. With the hypothesis that the shift between a\n",
            "given pair of domains is shared across tasks, we propose a new method for ZSDA\n",
            "by transferring domain shift from an irrelevant task (IrT) to the task of\n",
            "interest (ToI). Specifically, we first identify an IrT, where dual-domain\n",
            "samples are available, and capture the domain shift with a coupled generative\n",
            "adversarial networks (CoGAN) in this task. Then, we train a CoGAN for the ToI\n",
            "and restrict it to carry the same domain shift as the CoGAN for IrT does. In\n",
            "addition, we introduce a pair of co-training classifiers to regularize the\n",
            "training procedure of CoGAN in the ToI. The proposed method not only derives\n",
            "machine learning models for the non-available target-domain data, but also\n",
            "synthesizes the data themselves. We evaluate the proposed method on benchmark\n",
            "datasets and achieve the state-of-the-art performances.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: We propose a framework for learning neural scene representations directly\n",
            "from images, without 3D supervision. Our key insight is that 3D structure can\n",
            "be imposed by ensuring that the learned representation transforms like a real\n",
            "3D scene. Specifically, we introduce a loss which enforces equivariance of the\n",
            "scene representation with respect to 3D transformations. Our formulation allows\n",
            "us to infer and render scenes in real time while achieving comparable results\n",
            "to models requiring minutes for inference. In addition, we introduce two\n",
            "challenging new datasets for scene representation and neural rendering,\n",
            "including scenes with complex lighting and backgrounds. Through experiments, we\n",
            "show that our model achieves compelling results on these datasets as well as on\n",
            "standard ShapeNet benchmarks.\n",
            "Label(s): [' ', '.', 'C', 'L', 'M', 'V', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Monocular (relative or metric) depth estimation is a critical task for\n",
            "various applications, such as autonomous vehicles, augmented reality and image\n",
            "editing. In recent years, with the increasing availability of mobile devices,\n",
            "accurate and mobile-friendly depth models have gained importance. Increasingly\n",
            "accurate models typically require more computational resources, which inhibits\n",
            "the use of such models on mobile devices. The mobile use case is arguably the\n",
            "most unrestricted one, which requires highly accurate yet mobile-friendly\n",
            "architectures. Therefore, we try to answer the following question: How can we\n",
            "improve a model without adding further complexity (i.e. parameters)? Towards\n",
            "this end, we systematically explore the design space of a relative depth\n",
            "estimation model from various dimensions and we show, with key design choices\n",
            "and ablation studies, even an existing architecture can reach highly\n",
            "competitive performance to the state of the art, with a fraction of the\n",
            "complexity. Our study spans an in-depth backbone model selection process,\n",
            "knowledge distillation, intermediate predictions, model pruning and loss\n",
            "rebalancing. We show that our model, using only DIW as the supervisory dataset,\n",
            "achieves 0.1156 WHDR on DIW with 2.6M parameters and reaches 37 FPS on a mobile\n",
            "GPU, without pruning or hardware-specific optimization. A pruned version of our\n",
            "model achieves 0.1208 WHDR on DIW with 1M parameters and reaches 44 FPS on a\n",
            "mobile GPU.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Deep reinforcement learning has been applied more and more widely nowadays,\n",
            "especially in various complex control tasks. Effective exploration for noisy\n",
            "networks is one of the most important issues in deep reinforcement learning.\n",
            "Noisy networks tend to produce stable outputs for agents. However, this\n",
            "tendency is not always enough to find a stable policy for an agent, which\n",
            "decreases efficiency and stability during the learning process. Based on\n",
            "NoisyNets, this paper proposes an algorithm called NROWAN-DQN, i.e., Noise\n",
            "Reduction and Online Weight Adjustment NoisyNet-DQN. Firstly, we develop a\n",
            "novel noise reduction method for NoisyNet-DQN to make the agent perform stable\n",
            "actions. Secondly, we design an online weight adjustment strategy for noise\n",
            "reduction, which improves stable performance and gets higher scores for the\n",
            "agent. Finally, we evaluate this algorithm in four standard domains and analyze\n",
            "properties of hyper-parameters. Our results show that NROWAN-DQN outperforms\n",
            "prior algorithms in all these domains. In addition, NROWAN-DQN also shows\n",
            "better stability. The variance of the NROWAN-DQN score is significantly\n",
            "reduced, especially in some action-sensitive environments. This means that in\n",
            "some environments where high stability is required, NROWAN-DQN will be more\n",
            "appropriate than NoisyNets-DQN.\n",
            "Label(s): [' ', '.', 'A', 'G', 'I', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Vision-based person, hand or face detection approaches have achieved\n",
            "incredible success in recent years with the development of deep convolutional\n",
            "neural network (CNN). In this paper, we take the inherent correlation between\n",
            "the body and body parts into account and propose a new framework to boost up\n",
            "the detection performance of the multi-level objects. In particular, we adopt a\n",
            "region-based object detection structure with two carefully designed detectors\n",
            "to separately pay attention to the human body and body parts in a\n",
            "coarse-to-fine manner, which we call Detector-in-Detector network (DID-Net).\n",
            "The first detector is designed to detect human body, hand, and face. The second\n",
            "detector, based on the body detection results of the first detector, mainly\n",
            "focus on the detection of small hand and face inside each body. The framework\n",
            "is trained in an end-to-end way by optimizing a multi-task loss. Due to the\n",
            "lack of human body, face and hand detection dataset, we have collected and\n",
            "labeled a new large dataset named Human-Parts with 14,962 images and 106,879\n",
            "annotations. Experiments show that our method can achieve excellent performance\n",
            "on Human-Parts.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: We propose a segmentation-based bounding box generation method for\n",
            "omnidirectional pedestrian detection, which enables detectors to tightly fit\n",
            "bounding boxes to pedestrians without omnidirectional images for training.\n",
            "Because the appearance of pedestrians in omnidirectional images may be rotated\n",
            "to any angle, the performance of common pedestrian detectors is likely to be\n",
            "substantially degraded. Existing methods mitigate this issue by transforming\n",
            "images during inference or training detectors with omnidirectional images.\n",
            "However, the first approach substantially degrades the inference speed, and the\n",
            "second approach requires laborious annotations. To overcome these drawbacks, we\n",
            "leverage an existing large-scale dataset, whose segmentation annotations can be\n",
            "utilized, to generate tightly fitted bounding box annotations. We also develop\n",
            "a pseudo-fisheye distortion augmentation method, which further enhances the\n",
            "performance. Extensive analysis shows that our detector successfully fits\n",
            "bounding boxes to pedestrians and demonstrates substantial performance\n",
            "improvement.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Time series classification is an application of particular interest with the\n",
            "increase of data to monitor. Classical techniques for time series\n",
            "classification rely on point-to-point distances. Recently, Bag-of-Words\n",
            "approaches have been used in this context. Words are quantized versions of\n",
            "simple features extracted from sliding windows. The SIFT framework has proved\n",
            "efficient for image classification. In this paper, we design a time series\n",
            "classification scheme that builds on the SIFT framework adapted to time series\n",
            "to feed a Bag-of-Words. We then refine our method by studying the impact of\n",
            "normalized Bag-of-Words, as well as densely extract point descriptors. Proposed\n",
            "adjustements achieve better performance. The evaluation shows that our method\n",
            "outperforms classical techniques in terms of classification.\n",
            "Label(s): [' ', '.', 'G', 'L', 'c', 's']\n",
            " \n",
            "Abstract: In this paper, we propose a novel real-time 6D object pose estimation\n",
            "framework, named G2L-Net. Our network operates on point clouds from RGB-D\n",
            "detection in a divide-and-conquer fashion. Specifically, our network consists\n",
            "of three steps. First, we extract the coarse object point cloud from the RGB-D\n",
            "image by 2D detection. Second, we feed the coarse object point cloud to a\n",
            "translation localization network to perform 3D segmentation and object\n",
            "translation prediction. Third, via the predicted segmentation and translation,\n",
            "we transfer the fine object point cloud into a local canonical coordinate, in\n",
            "which we train a rotation localization network to estimate initial object\n",
            "rotation. In the third step, we define point-wise embedding vector features to\n",
            "capture viewpoint-aware information. To calculate more accurate rotation, we\n",
            "adopt a rotation residual estimator to estimate the residual between initial\n",
            "rotation and ground truth, which can boost initial pose estimation performance.\n",
            "Our proposed G2L-Net is real-time despite the fact multiple steps are stacked\n",
            "via the proposed coarse-to-fine framework. Extensive experiments on two\n",
            "benchmark datasets show that G2L-Net achieves state-of-the-art performance in\n",
            "terms of both accuracy and speed.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Constant-curvature Riemannian manifolds (CCMs) have been shown to be ideal\n",
            "embedding spaces in many application domains, as their non-Euclidean geometry\n",
            "can naturally account for some relevant properties of data, like hierarchy and\n",
            "circularity. In this work, we introduce the CCM adversarial autoencoder\n",
            "(CCM-AAE), a probabilistic generative model trained to represent a data\n",
            "distribution on a CCM. Our method works by matching the aggregated posterior of\n",
            "the CCM-AAE with a probability distribution defined on a CCM, so that the\n",
            "encoder implicitly learns to represent data on the CCM to fool the\n",
            "discriminator network. The geometric constraint is also explicitly imposed by\n",
            "jointly training the CCM-AAE to maximise the membership degree of the\n",
            "embeddings to the CCM. While a few works in recent literature make use of\n",
            "either hyperspherical or hyperbolic manifolds for different learning tasks,\n",
            "ours is the first unified framework to seamlessly deal with CCMs of different\n",
            "curvatures. We show the effectiveness of our model on three different datasets\n",
            "characterised by non-trivial geometry: semi-supervised classification on MNIST,\n",
            "link prediction on two popular citation datasets, and graph-based molecule\n",
            "generation using the QM9 chemical database. Results show that our method\n",
            "improves upon other autoencoders based on Euclidean and non-Euclidean\n",
            "geometries on all tasks taken into account.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Operating directly from raw high dimensional sensory inputs like images is\n",
            "still a challenge for robotic control. Recently, Reinforcement Learning methods\n",
            "have been proposed to solve specific tasks end-to-end, from pixels to torques.\n",
            "However, these approaches assume the access to a specified reward which may\n",
            "require specialized instrumentation of the environment. Furthermore, the\n",
            "obtained policy and representations tend to be task specific and may not\n",
            "transfer well. In this work we investigate completely self-supervised learning\n",
            "of a general image embedding and control primitives, based on finding the\n",
            "shortest time to reach any state. We also introduce a new structure for the\n",
            "state-action value function that builds a connection between model-free and\n",
            "model-based methods, and improves the performance of the learning algorithm. We\n",
            "experimentally demonstrate these findings in three simulated robotic tasks.\n",
            "Label(s): [' ', '.', 'A', 'E', 'G', 'I', 'L', 'N', 'O', 'R', 'c', 's']\n",
            " \n",
            "Abstract: We present a method for performing hierarchical object detection in images\n",
            "guided by a deep reinforcement learning agent. The key idea is to focus on\n",
            "those parts of the image that contain richer information and zoom on them. We\n",
            "train an intelligent agent that, given an image window, is capable of deciding\n",
            "where to focus the attention among five different predefined region candidates\n",
            "(smaller windows). This procedure is iterated providing a hierarchical image\n",
            "analysis.We compare two different candidate proposal strategies to guide the\n",
            "object search: with and without overlap. Moreover, our work compares two\n",
            "different strategies to extract features from a convolutional neural network\n",
            "for each region proposal: a first one that computes new feature maps for each\n",
            "region proposal, and a second one that computes the feature maps for the whole\n",
            "image to later generate crops for each region proposal. Experiments indicate\n",
            "better results for the overlapping candidate proposal strategy and a loss of\n",
            "performance for the cropped image features due to the loss of spatial\n",
            "resolution. We argue that, while this loss seems unavoidable when working with\n",
            "large amounts of object candidates, the much more reduced amount of region\n",
            "proposals generated by our reinforcement learning agent allows considering to\n",
            "extract features for each location without sharing convolutional computation\n",
            "among regions.\n",
            "Label(s): [' ', '.', 'C', 'G', 'L', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Natural Language Inference (NLI) datasets contain annotation artefacts\n",
            "resulting in spurious correlations between the natural language utterances and\n",
            "their respective entailment classes. These artefacts are exploited by neural\n",
            "networks even when only considering the hypothesis and ignoring the premise,\n",
            "leading to unwanted biases. Belinkov et al. (2019b) proposed tackling this\n",
            "problem via adversarial training, but this can lead to learned sentence\n",
            "representations that still suffer from the same biases. We show that the bias\n",
            "can be reduced in the sentence representations by using an ensemble of\n",
            "adversaries, encouraging the model to jointly decrease the accuracy of these\n",
            "different adversaries while fitting the data. This approach produces more\n",
            "robust NLI models, outperforming previous de-biasing efforts when generalised\n",
            "to 12 other datasets (Belinkov et al., 2019a; Mahabadi et al., 2020). In\n",
            "addition, we find that the optimal number of adversarial classifiers depends on\n",
            "the dimensionality of the sentence representations, with larger sentence\n",
            "representations being more difficult to de-bias while benefiting from using a\n",
            "greater number of adversaries.\n",
            "Label(s): [' ', '.', 'A', 'C', 'G', 'I', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Deep Neural Network (DNN) models have vulnerabilities related to security\n",
            "concerns, with attackers usually employing complex hacking techniques to expose\n",
            "their structures. Data poisoning-enabled perturbation attacks are complex\n",
            "adversarial ones that inject false data into models. They negatively impact the\n",
            "learning process, with no benefit to deeper networks, as they degrade a model's\n",
            "accuracy and convergence rates. In this paper, we propose an\n",
            "attack-agnostic-based defense method for mitigating their influence. In it, a\n",
            "Defensive Feature Layer (DFL) is integrated with a well-known DNN architecture\n",
            "which assists in neutralizing the effects of illegitimate perturbation samples\n",
            "in the feature space. To boost the robustness and trustworthiness of this\n",
            "method for correctly classifying attacked input samples, we regularize the\n",
            "hidden space of a trained model with a discriminative loss function called\n",
            "Polarized Contrastive Loss (PCL). It improves discrimination among samples in\n",
            "different classes and maintains the resemblance of those in the same class.\n",
            "Also, we integrate a DFL and PCL in a compact model for defending against data\n",
            "poisoning attacks. This method is trained and tested using the CIFAR-10 and\n",
            "MNIST datasets with data poisoning-enabled perturbation attacks, with the\n",
            "experimental results revealing its excellent performance compared with those of\n",
            "recent peer techniques.\n",
            "Label(s): [' ', '.', 'A', 'C', 'I', 'V', 'c', 's']\n",
            " \n",
            "Abstract: We propose the Margin Adaptation for Generative Adversarial Networks (MAGANs)\n",
            "algorithm, a novel training procedure for GANs to improve stability and\n",
            "performance by using an adaptive hinge loss function. We estimate the\n",
            "appropriate hinge loss margin with the expected energy of the target\n",
            "distribution, and derive principled criteria for when to update the margin. We\n",
            "prove that our method converges to its global optimum under certain\n",
            "assumptions. Evaluated on the task of unsupervised image generation, the\n",
            "proposed training procedure is simple yet robust on a diverse set of data, and\n",
            "achieves qualitative and quantitative improvements compared to the\n",
            "state-of-the-art.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Effective fusion of different types of features is the key to salient object\n",
            "detection. The majority of existing network structure design is based on the\n",
            "subjective experience of scholars and the process of feature fusion does not\n",
            "consider the relationship between the fused features and highest-level\n",
            "features. In this paper, we focus on the feature relationship and propose a\n",
            "novel global attention unit, which we term the \"perception- and-regulation\"\n",
            "(PR) block, that adaptively regulates the feature fusion process by explicitly\n",
            "modeling interdependencies between features. The perception part uses the\n",
            "structure of fully-connected layers in classification networks to learn the\n",
            "size and shape of objects. The regulation part selectively strengthens and\n",
            "weakens the features to be fused. An imitating eye observation module (IEO) is\n",
            "further employed for improving the global perception ability of the network.\n",
            "The imitation of foveal vision and peripheral vision enables IEO to scrutinize\n",
            "highly detailed objects and to organize the broad spatial scene to better\n",
            "segment objects. Sufficient experiments conducted on SOD datasets demonstrate\n",
            "that the proposed method performs favorably against 22 state-of-the-art\n",
            "methods.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Documents often exhibit various forms of degradation, which make it hard to\n",
            "be read and substantially deteriorate the performance of an OCR system. In this\n",
            "paper, we propose an effective end-to-end framework named Document Enhancement\n",
            "Generative Adversarial Networks (DE-GAN) that uses the conditional GANs (cGANs)\n",
            "to restore severely degraded document images. To the best of our knowledge,\n",
            "this practice has not been studied within the context of generative adversarial\n",
            "deep networks. We demonstrate that, in different tasks (document clean up,\n",
            "binarization, deblurring and watermark removal), DE-GAN can produce an enhanced\n",
            "version of the degraded document with a high quality. In addition, our approach\n",
            "provides consistent improvements compared to state-of-the-art methods over the\n",
            "widely used DIBCO 2013, DIBCO 2017 and H-DIBCO 2018 datasets, proving its\n",
            "ability to restore a degraded document image to its ideal condition. The\n",
            "obtained results on a wide variety of degradation reveal the flexibility of the\n",
            "proposed model to be exploited in other document enhancement problems.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: This paper proposes a novel approach for uncertainty quantification in dense\n",
            "Conditional Random Fields (CRFs). The presented approach, called\n",
            "Perturb-and-MPM, enables efficient, approximate sampling from dense multi-label\n",
            "CRFs via random perturbations. An analytic error analysis was performed which\n",
            "identified the main cause of approximation error as well as showed that the\n",
            "error is bounded. Spatial uncertainty maps can be derived from the\n",
            "Perturb-and-MPM model, which can be used to visualize uncertainty in image\n",
            "segmentation results. The method is validated on synthetic and clinical\n",
            "Magnetic Resonance Imaging data. The effectiveness of the approach is\n",
            "demonstrated on the challenging problem of segmenting the tumor core in\n",
            "glioblastoma. We found that areas of high uncertainty correspond well to\n",
            "wrongly segmented image regions. Furthermore, we demonstrate the potential use\n",
            "of uncertainty maps to refine imaging biomarkers in the case of extent of\n",
            "resection and residual tumor volume in brain tumor patients.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Trust region policy optimization (TRPO) is a popular and empirically\n",
            "successful policy search algorithm in Reinforcement Learning (RL) in which a\n",
            "surrogate problem, that restricts consecutive policies to be 'close' to one\n",
            "another, is iteratively solved. Nevertheless, TRPO has been considered a\n",
            "heuristic algorithm inspired by Conservative Policy Iteration (CPI). We show\n",
            "that the adaptive scaling mechanism used in TRPO is in fact the natural \"RL\n",
            "version\" of traditional trust-region methods from convex analysis. We first\n",
            "analyze TRPO in the planning setting, in which we have access to the model and\n",
            "the entire state space. Then, we consider sample-based TRPO and establish\n",
            "$\\tilde O(1/\\sqrt{N})$ convergence rate to the global optimum. Importantly, the\n",
            "adaptive scaling mechanism allows us to analyze TRPO in regularized MDPs for\n",
            "which we prove fast rates of $\\tilde O(1/N)$, much like results in convex\n",
            "optimization. This is the first result in RL of better rates when regularizing\n",
            "the instantaneous cost or reward.\n",
            "Label(s): [' ', '.', 'C', 'G', 'L', 'M', 'O', 'a', 'c', 'h', 'm', 's', 't']\n",
            " \n",
            "Abstract: This paper addresses the issue on how to more effectively coordinate the\n",
            "depth with RGB aiming at boosting the performance of RGB-D object detection.\n",
            "Particularly, we investigate two primary ideas under the CNN model: property\n",
            "derivation and property fusion. Firstly, we propose that the depth can be\n",
            "utilized not only as a type of extra information besides RGB but also to derive\n",
            "more visual properties for comprehensively describing the objects of interest.\n",
            "So a two-stage learning framework consisting of property derivation and fusion\n",
            "is constructed. Here the properties can be derived either from the provided\n",
            "color/depth or their pairs (e.g. the geometry contour adopted in this paper).\n",
            "Secondly, we explore the fusion method of different properties in feature\n",
            "learning, which is boiled down to, under the CNN model, from which layer the\n",
            "properties should be fused together. The analysis shows that different semantic\n",
            "properties should be learned separately and combined before passing into the\n",
            "final classifier. Actually, such a detection way is in accordance with the\n",
            "mechanism of the primary neural cortex (V1) in brain. We experimentally\n",
            "evaluate the proposed method on the challenging dataset, and have achieved\n",
            "state-of-the-art performance.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: There are many forms of feature information present in video data. Principle\n",
            "among them are object identity information which is largely static across\n",
            "multiple video frames, and object pose and style information which continuously\n",
            "transforms from frame to frame. Most existing models confound these two types\n",
            "of representation by mapping them to a shared feature space. In this paper we\n",
            "propose a probabilistic approach for learning separable representations of\n",
            "object identity and pose information using unsupervised video data. Our\n",
            "approach leverages a deep generative model with a factored prior distribution\n",
            "that encodes properties of temporal invariances in the hidden feature set.\n",
            "Learning is achieved via variational inference. We present results of learning\n",
            "identity and pose information on a dataset of moving characters as well as a\n",
            "dataset of rotating 3D objects. Our experimental results demonstrate our\n",
            "model's success in factoring its representation, and demonstrate that the model\n",
            "achieves improved performance in transfer learning tasks.\n",
            "Label(s): [' ', '.', 'C', 'G', 'L', 'M', 'V', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Automatic surgical instruction generation is a prerequisite towards\n",
            "intra-operative context-aware surgical assistance. However, generating\n",
            "instructions from surgical scenes is challenging, as it requires jointly\n",
            "understanding the surgical activity of current view and modelling relationships\n",
            "between visual information and textual description. Inspired by the neural\n",
            "machine translation and imaging captioning tasks in open domain, we introduce a\n",
            "transformer-backboned encoder-decoder network with self-critical reinforcement\n",
            "learning to generate instructions from surgical images. We evaluate the\n",
            "effectiveness of our method on DAISI dataset, which includes 290 procedures\n",
            "from various medical disciplines. Our approach outperforms the existing\n",
            "baseline over all caption evaluation metrics. The results demonstrate the\n",
            "benefits of the encoder-decoder structure backboned by transformer in handling\n",
            "multimodal context.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Deep learning-based style transfer between images has recently become a\n",
            "popular area of research. A common way of encoding \"style\" is through a feature\n",
            "representation based on the Gram matrix of features extracted by some\n",
            "pre-trained neural network or some other form of feature statistics. Such a\n",
            "definition is based on an arbitrary human decision and may not best capture\n",
            "what a style really is. In trying to gain a better understanding of \"style\", we\n",
            "propose a metric learning-based method to explicitly encode the style of an\n",
            "artwork. In particular, our definition of style captures the differences\n",
            "between artists, as shown by classification performances, and such that the\n",
            "style representation can be interpreted, manipulated and visualized through\n",
            "style-conditioned image generation through a Generative Adversarial Network. We\n",
            "employ this method to explore the style space of anime portrait illustrations.\n",
            "Label(s): [' ', '.', 'C', 'L', 'M', 'V', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: Ship detection in remote sensing images plays a crucial role in various\n",
            "applications and has drawn increasing attention in recent years. However,\n",
            "existing multi-oriented ship detection methods are generally developed on a set\n",
            "of predefined rotated anchor boxes. These predefined boxes not only lead to\n",
            "inaccurate angle predictions but also introduce extra hyper-parameters and high\n",
            "computational cost. Moreover, the prior knowledge of ship size has not been\n",
            "fully exploited by existing methods, which hinders the improvement of their\n",
            "detection accuracy. Aiming at solving the above issues, in this paper, we\n",
            "propose a \\emph{center-head point extraction based detector} (named CHPDet) to\n",
            "achieve arbitrary-oriented ship detection in remote sensing images. Our CHPDet\n",
            "formulates arbitrary-oriented ships as rotated boxes with head points which are\n",
            "used to determine the direction. The orientation-invariant model (OIM) is used\n",
            "to produce orientation-invariant feature maps. Keypoint estimation is performed\n",
            "to find the center of ships. Then, the size and head point of the ships are\n",
            "regressed. Finally, we use the target size as prior to finetune the results.\n",
            "Moreover, we introduce a new dataset for multi-class arbitrary-oriented ship\n",
            "detection in remote sensing images at a fixed ground sample distance (GSD)\n",
            "which is named FGSD2021. Experimental results on two ship detection datasets\n",
            "(i.e., FGSD2021 and HRSC2016) demonstrate that our CHPDet achieves\n",
            "state-of-the-art performance and can well distinguish between bow and stern.\n",
            "The code and dataset will be made publicly available.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: We introduce a novel geometry-oriented methodology, based on the emerging\n",
            "tools of topological data analysis, into the change point detection framework.\n",
            "The key rationale is that change points are likely to be associated with\n",
            "changes in geometry behind the data generating process. While the applications\n",
            "of topological data analysis to change point detection are potentially very\n",
            "broad, in this paper we primarily focus on integrating topological concepts\n",
            "with the existing nonparametric methods for change point detection. In\n",
            "particular, the proposed new geometry-oriented approach aims to enhance\n",
            "detection accuracy of distributional regime shift locations. Our simulation\n",
            "studies suggest that integration of topological data analysis with some\n",
            "existing algorithms for change point detection leads to consistently more\n",
            "accurate detection results. We illustrate our new methodology in application to\n",
            "the two closely related environmental time series datasets -ice phenology of\n",
            "the Lake Baikal and the North Atlantic Oscillation indices, in a research query\n",
            "for a possible association between their estimated regime shift locations.\n",
            "Label(s): [' ', '.', 'G', 'L', 'M', 'a', 'c', 's', 't']\n",
            " \n",
            "Abstract: In this paper, we propose a novel SpatioTemporal convolutional Dense Network\n",
            "(STDNet) to address the video-based crowd counting problem, which contains the\n",
            "decomposition of 3D convolution and the 3D spatiotemporal dilated dense\n",
            "convolution to alleviate the rapid growth of the model size caused by the\n",
            "Conv3D layer. Moreover, since the dilated convolution extracts the multiscale\n",
            "features, we combine the dilated convolution with the channel attention block\n",
            "to enhance the feature representations. Due to the error that occurs from the\n",
            "difficulty of labeling crowds, especially for videos, imprecise or\n",
            "standard-inconsistent labels may lead to poor convergence for the model. To\n",
            "address this issue, we further propose a new patch-wise regression loss (PRL)\n",
            "to improve the original pixel-wise loss. Experimental results on three\n",
            "video-based benchmarks, i.e., the UCSD, Mall and WorldExpo'10 datasets, show\n",
            "that STDNet outperforms both image- and video-based state-of-the-art methods.\n",
            "The source codes are released at \\url{https://github.com/STDNet/STDNet}.\n",
            "Label(s): [' ', '.', 'C', 'G', 'L', 'V', 'c', 's']\n",
            " \n",
            "Abstract: Despite learning-based visual odometry (VO) has shown impressive results in\n",
            "recent years, the pretrained networks may easily collapse in unseen\n",
            "environments. The large domain gap between training and testing data makes them\n",
            "difficult to generalize to new scenes. In this paper, we propose an online\n",
            "adaptation framework for deep VO with the assistance of scene-agnostic\n",
            "geometric computations and Bayesian inference. In contrast to learning-based\n",
            "pose estimation, our method solves pose from optical flow and depth while the\n",
            "single-view depth estimation is continuously improved with new observations by\n",
            "online learned uncertainties. Meanwhile, an online learned photometric\n",
            "uncertainty is used for further depth and pose optimization by a differentiable\n",
            "Gauss-Newton layer. Our method enables fast adaptation of deep VO networks to\n",
            "unseen environments in a self-supervised manner. Extensive experiments\n",
            "including Cityscapes to KITTI and outdoor KITTI to indoor TUM demonstrate that\n",
            "our method achieves state-of-the-art generalization ability among\n",
            "self-supervised VO methods.\n",
            "Label(s): [' ', '.', 'C', 'V', 'c', 's']\n",
            " \n",
            "Abstract: For many automated driving functions, a highly accurate perception of the\n",
            "vehicle environment is a crucial prerequisite. Modern high-resolution radar\n",
            "sensors generate multiple radar targets per object, which makes these sensors\n",
            "particularly suitable for the 2D object detection task. This work presents an\n",
            "approach to detect 2D objects solely depending on sparse radar data using\n",
            "PointNets. In literature, only methods are presented so far which perform\n",
            "either object classification or bounding box estimation for objects. In\n",
            "contrast, this method facilitates a classification together with a bounding box\n",
            "estimation of objects using a single radar sensor. To this end, PointNets are\n",
            "adjusted for radar data performing 2D object classification with segmentation,\n",
            "and 2D bounding box regression in order to estimate an amodal 2D bounding box.\n",
            "The algorithm is evaluated using an automatically created dataset which consist\n",
            "of various realistic driving maneuvers. The results show the great potential of\n",
            "object detection in high-resolution radar data using PointNets.\n",
            "Label(s): [' ', '.', 'C', 'G', 'L', 'M', 'V', 'a', 'c', 's', 't']\n",
            " \n"
          ]
        }
      ],
      "source": [
        "def invert_multi_hot(label):\n",
        "    # Assuming the label is a binary array where 1 indicates presence and 0 indicates absence of a category\n",
        "    return [unique_categories[i] for i, val in enumerate(label) if val == 1]\n",
        "\n",
        "# Iterate through batches of the training dataset\n",
        "for batch in train_batches:\n",
        "    text_batch, label_batch = zip(*batch)  # Unpack batch into text_batch and label_batch\n",
        "    # Iterate through samples in the batch\n",
        "    for i, text in enumerate(text_batch[:5]):\n",
        "        label = label_batch[i][None, ...]  # Convert label to 2D array\n",
        "        print(f\"Abstract: {text}\")\n",
        "        print(f\"Label(s): {invert_multi_hot(label[0])}\")\n",
        "        print(\" \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3881b07a",
      "metadata": {
        "id": "3881b07a",
        "outputId": "751e0d9c-6c99-4b56-ea74-d5d1fe582bca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "158810\n"
          ]
        }
      ],
      "source": [
        " # This code calculates the size of the vocabulary in the \"abstracts\" column of the train_df DataFrame.\n",
        "\n",
        "# Creating vocabulary with uniques words\n",
        "vocabulary = set()\n",
        "train_df[\"abstracts\"].str.lower().str.split().apply(vocabulary.update)\n",
        "vocabulary_size = len(vocabulary)\n",
        "print(vocabulary_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[1][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiP2bJ-Bfgur",
        "outputId": "b1b8e426-7f1e-42d7-bdfb-21e41569b9f2"
      },
      "id": "WiP2bJ-Bfgur",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('While accelerators such as GPUs have limited memory, deep neural networks are\\nbecoming larger and will not fit with the memory limitation of accelerators for\\ntraining. We propose an approach to tackle this problem by rewriting the\\ncomputational graph of a neural network, in which swap-out and swap-in\\noperations are inserted to temporarily store intermediate results on CPU\\nmemory. In particular, we first revise the concept of a computational graph by\\ndefining a concrete semantics for variables in a graph. We then formally show\\nhow to derive swap-out and swap-in operations from an existing graph and\\npresent rules to optimize the graph. To realize our approach, we developed a\\nmodule in TensorFlow, named TFLMS. TFLMS is published as a pull request in the\\nTensorFlow repository for contributing to the TensorFlow community. With TFLMS,\\nwe were able to train ResNet-50 and 3DUnet with 4.7x and 2x larger batch size,\\nrespectively. In particular, we were able to train 3DUNet using images of size\\nof $192^3$ for image segmentation, which, without TFLMS, had been done only by\\ndividing the images to smaller images, which affects the accuracy.',\n",
              " array([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0]))"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f8c291e",
      "metadata": {
        "id": "4f8c291e"
      },
      "source": [
        "# Text Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13305e60",
      "metadata": {
        "id": "13305e60"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "# Initialize a TextVectorization layer\n",
        "text_vectorizer = TextVectorization(max_tokens=vocabulary_size, ngrams=2, output_mode=\"tf_idf\")\n",
        "\n",
        "# Extract text data from all batches in the training set\n",
        "texts = []\n",
        "for batch in train_dataset:\n",
        "    text_batch = batch[0]  # Extract text batch\n",
        "    texts.extend(text_batch)  # Convert TensorFlow tensor to numpy array and extend the list\n",
        "texts = [str(text) for text in texts]\n",
        "# Adapt the TextVectorization layer using the text data\n",
        "text_vectorizer.adapt(texts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b82682c",
      "metadata": {
        "id": "6b82682c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Flatten the lists in the \"terms\" column into strings\n",
        "train_df['terms'] = train_df['terms'].apply(lambda x: ' '.join(x))\n",
        "val_df['terms'] = val_df['terms'].apply(lambda x: ' '.join(x))\n",
        "test_df['terms'] = test_df['terms'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Now train_df, val_df, and test_df have the \"terms\" column with strings instead of lists\n",
        "\n",
        "# Now you can proceed with creating TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_df['abstracts'].values, train_df['terms'].values, train_df['titles'].values))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_df['abstracts'].values, val_df['terms'].values, val_df['titles'].values))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_df['abstracts'].values, test_df['terms'].values, test_df['titles'].values))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85485df2",
      "metadata": {
        "id": "85485df2"
      },
      "source": [
        "# model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1595fa1d",
      "metadata": {
        "scrolled": true,
        "id": "1595fa1d",
        "outputId": "3f30a7d4-bd01-4943-d407-4b03dae9e7ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnimplementedError",
          "evalue": "Graph execution error:\n\nDetected at node binary_crossentropy/Cast defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-120-befe5871910d>\", line 25, in <cell line: 25>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2521, in binary_crossentropy\n\n2 root error(s) found.\n  (0) UNIMPLEMENTED:  Cast string to float is not supported\n\t [[{{node binary_crossentropy/Cast}}]]\n  (1) CANCELLED:  Function was cancelled before it was started\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_9317]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-120-befe5871910d>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node binary_crossentropy/Cast defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-120-befe5871910d>\", line 25, in <cell line: 25>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2521, in binary_crossentropy\n\n2 root error(s) found.\n  (0) UNIMPLEMENTED:  Cast string to float is not supported\n\t [[{{node binary_crossentropy/Cast}}]]\n  (1) CANCELLED:  Function was cancelled before it was started\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_9317]"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "# Convert the text data using TextVectorization layer\n",
        "text_vectorizer = layers.TextVectorization(max_tokens=10000, output_mode='tf-idf')\n",
        "text_vectorizer.adapt(train_df['abstracts'])\n",
        "\n",
        "# Create shallow MLP model with single neuron output\n",
        "model = models.Sequential([\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')  # Output layer with single neuron\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=20, callbacks=[early_stopping])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ccc3be2",
      "metadata": {
        "scrolled": true,
        "id": "1ccc3be2",
        "outputId": "dfdc6c05-4988-4149-ab75-ae1efb9a0d15"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plotting loss\n",
        "def plot_result(item):\n",
        "    plt.plot(history.history[item], label=item)\n",
        "    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(item)\n",
        "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_result(\"loss\")\n",
        "plot_result(\"binary_accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9864d7ce",
      "metadata": {
        "id": "9864d7ce"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cebc1c9c",
      "metadata": {
        "id": "cebc1c9c",
        "outputId": "d1f03771-463d-4dcf-b1c6-bd2649c19175"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 9s 553ms/step - loss: 0.0182 - binary_accuracy: 0.9946\n",
            "16/16 [==============================] - 9s 559ms/step - loss: 0.0183 - binary_accuracy: 0.9945\n",
            "Categorical accuracy on the test set: 99.46%.\n",
            "Categorical accuracy on the validation set: 99.45%.\n"
          ]
        }
      ],
      "source": [
        "# model evaltuation on test and val dataset\n",
        "_, binary_acc1 = model1.evaluate(test_dataset)\n",
        "_, binary_acc2 = model1.evaluate(validation_dataset)\n",
        "\n",
        "print(f\"Categorical accuracy on the test set: {round(binary_acc1 * 100, 2)}%.\")\n",
        "print(f\"Categorical accuracy on the validation set: {round(binary_acc2 * 100, 2)}%.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16f447f1",
      "metadata": {
        "id": "16f447f1"
      },
      "source": [
        "# Save Model and Text Vectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4434b752",
      "metadata": {
        "id": "4434b752"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model1.save(\"models/model.h5\")\n",
        "\n",
        "# Save the configuration of the text vectorizer\n",
        "saved_text_vectorizer_config = text_vectorizer.get_config()\n",
        "with open(\"models/text_vectorizer_config.pkl\", \"wb\") as f:\n",
        "    pickle.dump(saved_text_vectorizer_config, f)\n",
        "\n",
        "\n",
        "# Save the vocabulary\n",
        "with open(\"models/vocab.pkl\", \"wb\") as f:\n",
        "    pickle.dump(vocab, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fceee66a",
      "metadata": {
        "id": "fceee66a"
      },
      "source": [
        "# Load Model and Text Vectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34e7d7f6",
      "metadata": {
        "id": "34e7d7f6"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import pickle\n",
        "\n",
        "# Load the model\n",
        "loaded_model = keras.models.load_model(\"models/model.h5\")\n",
        "\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "# Load the configuration of the text vectorizer\n",
        "with open(\"models/text_vectorizer_config.pkl\", \"rb\") as f:\n",
        "    saved_text_vectorizer_config = pickle.load(f)\n",
        "\n",
        "# Create a new TextVectorization layer with the saved configuration\n",
        "loaded_text_vectorizer = TextVectorization.from_config(saved_text_vectorizer_config)\n",
        "\n",
        "# Load the saved weights into the new TextVectorization layer\n",
        "with open(\"models/text_vectorizer_weights.pkl\", \"rb\") as f:\n",
        "    weights = pickle.load(f)\n",
        "    loaded_text_vectorizer.set_weights(weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27e3fe39",
      "metadata": {
        "id": "27e3fe39"
      },
      "outputs": [],
      "source": [
        "# Load the vocabulary\n",
        "with open(\"models/vocab.pkl\", \"rb\") as f:\n",
        "    loaded_vocab = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "506f9a17",
      "metadata": {
        "id": "506f9a17"
      },
      "source": [
        "# Model Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e2985e7",
      "metadata": {
        "id": "8e2985e7"
      },
      "outputs": [],
      "source": [
        "def invert_multi_hot(encoded_labels):\n",
        "    \"\"\"Reverse a single multi-hot encoded label to a tuple of vocab terms.\"\"\"\n",
        "    hot_indices = np.argwhere(encoded_labels == 1.0)[..., 0]\n",
        "    return np.take(loaded_vocab, hot_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76f5b374",
      "metadata": {
        "id": "76f5b374"
      },
      "outputs": [],
      "source": [
        "def predict_category(abstract, model, vectorizer, label_lookup):\n",
        "    # Preprocess the abstract using the loaded text vectorizer\n",
        "    preprocessed_abstract = vectorizer([abstract])\n",
        "\n",
        "    # Make predictions using the loaded model\n",
        "    predictions = model.predict(preprocessed_abstract)\n",
        "\n",
        "    # Convert predictions to human-readable labels\n",
        "    predicted_labels = label_lookup(np.round(predictions).astype(int)[0])\n",
        "\n",
        "    return predicted_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f91a6e8",
      "metadata": {
        "id": "6f91a6e8",
        "outputId": "780fba18-868f-49e2-f43d-171f1ab1eb22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "Predicted Categories: ['cs.LG']\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "new_abstract = \"Graph neural networks (GNNs) have been widely used to learn vector\\nrepresentation of graph-structured data and achieved better task performance\\nthan conventional methods. The foundation of GNNs is the message passing\\nprocedure, which propagates the information in a node to its neighbors. Since\\nthis procedure proceeds one step per layer, the range of the information\\npropagation among nodes is small in the lower layers, and it expands toward the\\nhigher layers. Therefore, a GNN model has to be deep enough to capture global\\nstructural information in a graph. On the other hand, it is known that deep GNN\\nmodels suffer from performance degradation because they lose nodes' local\\ninformation, which would be essential for good model performance, through many\\nmessage passing steps. In this study, we propose multi-level attention pooling\\n(MLAP) for graph-level classification tasks, which can adapt to both local and\\nglobal structural information in a graph. It has an attention pooling layer for\\neach message passing step and computes the final graph representation by\\nunifying the layer-wise graph representations. The MLAP architecture allows\\nmodels to utilize the structural information of graphs with multiple levels of\\nlocalities because it preserves layer-wise information before losing them due\\nto oversmoothing. Results of our experiments show that the MLAP architecture\\nimproves the graph classification performance compared to the baseline\\narchitectures. In addition, analyses on the layer-wise graph representations\\nsuggest that aggregating information from multiple levels of localities indeed\\nhas the potential to improve the discriminability of learned graph\\nrepresentations.\"\n",
        "predicted_categories = predict_category(new_abstract, loaded_model, loaded_text_vectorizer, invert_multi_hot)\n",
        "print(\"Predicted Categories:\", predicted_categories)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e13ef48a",
      "metadata": {
        "id": "e13ef48a",
        "outputId": "ce7dbf05-d1ed-4e5b-8c18-b9b6678ff57e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 459ms/step\n",
            "Predicted Categories: ['cs.LG' 'cs.AI']\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "new_abstract = 'Deep networks and decision forests (such as random forests and gradient\\nboosted trees) are the leading machine learning methods for structured and\\ntabular data, respectively. Many papers have empirically compared large numbers\\nof classifiers on one or two different domains (e.g., on 100 different tabular\\ndata settings). However, a careful conceptual and empirical comparison of these\\ntwo strategies using the most contemporary best practices has yet to be\\nperformed. Conceptually, we illustrate that both can be profitably viewed as\\n\"partition and vote\" schemes. Specifically, the representation space that they\\nboth learn is a partitioning of feature space into a union of convex polytopes.\\nFor inference, each decides on the basis of votes from the activated nodes.\\nThis formulation allows for a unified basic understanding of the relationship\\nbetween these methods. Empirically, we compare these two strategies on hundreds\\nof tabular data settings, as well as several vision and auditory settings. Our\\nfocus is on datasets with at most 10,000 samples, which represent a large\\nfraction of scientific and biomedical datasets. In general, we found forests to\\nexcel at tabular and structured data (vision and audition) with small sample\\nsizes, whereas deep nets performed better on structured data with larger sample\\nsizes. This suggests that further gains in both scenarios may be realized via\\nfurther combining aspects of forests and networks. We will continue revising\\nthis technical report in the coming months with updated results.'\n",
        "predicted_categories = predict_category(new_abstract, loaded_model, loaded_text_vectorizer, invert_multi_hot)\n",
        "print(\"Predicted Categories:\", predicted_categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77b90c6e",
      "metadata": {
        "id": "77b90c6e"
      },
      "outputs": [],
      "source": [
        "# great resutls..................................."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5c90e37",
      "metadata": {
        "id": "b5c90e37"
      },
      "source": [
        "# =======Section 2========"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cecfe2d1",
      "metadata": {
        "id": "cecfe2d1"
      },
      "source": [
        "# 2 Recommendation System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "08b0ebec",
      "metadata": {
        "id": "08b0ebec"
      },
      "outputs": [],
      "source": [
        "arxiv_data.drop(columns = [\"terms\",\"abstracts\"], inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c94e6f2f",
      "metadata": {
        "id": "c94e6f2f"
      },
      "outputs": [],
      "source": [
        "arxiv_data.drop_duplicates(inplace= True)\n",
        "arxiv_data.reset_index(drop= True,inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8ca5e456",
      "metadata": {
        "id": "8ca5e456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "11db85d1-7f5a-4357-d3eb-3ce27148a439"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                 titles\n",
              "0      Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities\n",
              "1           Decision Forests vs. Deep Networks: Conceptual Similarities and Empirical Differences at Small Sample Sizes\n",
              "2                                                       Power up! Robust Graph Convolutional Network via Graph Powering\n",
              "3                                                  Releasing Graph Neural Networks with Differential Privacy Guarantees\n",
              "4                                   Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification\n",
              "...                                                                                                                 ...\n",
              "41100              An experimental study of graph-based semi-supervised classification with additional node information\n",
              "41101                                                          Bayesian Differential Privacy through Posterior Sampling\n",
              "41102                                       Mining Spatio-temporal Data on Industrialization from Historical Registries\n",
              "41103                                                 Wav2Letter: an End-to-End ConvNet-based Speech Recognition System\n",
              "41104                                                                                       Generalized Low Rank Models\n",
              "\n",
              "[41105 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-21b1c17e-c405-48a4-907c-524211c05bd6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>titles</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Decision Forests vs. Deep Networks: Conceptual Similarities and Empirical Differences at Small Sample Sizes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Power up! Robust Graph Convolutional Network via Graph Powering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Releasing Graph Neural Networks with Differential Privacy Guarantees</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41100</th>\n",
              "      <td>An experimental study of graph-based semi-supervised classification with additional node information</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41101</th>\n",
              "      <td>Bayesian Differential Privacy through Posterior Sampling</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41102</th>\n",
              "      <td>Mining Spatio-temporal Data on Industrialization from Historical Registries</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41103</th>\n",
              "      <td>Wav2Letter: an End-to-End ConvNet-based Speech Recognition System</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41104</th>\n",
              "      <td>Generalized Low Rank Models</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>41105 rows × 1 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-21b1c17e-c405-48a4-907c-524211c05bd6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-21b1c17e-c405-48a4-907c-524211c05bd6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-21b1c17e-c405-48a4-907c-524211c05bd6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-af615d60-0746-41b2-be50-8fefe8bc0a2b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-af615d60-0746-41b2-be50-8fefe8bc0a2b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-af615d60-0746-41b2-be50-8fefe8bc0a2b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_f752777e-9eec-471c-bd15-9a2e15c2c485\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('arxiv_data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f752777e-9eec-471c-bd15-9a2e15c2c485 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('arxiv_data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "arxiv_data",
              "summary": "{\n  \"name\": \"arxiv_data\",\n  \"rows\": 41105,\n  \"fields\": [\n    {\n      \"column\": \"titles\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 41105,\n        \"samples\": [\n          \"Semi-supervised Federated Learning for Activity Recognition\",\n          \"SATR-DL: Improving Surgical Skill Assessment and Task Recognition in Robot-assisted Surgery with Deep Neural Networks\",\n          \"A Hybrid Stochastic Policy Gradient Algorithm for Reinforcement Learning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "arxiv_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95b71915",
      "metadata": {
        "id": "95b71915"
      },
      "source": [
        "# Sentence Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5712474f",
      "metadata": {
        "id": "5712474f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "514a5734",
      "metadata": {
        "id": "514a5734",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81b3ec65-cac4-4e73-bea2-ab17b8047eb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "import pickle\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# Example sentences (assuming `arxiv_data` is defined)\n",
        "sentences = arxiv_data[\"titles\"]\n",
        "\n",
        "# Tokenization and normalization\n",
        "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
        "stop_words = set(stopwords.words('english'))\n",
        "normalized_sentences = [[word for word in sentence if word.isalnum() and word not in stop_words] for sentence in tokenized_sentences]\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(normalized_sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
        "\n",
        "# Saving the Word2Vec model\n",
        "with open('word2vec_model.pkl', 'wb') as f:\n",
        "    pickle.dump(word2vec_model, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "c502c4e0",
      "metadata": {
        "id": "c502c4e0"
      },
      "outputs": [],
      "source": [
        "# Function to generate sentence embeddings by averaging word embeddings\n",
        "def sentence_embedding(sentence, model):\n",
        "    vectors = []\n",
        "    for word in sentence:\n",
        "        if word in model.wv:\n",
        "            vectors.append(model.wv[word])\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "# Generate sentence embeddings\n",
        "all_sentence_embeddings = [sentence_embedding(sentence, word2vec_model) for sentence in normalized_sentences]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db125c7d",
      "metadata": {
        "id": "db125c7d"
      },
      "source": [
        "# Why select all-MiniLM-L6-v2?\n",
        "\n",
        "All-round model tuned for many use-cases. Trained on a large and diverse dataset of over 1 billion training pairs. Source\n",
        "\n",
        "Its small in size 80 MB with good performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "355ffc70",
      "metadata": {
        "id": "355ffc70"
      },
      "source": [
        "# Print the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "0af0618d",
      "metadata": {
        "id": "0af0618d"
      },
      "outputs": [],
      "source": [
        "# c = 0\n",
        "# # Iterate over pairs of sentences and their corresponding embeddings\n",
        "# for sentence, normalized_sentence in zip(sentences, normalized_sentences):\n",
        "#     # Generate sentence embedding\n",
        "#     vectors = []\n",
        "#     for word in normalized_sentence:\n",
        "#         if word in word2vec_model.wv:\n",
        "#             vectors.append(word2vec_model.wv[word])\n",
        "#     if vectors:\n",
        "#         sentence_embedding = np.mean(vectors, axis=0)\n",
        "#     else:\n",
        "#         sentence_embedding = np.zeros(word2vec_model.vector_size)\n",
        "\n",
        "#     # Print information\n",
        "#     print(\"Sentence:\", sentence)\n",
        "#     print(\"Embedding length:\", len(sentence_embedding)) # list of floats\n",
        "#     print(\"\")\n",
        "\n",
        "#     # Break out of the loop after printing information for the first 5 sentences\n",
        "#     if c >= 5:\n",
        "#         break\n",
        "#     c += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5cdb484",
      "metadata": {
        "id": "f5cdb484"
      },
      "source": [
        "# Save files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "a802f9f1",
      "metadata": {
        "id": "a802f9f1"
      },
      "outputs": [],
      "source": [
        "# Saving sentences and corresponding embeddings\n",
        "with open('embeddings.pkl', 'wb') as f:\n",
        "    pickle.dump(all_sentence_embeddings, f)\n",
        "\n",
        "with open('sentences.pkl', 'wb') as f:\n",
        "    pickle.dump(sentences, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dcea57f",
      "metadata": {
        "id": "1dcea57f"
      },
      "source": [
        "# Recommendation for similar papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "dcb1a450",
      "metadata": {
        "id": "dcb1a450"
      },
      "outputs": [],
      "source": [
        "# Load saved files\n",
        "sentences = pickle.load(open('sentences.pkl','rb'))\n",
        "word2vec_model = pickle.load(open('word2vec_model.pkl','rb'))\n",
        "all_sentence_embeddings = pickle.load(open('embeddings.pkl','rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "84ffe218",
      "metadata": {
        "id": "84ffe218"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def recommendation(input_paper, embeddings, sentences):\n",
        "    # Calculate embeddings for the input paper\n",
        "    input_embedding = sentence_embedding(input_paper, word2vec_model)\n",
        "\n",
        "    # Calculate cosine similarity scores between the embeddings of input_paper and all papers in the dataset.\n",
        "    cosine_scores = cosine_similarity([input_embedding], embeddings)[0]\n",
        "\n",
        "    # Get the indices of the top-k most similar papers based on cosine similarity.\n",
        "    top_similar_papers_indices = np.argsort(cosine_scores)[::-1][:5]\n",
        "\n",
        "    # Retrieve the titles of the top similar papers.\n",
        "    papers_list = [sentences[i] for i in top_similar_papers_indices]\n",
        "\n",
        "    return papers_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "12c90388",
      "metadata": {
        "id": "12c90388",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "704b8bb9-f146-48ec-cc74-61b79c1e2910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Recommended Papers:\n",
            "1. A Neural Network for Semigroups\n",
            "2. SGAD: Soft-Guided Adaptively-Dropped Neural Network\n",
            "3. Neural Network Branch-and-Bound for Neural Network Verification\n",
            "4. MGIC: Multigrid-in-Channels Neural Network Architectures\n",
            "5. Which Minimizer Does My Neural Network Converge To?\n"
          ]
        }
      ],
      "source": [
        "input_paper = \"Neural Network\"\n",
        "input_paper_tokens = word_tokenize(input_paper.lower())\n",
        "recommendations = recommendation(input_paper_tokens, all_sentence_embeddings, sentences)\n",
        "print(\"Top 5 Recommended Papers:\")\n",
        "for i, paper in enumerate(recommendations, 1):\n",
        "    print(f\"{i}. {paper}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "026a1eb1",
      "metadata": {
        "id": "026a1eb1"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "813491da",
      "metadata": {
        "id": "813491da"
      },
      "outputs": [],
      "source": [
        "base_url = \"https://arxiv.org/search/?query={}&searchtype=all&abstracts=show&order=-announced_date_first&size=50\"\n",
        "\n",
        "final_urls = []\n",
        "titles = []\n",
        "abstracts = []\n",
        "for paper in recommendations:\n",
        "    query = '+'.join(paper.split())\n",
        "    url = base_url.format(query)\n",
        "    web = requests.get(url)\n",
        "    soup = BeautifulSoup(web.content,\"html.parser\")\n",
        "    links = soup.find_all(\"p\", attrs={'class':'list-title is-inline-block'})\n",
        "    for i in range(len(links)):\n",
        "      final_urls.append(links[i].find_all(\"a\")[0].get('href'))\n",
        "\n",
        "for i in range(len(final_urls)):\n",
        "  web2 = requests.get(final_urls[i])\n",
        "  soup2 = BeautifulSoup(web2.content,\"html.parser\")\n",
        "  title = soup2.find(\"h1\",attrs={'class':'title mathjax'}).text\n",
        "  title=title.strip(\"Title\").strip(':')\n",
        "  titles.append(title)\n",
        "  abstract = soup2.find('blockquote',attrs={'class':'abstract mathjax'})\n",
        "  abstract=abstract.text.strip('\\nAbstract:')\n",
        "  abstracts.append(abstract)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(titles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0pxkpFRqKil",
        "outputId": "2f58f04b-9528-4f1a-ef7e-41a091bc3211"
      },
      "id": "i0pxkpFRqKil",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abstracts[-7]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "PA_hGt8GqQQz",
        "outputId": "4612d05c-a184-4a80-8e1b-67c5a674db56"
      },
      "id": "PA_hGt8GqQQz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Motif discovery is gaining increasing attention in the domain of functional data analysis. Functional motifs are typical \"shapes\" or \"patterns\" that recur multiple times in different portions of a single curve and/or in misaligned portions of multiple curves. In this paper, we define functional motifs using an additive model and we propose funBIalign for their discovery and evaluation. Inspired by clustering and biclustering techniques, funBIalign is a multi-step procedure which uses agglomerative hierarchical clustering with complete linkage and a functional distance based on mean squared residue scores to discover functional motifs, both in a single curve (e.g., time series) and in a set of curves. We assess its performance and compare it to other recent methods through extensive simulations. Moreover, we use funBIalign for discovering motifs in two real-data case studies; one on food price inflation and one on temperature changes.\\n    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_urls[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FQA5rvqbqX75",
        "outputId": "3767e2ec-18fb-4b5e-d10c-8f3d2775ccb6"
      },
      "id": "FQA5rvqbqX75",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://arxiv.org/abs/2403.08011'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r4jsXBoRqZvK"
      },
      "id": "r4jsXBoRqZvK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T4AGoqN4qbES"
      },
      "id": "T4AGoqN4qbES",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "86GG6OXNqk2m"
      },
      "id": "86GG6OXNqk2m",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}